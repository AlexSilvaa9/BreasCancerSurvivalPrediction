---
editor: 
  markdown: 
      wrap: sentence
title: Breast Cancer Survival Prediction
code-fold: true
code-tools: true


format:
  html:
    theme: morph

warning: false
---

::: {style="display:inline-block;"}
<link href="https://cdn-icons-png.flaticon.com/128/3916/3916763.png" rel="icon">

<h6 style="text-align:center">

Alejandro Silva Rodriguez

</h6>

::: {style="text-align:center"}
<a href="https://alexsilvaa9.github.io" title="Visit my Portfolio" target="_blank" class="btn btn-light" style="margin: 10px; padding: 7;"> <img src="https://cdn-icons-png.flaticon.com/512/10013/10013417.png" alt="LinkedIn" class="icon-link" width="30" height="30"/> </a> <a href="https://github.com/AlexSilvaa9" title="Viisit my GitHub" target="_blank" class="btn btn-light" style="margin: 10px; padding: 7;"> <img src="https://cdn-icons-png.flaticon.com/512/25/25231.png" alt="GitHub" class="icon-link" width="25" height="25"/> </a> <a href="https://www.linkedin.com/in/alejandro-silva-rodr%C3%ADguez-133293257/" title="Visit my LinkedIn" target="_blank"class="btn btn-light" style="margin: 10px; padding: 7;"> <img src="https://cdn-icons-png.flaticon.com/512/61/61109.png" alt="LinkedIn" class="icon-link" width="25" height="25"/> </a>
:::
:::

```{=html}
<style>
  
    .fullcontent{
      background-color: rgb(226, 231, 236);
      text-align: justify;  
    }
   
    p{
    color:black;
    margin: 0;
    padding: 0;
    }
    .icon-link {
        text-aling:center;
    }

    .icon-link:hover {
        opacity: 0.4;
    }
    a {
        text-decoration: none;
    }
    a:hover {
        text-decoration: none;
    }
 
/* Hacer que los encabezados h1 y h2 sean sticky */
h1 {
  position: -webkit-sticky; 
  position: sticky;
  top: 20px;
  display:inline-block;
 

  z-index: 1000; /* Z-index para asegurar que los encabezados sean visibles */
  padding: 10px; /* Espacio interior alrededor del encabezado */
}


</style>
```
```{=html}

<script>
  window.onscroll = function() {
    scrollFunction();
  };

  function scrollFunction() {
    if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
      document.getElementById("scrollToTopContainer").style.display = "block";
    } else {
      document.getElementById("scrollToTopContainer").style.display = "none";
    }
  }

  function goToTop() {
    document.body.scrollTop = 0;
    document.documentElement.scrollTop = 0;
  }
</script>
```
::: index
# Index

-   [1. Introduction](#introduction)
-   [2. Data Cleaning](#data-cleaning)
-   [3. Statistic Analysis](#statistic-analysis)
-   [4. Variable selection](#variable-selection)
-   [5. Models](#models)
-   [6. Models comparison](#models-comparison)
-   [7, Final Model](#final-model)
-   [8. Deployment](#deployment)
-   [9. Conclusions](#conclusions)

:::

```{r global.options, include = TRUE}
knitr::opts_chunk$set(
    fig.align   = 'center', # how to align graphics in the final doc. 'left', 'right', 'center'
    warning     = FALSE)    # if FALSE knitr will not display any warning messages in the final document
```

```{r, echo=FALSE,warning=FALSE,results='hide'}
library(readxl)
library(ggplot2)
library(tidyverse)
library(dplyr)
library(ggthemes)
library(gridExtra)
source("../analisis_automatico.R")
library(kableExtra)
library(broom)
library(MASS)
library(reshape2)
library(ROCR)
library(caret)
library(purrr)
library(tidyr)
library(e1071)
library(nnet)
library(pROC)
library(rpart)
library(rpart.plot)
library(class)
library(knitr)
library(dplyr)
library(ROSE)
library(randomForest)
library(mboost)
library(mice)
```
# Introduction {#introduction}

The aim of this report is to develop a predictive model to estimate the survival of breast cancer patients. 
This feature can boost the efficiency of oncology treatments and drastically improve the wellbeing of patients.
Using a dataset including both clinical and demographic data, we are going to analyze the relations between the variables in order to build a valuable predictor.
On top of that, we are going to carry out a model comparison to select the most accurate one and deploy it in a Shiny app.

# Methodology

This study aims to develop predictive models for breast cancer. The dataset includes 500 patients with variables such as age at diagnosis, date at diagnosis, menopausal status, type of surgery, histology, tumor size, grade, ER, PR, HER2, Ki67, IHC phenotype, lymph nodes, stage, and current status.

The methodology involves:

1. **Data Preprocessing:** Exploratory data analysis will be conducted to understand data distributions and identify outliers. Missing data will be handled through imputation techniques.

2. **Model Development:** Various machine learning algorithms such as logistic regression, decision trees, random forests will be employed to develop predictive models. Feature engineering and selection techniques will be utilized to improve model performance.

3. **Model Evaluation:** The models will be evaluated using 5x2 cross-validation to ensure robustness and generalizability. Performance metrics such as AUC-ROC, precision, recall, and F1-score will be used to assess model performance.

4. **Variable Selection:** Different variable selection methods including filtering, wrapper, and embedded methods will be explored to identify the most important predictors for the event of interest.

5. **Model Comparison:** The performance of different models will be compared based on evaluation metrics to select the best-performing model.

6. **Deployment:** The final selected model will be deployed in an application framework for practical use.


##### Dataset description:

```{r,results='asis'}
# Create data for the table
# Crear los datos para la tabla
column <- c("EDAD_DCO", "F.DIAG", "MENOP", "CIRUGIA", "HISTOLOGIA", "TAM", 
            "GRADO", "RE", "RP", "HER2", "KI67", "FENOTIPO", "GANGLIOS", "ESTADIO", "ESTADO_ACTUAL")

description <- c("Patient age at diagnosis in years",
                 "Date of diagnosis",
                 "Menopausal status",
                 "Surgery type",
                 "Histology type",
                 "Tumor size (cm)",
                 "Tumor grade (1 to 3)",
                 "Estrogen receptors percentage",
                 "Progesterone receptors percentage",
                 "Her2 expression (Positive/Negative)",
                 "Ki-67 index (percentage)",
                 "Phenotype determined by PAM50",
                 "Number of affected lymph nodes",
                 "Disease stage (from T1 to T4)",
                 "Current status of the patient")

# Crear el dataframe
datos <- data.frame(Column = column, Description = description)
# Generar la tabla HTML con kable y aplicar estilos con kableExtra
styled_table <- datos %>%
  kable(format = "html", align = "c", caption = "Dataset Description") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE)

# Imprimir la tabla estilizada
print(styled_table)


```



# Data Cleaning {#data-cleaning}

The first step is to import and clean the data.

Some of the most important data transformation are:

<br>

#### Age at diagnosis stratification (EDAD_DCO)

The age at diagnosis variable is stratified into different age cohorts to reflect distinct clinical and pathological differences observed across age groups. This detailed age stratification allows for a more accurate analysis of prognosis and treatment outcomes, highlighting the need for tailored treatment strategies based on age.


In a study by Jackson, E. B., Gondara, L., Speers, C.(2023) 24,469 breast cancer patients were stratified into age cohorts: <35, 35–39, 40–49, 50–59, 60–69, 70–79, and ≥80. This stratification was chosen to reflect distinct clinical and pathological differences observed across age groups. Younger patients (<40) typically present with more aggressive disease and require more intensive treatments, while elderly patients (≥80) often have less aggressive, hormone-sensitive cancers and receive less aggressive treatments. This detailed age stratification allowed for a more accurate analysis of prognosis and treatment outcomes, highlighting the need for tailored treatment strategies based on age.

<br>

This stratification is common in scientific studies and clinical practice, as it provides valuable insights into the impact of age on breast cancer prognosis and treatment outcomes. We are going to modify the stratification, setting the following age groups: 0-39, 40-49, 50-59, 60-69, 70-79, and ≥80 because of the fact that the dataset is not big enough to have a more detailed stratification.

```{r}
#Importamos los datos
data <- read_excel("exam_dataset_23.xlsx", sheet = 1) # Lee la primera hoja


data$EDAD_DCO <- cut(data$EDAD_DCO, breaks = c(-Inf, 39, 49, 59,69,79, Inf), labels = c("0-39", "40-49", "50-59", "60-69","70-79","≥80"))
data$EDAD_DCO <- as.factor(data$EDAD_DCO)

# F.diag codificado por decadas solo hay dos
data <- data %>%
  mutate(F.DIAG = cut(year(F.DIAG), 
                      breaks = seq(2000, 2020, by = 10), 
                      labels = c("2000-2009", "2010-2019"), 
                      right = FALSE))
data$F.DIAG <- as.factor(data$F.DIAG)
data$MENOP <- data$MENOP |> as.factor()
data$CIRUGIA <- data$CIRUGIA |> as.factor()
data$GRADO <- data$GRADO |> as.factor()
data$HER2 <- data$HER2 |> as.factor()
data$ESTADO_ACTUAL <- data$ESTADO_ACTUAL |> as.factor()
data$ESTADIO <- data$ESTADIO |> as.factor()
data$FENOTIPO <- data$FENOTIPO |> as.factor()

data <- data %>%
  mutate(ESTADO_ACTUAL = fct_recode(ESTADO_ACTUAL,
                                    "Exitus" = "MCE",
                                    "Exitus" = "MSE",
                                    "Exitus" = "MXE",
                                    "Vivo" = "VCE",
                                    "Vivo" = "VSE"))
# Reagrupar las categorías de ESTADIO en I, II, III, IV
data <- data %>%
  mutate(ESTADIO = case_when(
    ESTADIO %in% c("I", "IA", "IB") ~ "I",
    ESTADIO %in% c("II", "IIA", "IIB") ~ "II",
    ESTADIO %in% c("III", "IIIA", "IIIB", "IIIC") ~ "III",
    ESTADIO %in% c("IV") ~ "IV",
    TRUE ~ NA_character_  # Para manejar cualquier otro valor o NA
  ))
data$ESTADIO <- as.factor(data$ESTADIO)
```
#### Number of affected lymph nodes (GANGLIOS)

The variable **GANGLIOS (N)** refers to the involvement of lymph nodes, indicating whether cancer has spread to them and, if so, how many are affected. Here's the interpretation of the different **N** levels according to gepac (2024):

- **N0**: No lymph nodes affected.
- **N1**: One to three lymph nodes affected.
- **N2**: Four to nine lymph nodes affected.
- **N3**: Ten or more lymph nodes affected, or affected nodes that are distant from the breast.


```{r}

data$GANGLIOS <- cut(data$GANGLIOS, breaks = c(-Inf,0, 3, 9, Inf), labels = c("N0","N1", "N2", "N3"))
```
#### Histology groups (HISTOLOGIA)

The histology of breast cancer refers to the type of cells that make up the tumor. The most common special histological types of breast cancer include ductal, lobular, and other types such as adenoid, colloid, medullary, mixed, mucinous, papillary, phyllodes, and tubular. Ductal carcinoma in situ (DCIS) and invasive ductal carcinoma (IDC) are the most common types of breast cancer, accounting for the majority of newly diagnosed cases. Nascimento and Otoni (2020)

We are going to group the histological types into three broad categories: ductal, lobular, and other types. This grouping will help simplify the analysis and interpretation of the data.



```{r}

# Define the broad categories and map existing values to these categories
data$HISTOLOGIA[data$HISTOLOGIA %in% c("DCIS", "DUCTAL")] <- "DUCTAL"
data$HISTOLOGIA[data$HISTOLOGIA %in% c("LOBULILLAR")] <- "LOBULAR"
data$HISTOLOGIA[data$HISTOLOGIA %in% c("ADENOIDE", "COLOIDE", "MEDULAR", "MIXTO", "MUCINOSO", "PAPILAR", "PHILOIDES", "TUBULAR")] <- "OTHER"

data$HISTOLOGIA <- as.factor(data$HISTOLOGIA)
```
#### Data imputation and normalization

-   Numeric variables normalization
```{r}
# Select only numeric variables
numeric_vars <- sapply(data, is.numeric)
numeric_data <- data[, numeric_vars]

# Normalize the numeric variables
normalized_data <- as.data.frame(scale(numeric_data))

# Combine normalized numeric variables with non-numeric variables
data <- cbind(data[, !numeric_vars], normalized_data)

```

-   Numerical variables imputation by mean.
```{r}

# Identify numeric variables
numeric_vars <- sapply(data, is.numeric)
# Impute missing values in numeric variables using the mean
for (var in names(data)[numeric_vars]) {
  mean_value <- mean(data[[var]], na.rm = TRUE) # Calculate the mean excluding NA values
  data[[var]][is.na(data[[var]])] <- mean_value # Replace NA values with the mean
}
# View the summary of the updated data

```

-   Impute categorical variables by mode:

```{r}


# Función para calcular la moda de un vector
Mode <- function(x) {
  if (length(unique(x)) == 1) {
    return(unique(x))
  } else {
    tbl <- table(x)
    return(names(tbl)[which.max(tbl)])
  }
}

# Obtener nombres de columnas categóricas
columnas_categoricas <- sapply(data, is.factor)

# Imputación por moda para todas las columnas categóricas con valores perdidos
data[columnas_categoricas] <- lapply(data[columnas_categoricas], function(x) {
  moda <- Mode(x)
  x[is.na(x)] <- moda
  return(x)
})
```




## Summary 

::: {.panel-tabset style="background-color: #f0f0f0;box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);border-radius: 5px;"}

Claro, aquí tienes el código para cada una de las variables que mencionaste:
```{r}

```

### EDAD_DCO

```{r}
summary(data$EDAD_DCO)
```

### F,DIAG

```{r}
summary(data$F.DIAG)
```

### MENOP

```{r}
summary(data$MENOP)
```

### CIRUGIA

```{r}
summary(data$CIRUGIA)
```

### HISTOLOGIA

```{r}
summary(data$HISTOLOGIA)
```

### GRADO

```{r}
summary(data$GRADO)
```

### HER2

```{r}
summary(data$HER2)
```

### FENOTIPO

```{r}
summary(data$FENOTIPO)
```

### GANGLIOS

```{r}
summary(data$GANGLIOS)
```

### ESTADIO

```{r}
summary(data$ESTADIO)
```

### ESTADO_ACTUAL

```{r}
summary(data$ESTADO_ACTUAL)
```

### TAM

```{r}
summary(data$TAM)
```

### RE

```{r}
summary(data$RE)
```

### RP

```{r}
summary(data$RP)
```

### KI67

```{r}
summary(data$KI67)
```
:::

# Statistic Analysis {#statistic-analysis}
::: {.panel-tabset style="background-color: #f0f0f0;box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);border-radius: 5px;"}

## Univariate Analysis {#univariate-analysis}

We are going to analyze and visualize every variable so as to understand the behavior of our problem variables.

```{r, warning=FALSE,results='hide'}
columnas <- colnames(data)

# Aplicar la función 'analsis_univariante_automatico()' a cada columna
resultados_univariante <- lapply(columnas, function(col) {
  an <- analsis_univariante_automatico(data[[col]], col)
  if(an$tipo=="factor"){
    o <- an$grafica
  }else if(an$tipo=="numeric"){
    o <- an$hist
  }
  return(o)
})
```

```{r setup, include=FALSE}
library(gridExtra)
library(ggplot2)

# Crear una carpeta para guardar las imágenes si no existe
if (!dir.exists("images")) {
  dir.create("images")
}

# Suponiendo que 'data' es tu data frame, que ya has cargado anteriormente
# Ejemplo:
# data <- mtcars

# Supongamos que 'analsis_univariante_automatico' es una función definida previamente
# y que toma una columna de datos y su nombre como argumentos

# columnas <- colnames(data)

# Aplicar la función 'analsis_univariante_automatico()' a cada columna
resultados_univariante <- lapply(columnas, function(col) {
  an <- analsis_univariante_automatico(data[[col]], col)
  if(an$tipo == "factor"){
    o <- an$grafica
  } else if(an$tipo == "numeric"){
    o <- an$hist
  }
  return(o)
})

# Si hay más de una imagen, guardarlas con nombres diferentes
for (i in 1:length(resultados_univariante)) {
  ggsave(filename = paste0("images/image", i, ".png"), plot = resultados_univariante[[i]])
}
```

```{=html}
<!-- Div para la imagen principal -->
<div id="main-image">
  <img src="images/image1.png" alt="Imagen Principal" id="displayed-image">
</div>
<!-- Div para las miniaturas -->
<div class="thumbnail-gallery">
  ```{r, results='asis', echo=FALSE}
  for (i in 1:length(resultados_univariante)) {
    cat(sprintf('<img src="images/image%d.png" alt="Imagen %d" onclick="changeImage(\'images/image%d.png\')">', i, i, i))
  }
  ```
</div>
<!-- JavaScript para cambiar la imagen principal -->
<script>
function changeImage(imageSrc) {
  document.getElementById('displayed-image').src = imageSrc;
}
</script>
<!-- CSS para estilizar la galería -->
<style>
#main-image {
  text-align: center;
  margin-bottom: 20px;
}

#main-image img {
  width: 600px;
  height: auto;
  border: 2px solid #ddd;
  border-radius: 5px;
}

.thumbnail-gallery {
  display: flex;
  flex-wrap: wrap;
  justify-content: center;
  gap: 10px;
  max-width: 100%;
  overflow-x: auto;
}

.thumbnail-gallery img {
  width: 50px; /* Reducir el tamaño de las miniaturas */
  height: auto;
  cursor: pointer;
  border: 2px solid #ddd;
  border-radius: 5px;
  transition: transform 0.2s;
}

.thumbnail-gallery img:hover {
  transform: scale(1.1);
  border-color: #aaa;
}
</style>
```

The most notable problem is the great imbalance between categories in variables like ESTADO_ACTUAL or HISTOLOGIA.

There is a decrease in the frecuency of the factor with worse prognosis in some variables like GANGLIOS OR ESTADIO that added to the fact that there are a low percentage of exitus could mean that most of the patients have a good prognosis.



## Bivariate Analysis {#bivariate-analysis}

In this section, we are going to measure and visualize the relation of every variable with the target.
This is very useful to select the most important variables for our model.

```{r,warning=FALSE,results='asis'}
resultados_bi_ESTADO_ACTUAL <- lapply(columnas, function(col) {
  an <- analisis_bivariante_automatico(data$ESTADO_ACTUAL, data[[col]], "ESTADO_ACTUAL", col)
  return(an$grafica)
})

# Guardar las imágenes generadas en una carpeta
if (!dir.exists("images_bi")) {
  dir.create("images_bi")
}
for (i in 1:length(resultados_bi_ESTADO_ACTUAL)) {
  ggsave(filename = paste0("images_bi/image", i, ".png"), plot = resultados_bi_ESTADO_ACTUAL[[i]])
}
```


```{=html}
<!-- Div para la imagen principal -->
<div id="main-image_bi">
  <img src="images_bi/image1.png" alt="Imagen Principal" id="displayed-image_bi">
</div>
<!-- Div para las miniaturas -->
<div class="thumbnail-gallery">
  ```{r, results='asis', echo=FALSE}
  for (i in 1:length(resultados_bi_ESTADO_ACTUAL)) {
    cat(sprintf('<img src="images_bi/image%d.png" alt="Imagen %d" onclick="changeImage_bi(\'images_bi/image%d.png\')">', i, i, i))
  }
  ```
</div>
<script>
function changeImage_bi(imageSrc) {
  document.getElementById('displayed-image_bi').src = imageSrc;
}
</script>
<style>
#main-image_bi {
  text-align: center;
  margin-bottom: 20px;
}

#main-image_bi img {
  width: 600px;
  height: auto;
  border: 2px solid #ddd;
  border-radius: 5px;
}
</style>
```

```{r,warning=FALSE,results='asis'}




resultados_para_tabla <- lapply(columnas, function(col) {
  an <- analisis_bivariante_automatico(data$ESTADO_ACTUAL, data[[col]],"ESTADO_ACTUAL",col)
  
  return(an)
  })
p_values_1 <- sapply(resultados_para_tabla, function(x) x$test$p.value)
formatted_p_values_1 <- ifelse(p_values_1 < 0.0000001, format(p_values_1, scientific = TRUE), p_values_1)

# Crear una tabla con los resultados
tabla_resultados <- data.frame(
  Data_Name = sapply(resultados_para_tabla, function(x) x$test$data.name),
  Estadistico = sapply(resultados_para_tabla, function(x) ifelse(is.null(x$test$statistic), NA, x$test$statistic)),
  Odds = sapply(resultados_para_tabla, function(x) ifelse(is.null(x$test$estimate), NA, x$test$estimate)),
  P_valor = formatted_p_values_1,
  Method = sapply(resultados_para_tabla, function(x) x$test$method),
  Conclusion = sapply(resultados_para_tabla, function(x) x$conclusion)
)

# Mostrar la tabla usando kable
tabla <- kable(tabla_resultados, format = "html", align = "ccc", caption = "Tabla de Resultados", row.names = FALSE, booktabs = TRUE) %>%
  kable_styling(bootstrap_options = "striped", full_width = FALSE)

tabla_con_scroll <- scroll_box(tabla, height = "400px")

# Mostrar la tabla
print(tabla_con_scroll)
```
The target variable is dependent to diagnosis date, phenotype, stage and estrogen receptors. These variables are the most important for our model.
This matches with the published scientific evidence.

## Association Analysis {#association-analysis}

To recollect even more information, we are going to build a association matrix where we can see p-values(meaning of association) of every variable with each others

```{r}
library(ggplot2)
library(reshape2)

# Definir la función para calcular p-valores y generar un mapa de calor
generar_mapa_calor_p_valores <- function(data, analisis_bivariante_automatico) {
  num_variables <- ncol(data)
  p_valores <- matrix(NA, nrow = num_variables, ncol = num_variables)
  
  # Función para calcular los p-valores entre dos variables
  calcular_p_valores <- function(i, j) {
    an <- analisis_bivariante_automatico(data[, i], data[, j], colnames(data)[i], colnames(data)[j])
    return(an$test$p.value)
  }
  
  # Calcular los p-valores para todas las combinaciones de variables utilizando lapply
  indices <- expand.grid(1:num_variables, 1:num_variables)
  p_valores <- matrix(unlist(lapply(1:nrow(indices), function(idx) {
    calcular_p_valores(indices[idx, 1], indices[idx, 2])
  })), nrow = num_variables, byrow = TRUE)
  
  # Agregar nombres de columnas y filas para identificar las variables
  colnames(p_valores) <- colnames(data)
  rownames(p_valores) <- colnames(data)
  
  # Convertir la matriz en un data frame largo
  p_valores_long <- melt(p_valores, varnames = c("Variable1", "Variable2"), value.name = "p_value")
  
  # Crear el mapa de calor usando ggplot2
  ggplot(data = p_valores_long, aes(x = Variable1, y = Variable2, fill = p_value)) +
    geom_tile() +
    scale_fill_gradient(low = "lightblue", high = "#7469B6") +
    labs(title = "Heat Map of p-values", x = "Variables", y = "Variables") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}

# Ejemplo de uso con un conjunto de datos llamado 'data'
# Asegúrate de definir la función 'analisis_bivariante_automatico' antes de usar esta función
generar_mapa_calor_p_valores(data, analisis_bivariante_automatico)


```

The association results presented in the table depict the strength and significance of relationships between various variables in the study.
Each value in the table represents a measure of association, such as p-values, reflecting the level of dependency or independence between pairs of variables.
Upon reviewing the table, several notable associations emerge. For instance, the variables **ESTADO_ACTUAL** and **FENOTIPO** exhibit a strong association, with a p-value of 0.0000001, indicating a significant relationship between the two variables. Similarly, the variables **ESTADO_ACTUAL** and **ESTADIO** also demonstrate a significant association, with a p-value of 0.0000001, suggesting a strong relationship between disease stage and patient status. These findings provide valuable insights into the interplay between different variables and their impact on patient outcomes.

## Variable imbalance {#variable-unbalance}

In this section, we are going to analyze the imbalance in the target variable and the effect of this imbalance on the model performance.

```{r}
# Supongamos que data$ESTADO_ACTUAL es la variable objetivo
frecuencia_clases <- table(data$ESTADO_ACTUAL)
porcentaje_clases <- prop.table(frecuencia_clases) * 100

# Crear un data frame con los porcentajes
df_porcentaje <- as.data.frame(porcentaje_clases)
colnames(df_porcentaje) <- c("Clase", "Porcentaje")

# Mostrar la tabla usando kable
kable(df_porcentaje, col.names = c("Class", "Percentage (%)"), caption = "Target imbalance")
```
The fact that the target is completely unbalanced can lead to a model that is not able to predict the minority class, this is a common problem in classification models and we are going to address it in the next sections.
:::


# Variable selection {#variable-selection}

Variable selection is a crucial step in building predictive models, helping to identify the most relevant features for the target prediction. There are three main methods for variable selection: filter methods, wrapper methods, and embedded methods. In this section, we will explore these methods to select the most important variables for our predictive model.

::: {.panel-tabset style="background-color: #f0f0f0;box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);border-radius: 5px;"}

## Filter methods {#filter-methods}

Filter methods select important variables before the learning algorithm is applied. These methods are independent of the learning algorithm and are based on intrinsic characteristics of the data, such as correlation, mutual information, or variance. We are going to use the p-value of the adecuated test to select the most important variables in terms of dependence with the target variable.

Here is an example of a filter with all the data, remember that we have to do this process in every fold of the cross validation to avoid data leakage.
```{r}
filter_dependency <- function(data, dependent_variable, columns, significance_level = 0.05) {
  # Realizar el análisis bivariante y obtener los resultados
  resultados_para_tabla <- lapply(columns, function(col) {
    analisis_bivariante_automatico(data[[dependent_variable]], data[[col]], dependent_variable, col)
  })
  
  # Obtener los p-values de las pruebas bivariadas
  p_values <- sapply(resultados_para_tabla, function(x) x$test$p.value)
  
  # Seleccionar las variables con p-value por debajo del umbral
  columnas_seleccionadas <- columns[p_values < significance_level]
  
  # Filtrar el conjunto de datos para incluir solo las columnas seleccionadas y la columna dependiente
  data_filtrada <- data[, c(dependent_variable, columnas_seleccionadas)]
  
  # Retornar el conjunto de datos filtrado
  return(data_filtrada)
}
filtered_data <- filter_dependency(data, "ESTADO_ACTUAL", columnas)
if ("ESTADO_ACTUAL.1" %in% colnames(filtered_data)) {
    filtered_data <- filtered_data[, !colnames(filtered_data) %in% "ESTADO_ACTUAL.1", drop = FALSE]
  }
summary(filtered_data)


```
## Wrapper methods {#wrapper-methods}

Wrapper methods use a learning algorithm to evaluate the importance of variables. In each iteration, a different subset of variables is evaluated, and the set that provides the best model performance is selected. These methods are typically more accurate than filter methods but are also more computationally expensive.

A common example of a wrapper method is forward selection or backward elimination. These methods test different combinations of variables and select those that improve model performance.

In the next sections, we will implement a wrapper method to select the best variables with brute force. Basically, we are going to iterate over all the possible combinations of variables and select the one that provides the best model performance.

## Embedded methods {#embedded-methods}

Embedded methods perform variable selection during the training process of the model. These methods are specific to the learning algorithm and are built into the model. For example, decision trees and random forests have built-in feature selection capabilities based on the importance of variables in the model.

We are going to select the most important variables using the Random Forest algorithm, which provides a measure of variable importance based on the Mean Decrease Gini index.

Example of embedded method:

```{r}
seleccionar_variables_importantes <- function(data, target, top_n = 10, seed = 123) {
  # Asegurarse de que el target esté en el data frame
  if (!target %in% names(data)) {
    stop("La variable objetivo no está en el conjunto de datos.")
  }
  
  # Convertir la variable target a un factor si no lo es
  if (!is.factor(data[[target]])) {
    data[[target]] <- as.factor(data[[target]])
  }
  
  # Definir la fórmula del modelo
  formula <- as.formula(paste(target, "~ ."))
  
  # Entrenar el modelo de Random Forest
  set.seed(seed)
  modelo_rf <- randomForest(formula, data = data, importance = TRUE)
  
  # Extraer la importancia de las variables
  importancia <- importance(modelo_rf)
  
  # Convertir a un dataframe para facilidad de manejo
  importancia_df <- as.data.frame(importancia)
  importancia_df$variable <- rownames(importancia_df)
  
  # Ordenar por la importancia de MeanDecreaseGini
  importancia_df <- importancia_df[order(-importancia_df$MeanDecreaseGini), ]
  
  # Seleccionar las 'top_n' variables más importantes
  variables_seleccionadas <- importancia_df$variable[1:top_n]
  
  return(variables_seleccionadas)
}
variables_importantes <- seleccionar_variables_importantes(data, "ESTADO_ACTUAL", top_n = 4)
filtered_data <- data[,c("ESTADO_ACTUAL",variables_importantes)]
summary(filtered_data)

```
:::

# Models

The aim of this section is to build a predictive model to estimate the survival chance. We are going to compare different models and select the one that provides the best performance based on the evaluation metrics. On top of that, we are going to compare the different variable selection methods to determine the most effective approach. 

For each model, we will evaluate the performance using a hold-out validation and a 5x2 cross-validation to ensure the honesty of the results, it is a good compromise between computational cost and robustness of the results.

## First Aproach

In this section we are going to build a logistic regression model using all the data aiming to see if the model is able to predict the target variable with a good accuracy.

```{r}
# Ajustar el modelo logístico
modelo_primero <- glm(formula= ESTADO_ACTUAL ~ ., data = data, family = binomial("logit"))

# Predecir en los datos de validación
predicciones <- predict(modelo_primero, newdata = data, type = "response")
```
::: {.panel-tabset}

### Predictions

```{r}
# Calcular las predicciones para el conjunto de test
clases_predichas <- ifelse(predicciones>= 0.8, "Vivo", "Exitus")


mostrar_metricas(data$ESTADO_ACTUAL,clases_predichas)
```

### Roc curve and auc

```{r}
# Calcular la curva ROC y el AUC para el conjunto de test
plot_roc_curve(predicciones,data$ESTADO_ACTUAL)
```
At first sight the model seems to be accurate, the metrics are too optimistic though.
:::

::: {.panel-tabset style="background-color: rgb(226, 231, 236);box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);border-radius: 5px;"}

# Logistic Regression {#logistic-regression}

::: {.panel-tabset style="background-color: #f0f0f0;box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);border-radius: 5px;"}

## Hold Out {#hold-out}

In every model, we are going to over sample the training data to avoid the imbalance in the target variable, since in the last activity we corroborated the eficacy of this method.

We are going to build a logistic regression model using the best variables selected by the wrapper cause it is the most accurate method.

```{r,message=FALSE, warning=FALSE}

start_time <- Sys.time()
#parameters to change:
proportion <- 0.91

# Dividir el conjunto de datos en entrenamiento y test
indices_entrenamiento <- sample(nrow(data), 0.75 * nrow(data))  # 75% para entrenamiento
datos_entrenamiento <- data[indices_entrenamiento, ]
datos_test <- data[-indices_entrenamiento, ]

# Dividir datos de entrenamiento en entrenamiento y validación
train_indices <- sample(nrow(datos_entrenamiento), 0.8 * nrow(datos_entrenamiento))  # 80% para entrenamiento
datos_entrenamiento_split <- datos_entrenamiento[train_indices, ]
datos_validacion <- datos_entrenamiento[-train_indices, ]
```

```{r,eval=FALSE}
datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ .,data=datos_entrenamiento_split,p =proportion)$data

# Establecer la fórmula base para el modelo logístico
formula_base <- formula(ESTADO_ACTUAL ~ 1)  # Fórmula inicial sin predictoras

# Obtener nombres de variables predictoras
variables <- names(datos_entrenamiento_oversampled)[-which(names(datos_entrenamiento_oversampled) == "ESTADO_ACTUAL")]

# Obtener todas las combinaciones posibles de variables predictoras
combinaciones_variables <- unlist(lapply(1:length(variables), function(x) combn(variables, x, simplify = FALSE)), recursive = FALSE)

# Establecer variables para almacenar el mejor modelo y su AUC
mejor_modelo <- NULL
mejor_auc <- 0

# Iterar sobre todas las combinaciones de variables
for (combinacion in combinaciones_variables) {
  # Construir la fórmula con las variables actuales
  formula_actual <- reformulate(combinacion, response = "ESTADO_ACTUAL")
  
  # Ajustar el modelo logístico
  modelo_actual <- glm(formula_actual, data = datos_entrenamiento_split, family = binomial("logit"))
  
  # Predecir en los datos de validación
  predicciones_validacion <- predict(modelo_actual, newdata = datos_validacion, type = "response")
  
  # Calcular el AUC para la combinación actual
  auc_actual <- pROC::roc(datos_validacion$ESTADO_ACTUAL, predicciones_validacion)$auc
  
  # Actualizar el mejor modelo si se encuentra uno con un AUC mayor
  if (auc_actual > mejor_auc) {
    mejor_auc <- auc_actual
    mejor_modelo <- modelo_actual
  }
}
modelo_primero <- mejor_modelo

end_time <- Sys.time()
execution_time <- end_time - start_time
execution_time_p <- as.numeric(execution_time, units = "secs")
save("modelo_primero", file = "modelo_primero.RData")
save("execution_time_p", file = "execution_time_p.RData")
```

```{r}
load("modelo_primero.RData")
load("execution_time_p.RData")
cat("Execution time:", execution_time_p, "seconds \n")


```

::: {.panel-tabset}

### Predictions

```{r}
# Calcular las predicciones para el conjunto de test
predicciones_test <- predict(modelo_primero, newdata = datos_test, type = "response")
clases_predichas_test <- ifelse(predicciones_test >= 0.85, "Vivo", "Exitus")


mostrar_metricas(datos_test$ESTADO_ACTUAL,clases_predichas_test)
```

### Roc curve and auc

```{r}
# Calcular la curva ROC y el AUC para el conjunto de test
plot_roc_curve(predicciones_test,datos_test$ESTADO_ACTUAL)
```
:::

The performance of the model is acceptable but it seems that it is not robust at all.

## Hold Out Oversampling {#hold-out}

In every model, we are going to over sample the training data to avoid the imbalance in the target variable, since in the last activity we corroborated the eficacy of this method.

We are going to build a logistic regression model using the best variables selected by the wrapper cause it is the most accurate method.

```{r,message=FALSE, warning=FALSE,eval=FALSE}

start_time <- Sys.time()
#parameters to change:
proportion <- 0.95

# Dividir el conjunto de datos en entrenamiento y test
indices_entrenamiento <- sample(nrow(data), 0.75 * nrow(data))  # 75% para entrenamiento
datos_entrenamiento <- data[indices_entrenamiento, ]
datos_test <- data[-indices_entrenamiento, ]

# Dividir datos de entrenamiento en entrenamiento y validación
train_indices <- sample(nrow(datos_entrenamiento), 0.8 * nrow(datos_entrenamiento))  # 80% para entrenamiento
datos_entrenamiento_split <- datos_entrenamiento[train_indices, ]
datos_validacion <- datos_entrenamiento[-train_indices, ]

datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ .,data=datos_entrenamiento_split,p =proportion)$data

# Establecer la fórmula base para el modelo logístico
formula_base <- formula(ESTADO_ACTUAL ~ 1)  # Fórmula inicial sin predictoras

# Obtener nombres de variables predictoras
variables <- names(datos_entrenamiento_oversampled)[-which(names(datos_entrenamiento_oversampled) == "ESTADO_ACTUAL")]

# Obtener todas las combinaciones posibles de variables predictoras
combinaciones_variables <- unlist(lapply(1:length(variables), function(x) combn(variables, x, simplify = FALSE)), recursive = FALSE)

# Establecer variables para almacenar el mejor modelo y su AUC
mejor_modelo <- NULL
mejor_auc <- 0

# Iterar sobre todas las combinaciones de variables
for (combinacion in combinaciones_variables) {
  # Construir la fórmula con las variables actuales
  formula_actual <- reformulate(combinacion, response = "ESTADO_ACTUAL")
  
  # Ajustar el modelo logístico
  modelo_actual <- glm(formula_actual, data = datos_entrenamiento_oversampled, family = binomial("logit"))
  
  # Predecir en los datos de validación
  predicciones_validacion <- predict(modelo_actual, newdata = datos_validacion, type = "response")
  
  # Calcular el AUC para la combinación actual
  auc_actual <- pROC::roc(datos_validacion$ESTADO_ACTUAL, predicciones_validacion)$auc
  
  # Actualizar el mejor modelo si se encuentra uno con un AUC mayor
  if (auc_actual > mejor_auc) {
    mejor_auc <- auc_actual
    mejor_modelo <- modelo_actual
  }
}

summary(mejor_modelo)

end_time <- Sys.time()
execution_time <- end_time - start_time
execution_time <- as.numeric(execution_time, units = "secs")
save("mejor_modelo", file = "mejor_modelo.RData")
save("execution_time", file = "execution_time.RData")
```

```{r}
load("mejor_modelo.RData")
load("execution_time.RData")
cat("Execution time:", execution_time, "seconds \n")


```

::: {.panel-tabset}

### Predictions

```{r}
# Calcular las predicciones para el conjunto de test
predicciones_test <- predict(mejor_modelo, newdata = datos_test, type = "response")
clases_predichas_test <- ifelse(predicciones_test >= 0.5, "Vivo", "Exitus")


mostrar_metricas(datos_test$ESTADO_ACTUAL,clases_predichas_test)
```

### Roc curve and auc

```{r}
# Calcular la curva ROC y el AUC para el conjunto de test
plot_roc_curve(predicciones_test,datos_test$ESTADO_ACTUAL)
```
:::

The performance of the model is acceptable but it seems that it is not robust at all.
```{r}
knitr::opts_chunk$set(eval = FALSE)
```

## 5x2 Cross Validation Filter 

We are going to perform a 5x2 cross validation to ensure the robustness of the model.
First, we create the outer folds and then we iterate over them to create the inner folds and perform the model selection process.


```{r,warning=FALSE,message=FALSE}
start_time <- Sys.time()


# Configurar la validación cruzada 5x2
k_outer <- 5  # Número de pliegues externos
k_inner <- 2  # Número de pliegues internos
folds_outer <- createFolds(data$ESTADO_ACTUAL, k = k_outer, list = FALSE)  # Obtener índices de pliegues externos

# Lista para almacenar resultados
resultados_auc <- numeric(k_outer)
resultados_sensibilidad <- numeric(k_outer)
resultados_accuracy <- numeric(k_outer)
indices <- 1:nrow(data)

# Inicializar matrices para almacenar predicciones y objetivos
predicciones_totales <- matrix(NA, nrow = nrow(data), ncol = k_outer)
targets_totales <- numeric(nrow(data))

# Iterar sobre cada pliegue externo y realizar el proceso de modelado
for (i_outer in 1:k_outer) {

  # Obtener los índices para el pliegue externo actual
  test_indices_outer <- indices[folds_outer==i_outer]  # Índices correspondientes al pliegue externo de test
  train_indices_outer <- setdiff(1:nrow(data), test_indices_outer)  # Índices correspondientes al conjunto de entrenamiento
  
  # Dividir datos en entrenamiento y test para el pliegue externo
  datos_entrenamiento_outer <- data[train_indices_outer, ]
  datos_test_outer <- data[test_indices_outer, ]
  

  # Configurar la validación cruzada interna
  folds_inner <- createFolds(datos_entrenamiento_outer$ESTADO_ACTUAL, k = k_inner, list = FALSE)  # Obtener índices de pliegues internos
  
  # Almacenar predicciones y test para formar una curva ROC general
  predicciones_auc <- numeric(nrow(datos_entrenamiento_outer))
  targets_auc <- numeric(nrow(datos_entrenamiento_outer))
  
  # Iterar sobre cada pliegue interno para selección de modelo
  indices_inner <- 1:nrow(datos_entrenamiento_outer)
  resultados_auc_inner <- numeric(k_inner)
  resultados_sensibilidad_inner <- numeric(k_inner)
  resultados_accuracy_inner <- numeric(k_inner)
  for (i_inner in 1:k_inner) {
    filter_data <- filter_dependency(datos_entrenamiento_outer,"ESTADO_ACTUAL",colnames(datos_entrenamiento_outer),0.05)
    if ("ESTADO_ACTUAL.1" %in% colnames(filter_data)) {
      filter_data <- filter_data[, !colnames(filter_data) %in% "ESTADO_ACTUAL.1", drop = FALSE]
    }
    ###############################
    #### Aqui el filtrado##########
    ###############################
#con poca variables solo unaa la vez o filtrado o wrapper
    
    
    
    # Obtener los índices para el pliegue interno actual
    test_indices_inner <- indices_inner[folds_inner==i_inner]  # Índices correspondientes al pliegue interno de test
    train_indices_inner <- setdiff(1:nrow(filter_data), test_indices_inner)  # Índices correspondientes al conjunto de entrenamiento interno
    
    # Dividir datos en entrenamiento y test para el pliegue interno
    datos_entrenamiento_inner <- filter_data[train_indices_inner, ]
    datos_validacion <- filter_data[test_indices_inner, ]
    
 
    # Aplicar oversampling en el conjunto de entrenamiento interno
    datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ .,
                                            data = datos_entrenamiento_inner,
                                            p = 0.93)$data

    
    # Ajustar el modelo logístico
    modelo_actual <- glm("ESTADO_ACTUAL ~ .", data = datos_entrenamiento_oversampled, family = binomial("logit"))
    
    
    
    # Realizar predicciones en el conjunto de entrenamiento externo para el AUC general
    predicciones_auc[test_indices_inner] <- predict(modelo_actual, newdata = datos_validacion, type = "response")
    predicciones_clasificadas <- ifelse(predicciones_auc[test_indices_inner] >= 0.5, "SI", "NO")
  
    resultados_sensibilidad_inner[i_inner] <- sum(predicciones_clasificadas == "SI" & datos_validacion[["ESTADO_ACTUAL"]] == "SI") / sum(datos_validacion[["ESTADO_ACTUAL"]] == "SI")
    resultados_accuracy_inner[i_inner] <- mean(predicciones_clasificadas == datos_validacion[["ESTADO_ACTUAL"]])

    targets_auc[test_indices_inner] <- as.numeric(datos_validacion$ESTADO_ACTUAL) - 1

  }
  
  # Calcular el AUC final para este fold externo
  auc_fold_externo <- pROC::roc(targets_auc, predicciones_auc)$auc
  accuracy_fold_externo <- mean(resultados_accuracy_inner)
  sensibilidad_fold_externo <- mean(resultados_sensibilidad_inner)
 
  
  # Almacenar el AUC del fold externo actual
  resultados_auc[i_outer] <- auc_fold_externo
  resultados_accuracy[i_outer] <- accuracy_fold_externo
  resultados_sensibilidad[i_outer] <- sensibilidad_fold_externo
  
  # Predecir en los datos de test para este pliegue externo y almacenar predicciones
  predicciones_test <- predict(modelo_actual, newdata = datos_test_outer, type = "response")
  predicciones_totales[test_indices_outer, i_outer] <- predicciones_test
  targets_totales[test_indices_outer] <- as.numeric(datos_test_outer$ESTADO_ACTUAL) - 1
 
}

# Calcular AUC promedio
auc_promedio <- mean(resultados_auc)
accuracy_promedio <- mean(resultados_accuracy)
sensibilidad_promedio <- mean(resultados_sensibilidad)


# Crear tabla con todas las métricas
tabla_metricas <- data.frame(
  Pliegue = 1:k_outer,
  AUC = sapply(resultados_auc, round, digits = 3),
  Accuracy = sapply(resultados_accuracy, round, digits = 3),
  Recall = sapply(resultados_sensibilidad, round, digits = 3)
)


# Agregar fila con promedio de métricas
tabla_metricas_logistic_filter <- rbind(tabla_metricas, c("Mean", round(auc_promedio,3), round(accuracy_promedio,3), round(sensibilidad_promedio,3)))

end_time <- Sys.time()
execution_time_logistic_filter <- end_time - start_time
execution_time_logistic_filter <- as.numeric(execution_time_logistic_filter, units = "secs")
cat("Execution time:", execution_time_logistic_filter,  "seconds \n")
```

::: {.panel-tabset}

# Metrics Table
```{r}

# Imprimir tabla formateada con kableExtra
kable(tabla_metricas_logistic_filter, caption = "Cross Validation Metrics", align = "c") %>%
  kable_styling(full_width = FALSE)
```

# Metrics Plot
```{r}
crear_plot_metricas_per_fold(tabla_metricas_logistic_filter)
```

:::

Now we have proved the instability of the model. 

## 5x2 Cross Validation Embedded 


We are going to select the most important variables with a embedded method.
```{r,warning=FALSE,message=FALSE,eval=FALSE}
start_time <- Sys.time()


# Configurar la validación cruzada 5x2
k_outer <- 5  # Número de pliegues externos
k_inner <- 2  # Número de pliegues internos
folds_outer <- createFolds(data$ESTADO_ACTUAL, k = k_outer, list = FALSE)  # Obtener índices de pliegues externos

# Lista para almacenar resultados
resultados_auc <- numeric(k_outer)
resultados_sensibilidad <- numeric(k_outer)
resultados_accuracy <- numeric(k_outer)
indices <- 1:nrow(data)

# Inicializar matrices para almacenar predicciones y objetivos
predicciones_totales <- matrix(NA, nrow = nrow(data), ncol = k_outer)
targets_totales <- numeric(nrow(data))

# Iterar sobre cada pliegue externo y realizar el proceso de modelado
for (i_outer in 1:k_outer) {

  # Obtener los índices para el pliegue externo actual
  test_indices_outer <- indices[folds_outer==i_outer]  # Índices correspondientes al pliegue externo de test
  train_indices_outer <- setdiff(1:nrow(data), test_indices_outer)  # Índices correspondientes al conjunto de entrenamiento
  
  # Dividir datos en entrenamiento y test para el pliegue externo
  datos_entrenamiento_outer <- data[train_indices_outer, ]
  datos_test_outer <- data[test_indices_outer, ]
  

  # Configurar la validación cruzada interna
  folds_inner <- createFolds(datos_entrenamiento_outer$ESTADO_ACTUAL, k = k_inner, list = FALSE)  # Obtener índices de pliegues internos
  
  # Almacenar predicciones y test para formar una curva ROC general
  predicciones_auc <- numeric(nrow(datos_entrenamiento_outer))
  targets_auc <- numeric(nrow(datos_entrenamiento_outer))
  
  # Iterar sobre cada pliegue interno para selección de modelo
  indices_inner <- 1:nrow(datos_entrenamiento_outer)
  resultados_auc_inner <- numeric(k_inner)
  resultados_sensibilidad_inner <- numeric(k_inner)
  resultados_accuracy_inner <- numeric(k_inner)
  for (i_inner in 1:k_inner) {
  
    variables_importantes <- seleccionar_variables_importantes(datos_entrenamiento_outer,"ESTADO_ACTUAL",6)
    filter_data <- datos_entrenamiento_outer[,c("ESTADO_ACTUAL",variables_importantes)]
    ###############################
    #### Aqui el filtrado##########
    ###############################
#con poca variables solo unaa la vez o filtrado o wrapper
    
    
    
    # Obtener los índices para el pliegue interno actual
    test_indices_inner <- indices_inner[folds_inner==i_inner]  # Índices correspondientes al pliegue interno de test
    train_indices_inner <- setdiff(1:nrow(filter_data), test_indices_inner)  # Índices correspondientes al conjunto de entrenamiento interno
    
    # Dividir datos en entrenamiento y test para el pliegue interno
    datos_entrenamiento_inner <- filter_data[train_indices_inner, ]
    datos_validacion <- filter_data[test_indices_inner, ]
    
 
    # Aplicar oversampling en el conjunto de entrenamiento interno
    datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ .,
                                            data = datos_entrenamiento_inner,
                                            p = 0.28)$data

    
    # Ajustar el modelo logístico
    modelo_actual <- glm("ESTADO_ACTUAL ~ .", data = datos_entrenamiento_oversampled, family = binomial("logit"))
    
    # Predecir y calcular el AUC en los datos de validación
    predicciones_validacion <- predict(modelo_actual, newdata = datos_validacion, type = "response")
    
    # Calcular el AUC
    auc_actual <- pROC::roc(as.numeric(datos_validacion$ESTADO_ACTUAL) - 1, predicciones_validacion)$auc
    
    # Actualizar el mejor AUC y el mejor modelo si es necesario
    
    
    # Realizar predicciones en el conjunto de entrenamiento externo para el AUC general
    predicciones_auc[test_indices_inner] <- predict(mejor_modelo_auc, newdata = datos_validacion, type = "response")
    predicciones_clasificadas <- ifelse(predicciones_auc[test_indices_inner] >= 0.5, "SI", "NO")
  
    resultados_sensibilidad_inner[i_inner] <- sum(predicciones_clasificadas == "SI" & datos_validacion[["ESTADO_ACTUAL"]] == "SI") / sum(datos_validacion[["ESTADO_ACTUAL"]] == "SI")
    resultados_accuracy_inner[i_inner] <- mean(predicciones_clasificadas == datos_validacion[["ESTADO_ACTUAL"]])

    targets_auc[test_indices_inner] <- as.numeric(datos_validacion$ESTADO_ACTUAL) - 1

  }
  
  # Calcular el AUC final para este fold externo
  auc_fold_externo <- pROC::roc(targets_auc, predicciones_auc)$auc
  accuracy_fold_externo <- mean(resultados_accuracy_inner)
  sensibilidad_fold_externo <- mean(resultados_sensibilidad_inner)
 
  
  # Almacenar el AUC del fold externo actual
  resultados_auc[i_outer] <- auc_fold_externo
  resultados_accuracy[i_outer] <- accuracy_fold_externo
  resultados_sensibilidad[i_outer] <- sensibilidad_fold_externo
  
  # Predecir en los datos de test para este pliegue externo y almacenar predicciones
  predicciones_test <- predict(mejor_modelo_auc, newdata = datos_test_outer, type = "response")
  predicciones_totales[test_indices_outer, i_outer] <- predicciones_test
  targets_totales[test_indices_outer] <- as.numeric(datos_test_outer$ESTADO_ACTUAL) - 1
 
}

# Calcular AUC promedio
auc_promedio <- mean(resultados_auc)
accuracy_promedio <- mean(resultados_accuracy)
sensibilidad_promedio <- mean(resultados_sensibilidad)


# Crear tabla con todas las métricas
tabla_metricas <- data.frame(
  Pliegue = 1:k_outer,
  AUC = sapply(resultados_auc, round, digits = 3),
  Accuracy = sapply(resultados_accuracy, round, digits = 3),
  Recall = sapply(resultados_sensibilidad, round, digits = 3)
)


# Agregar fila con promedio de métricas
tabla_metricas_logistic_embeded <- rbind(tabla_metricas, c("Mean", round(auc_promedio,3), round(accuracy_promedio,3), round(sensibilidad_promedio,3)))
save(tabla_metricas_logistic_embeded, file = "tabla_metricas_logistic_embeded.RData")


end_time <- Sys.time()
execution_time_logistic_embeded <- end_time - start_time
execution_time_logistic_embeded <- as.numeric(execution_time_logistic_embeded, units = "secs")
save(execution_time_logistic_embeded, file = "execution_time_logistic_embeded.RData")
```

```{r}
load("tabla_metricas_logistic_embeded.RData")
load("execution_time_logistic_embeded.RData")

cat("Execution time:", execution_time_logistic_embeded,  "seconds \n")

```

::: {.panel-tabset}

# Metrics Table
```{r}

# Imprimir tabla formateada con kableExtra
kable(tabla_metricas_logistic_embeded, caption = "Cross Validation Metrics", align = "c") %>%
  kable_styling(full_width = FALSE)
```

# Metrics Plot
```{r}
crear_plot_metricas_per_fold(tabla_metricas_logistic_embeded)
```

:::
The result is worse in this case and the execution time is higher due to the time spent in the variable selection.

## 5x2 Cross Validation Wrapper

Finally, we are going to honestly measure the performance of the model with bruteforce variable selection.

```{r,warning=FALSE,message=FALSE,eval=FALSE}
start_time <- Sys.time()

# Establecer la fórmula base para el modelo logístico
formula_base <- formula(ESTADO_ACTUAL ~ 1)  # Fórmula inicial sin predictoras

# Obtener nombres de variables predictoras
variables <- names(data)[-which(names(data) == "ESTADO_ACTUAL")]

# Obtener todas las combinaciones posibles de variables predictoras
combinaciones_variables <- unlist(lapply(1:length(variables), function(x) combn(variables, x, simplify = FALSE)), recursive = FALSE)


# Configurar la validación cruzada 5x2
k_outer <- 5  # Número de pliegues externos
k_inner <- 2  # Número de pliegues internos
folds_outer <- createFolds(data$ESTADO_ACTUAL, k = k_outer, list = FALSE)  # Obtener índices de pliegues externos

# Lista para almacenar resultados
resultados_auc <- numeric(k_outer)
resultados_sensibilidad <- numeric(k_outer)
resultados_accuracy <- numeric(k_outer)
indices <- 1:nrow(data)

# Inicializar matrices para almacenar predicciones y objetivos
predicciones_totales <- matrix(NA, nrow = nrow(data), ncol = k_outer)
targets_totales <- numeric(nrow(data))

# Iterar sobre cada pliegue externo y realizar el proceso de modelado
for (i_outer in 1:k_outer) {

  # Obtener los índices para el pliegue externo actual
  test_indices_outer <- indices[folds_outer==i_outer]  # Índices correspondientes al pliegue externo de test
  train_indices_outer <- setdiff(1:nrow(data), test_indices_outer)  # Índices correspondientes al conjunto de entrenamiento
  
  # Dividir datos en entrenamiento y test para el pliegue externo
  datos_entrenamiento_outer <- data[train_indices_outer, ]
  datos_test_outer <- data[test_indices_outer, ]
  

  # Configurar la validación cruzada interna
  folds_inner <- createFolds(datos_entrenamiento_outer$ESTADO_ACTUAL, k = k_inner, list = FALSE)  # Obtener índices de pliegues internos
  
  # Almacenar predicciones y test para formar una curva ROC general
  predicciones_auc <- numeric(nrow(datos_entrenamiento_outer))
  targets_auc <- numeric(nrow(datos_entrenamiento_outer))
  
  # Iterar sobre cada pliegue interno para selección de modelo
  indices_inner <- 1:nrow(datos_entrenamiento_outer)
  resultados_auc_inner <- numeric(k_inner)
  resultados_sensibilidad_inner <- numeric(k_inner)
  resultados_accuracy_inner <- numeric(k_inner)
  for (i_inner in 1:k_inner) {
    ###############################
    #### Aqui el filtrado##########
    ###############################
#con poca variables solo unaa la vez o filtrado o wrapper
    
    
    
    # Obtener los índices para el pliegue interno actual
    test_indices_inner <- indices_inner[folds_inner==i_inner]  # Índices correspondientes al pliegue interno de test
    train_indices_inner <- setdiff(1:nrow(datos_entrenamiento_outer), test_indices_inner)  # Índices correspondientes al conjunto de entrenamiento interno
    
    # Dividir datos en entrenamiento y test para el pliegue interno
    datos_entrenamiento_inner <- datos_entrenamiento_outer[train_indices_inner, ]
    datos_validacion <- datos_entrenamiento_outer[test_indices_inner, ]
    
 
    # Aplicar oversampling en el conjunto de entrenamiento interno
    datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ .,
                                            data = datos_entrenamiento_inner,
                                            p = 0.28)$data
    
    # Inicializar variables para almacenar el mejor modelo y su métrica asociada
    mejor_auc <- 0
    mejor_modelo_auc <- NULL
    
    # Iterar sobre todas las combinaciones de variables
    for (combinacion in combinaciones_variables) {
      # Construir la fórmula con las variables actuales
      formula_actual <- as.formula(paste("ESTADO_ACTUAL", "~", paste(combinacion, collapse = "+")))
      
      # Ajustar el modelo logístico
      modelo_actual <- glm(formula_actual, data = datos_entrenamiento_oversampled, family = binomial("logit"))
      
      # Predecir y calcular el AUC en los datos de validación
      predicciones_validacion <- predict(modelo_actual, newdata = datos_validacion, type = "response")
      
      # Calcular el AUC
      auc_actual <- pROC::roc(as.numeric(datos_validacion$ESTADO_ACTUAL) - 1, predicciones_validacion)$auc
      
      # Actualizar el mejor AUC y el mejor modelo si es necesario
      if (auc_actual > mejor_auc) {
        mejor_auc <- auc_actual
        mejor_modelo_auc <- modelo_actual
      }
    }
    
    # Realizar predicciones en el conjunto de entrenamiento externo para el AUC general
    predicciones_auc[test_indices_inner] <- predict(mejor_modelo_auc, newdata = datos_validacion, type = "response")
    predicciones_clasificadas <- ifelse(predicciones_auc[test_indices_inner] >= 0.5, "SI", "NO")
  
    resultados_sensibilidad_inner[i_inner] <- sum(predicciones_clasificadas == "SI" & datos_validacion[["ESTADO_ACTUAL"]] == "SI") / sum(datos_validacion[["ESTADO_ACTUAL"]] == "SI")
    resultados_accuracy_inner[i_inner] <- mean(predicciones_clasificadas == datos_validacion[["ESTADO_ACTUAL"]])

    targets_auc[test_indices_inner] <- as.numeric(datos_validacion$ESTADO_ACTUAL) - 1

  }
  
  # Calcular el AUC final para este fold externo
  auc_fold_externo <- pROC::roc(targets_auc, predicciones_auc)$auc
  accuracy_fold_externo <- mean(resultados_accuracy_inner)
  sensibilidad_fold_externo <- mean(resultados_sensibilidad_inner)
 
  
  # Almacenar el AUC del fold externo actual
  resultados_auc[i_outer] <- auc_fold_externo
  resultados_accuracy[i_outer] <- accuracy_fold_externo
  resultados_sensibilidad[i_outer] <- sensibilidad_fold_externo
  
  # Predecir en los datos de test para este pliegue externo y almacenar predicciones
  predicciones_test <- predict(mejor_modelo_auc, newdata = datos_test_outer, type = "response")
  predicciones_totales[test_indices_outer, i_outer] <- predicciones_test
  targets_totales[test_indices_outer] <- as.numeric(datos_test_outer$ESTADO_ACTUAL) - 1
 
}

# Calcular AUC promedio
auc_promedio <- mean(resultados_auc)
accuracy_promedio <- mean(resultados_accuracy)
sensibilidad_promedio <- mean(resultados_sensibilidad)


# Crear tabla con todas las métricas
tabla_metricas <- data.frame(
  Pliegue = 1:k_outer,
  AUC = sapply(resultados_auc, round, digits = 3),
  Accuracy = sapply(resultados_accuracy, round, digits = 3),
  Recall = sapply(resultados_sensibilidad, round, digits = 3)
)


# Agregar fila con promedio de métricas
tabla_metricas_logistic_wrapper <- rbind(tabla_metricas, c("Mean", round(auc_promedio,3), round(accuracy_promedio,3), round(sensibilidad_promedio,3)))

end_time <- Sys.time()
execution_time <- end_time - start_time
execution_time_logistic_wrapper <- as.numeric(execution_time, units = "secs")
save(tabla_metricas_logistic_wrapper, file = "tabla_metricas_logistic_wrapper.RData")
save(execution_time_logistic_wrapper, file = "execution_time_logistic_wrapper.RData")
```

```{r}
load("tabla_metricas_logistic_wrapper.RData")
load("execution_time_logistic_wrapper.RData")
cat("Execution time:", execution_time_logistic_wrapper,  "seconds \n")
```

::: {.panel-tabset}

# Metrics Table
```{r}

# Imprimir tabla formateada con kableExtra
kable(tabla_metricas_logistic_wrapper, caption = "Cross Validation Metrics", align = "c") %>%
  kable_styling(full_width = FALSE)
```

# Metrics Plot
```{r}
crear_plot_metricas_per_fold(tabla_metricas_logistic_wrapper)

```

:::

The metrics are definetly accurate but it is too instable for a predictive model deployed in production.

:::


# Neural Networks {#neural-networks}

::: {.panel-tabset style="background-color: #f0f0f0;box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);border-radius: 5px;"}

## Hold out

In this section, we are going to build a neural network composed by a hidden layer leaned on nnet library.

After experimentation, we have noticed that hyperparameters must be hindered to avoid overfitting. High number of iterations or lots of neurons in the hidden layer tends to converge into overfitting.
```{r,warning=FALSE,message=FALSE}
start_time <- Sys.time()
# Definir la fórmula para el modelo (sustituye "var_pred1 + var_pred2 + ..." con tus variables predictoras)
formula <- ESTADO_ACTUAL ~ .

# Dividir el conjunto de datos en entrenamiento y test
indices_entrenamiento <- sample(nrow(data), 0.75 * nrow(data))  # 75% para entrenamiento
datos_entrenamiento <- data[indices_entrenamiento, ]
datos_test <- data[-indices_entrenamiento, ]

# Dividir datos de entrenamiento en entrenamiento y validación
train_indices <- sample(nrow(datos_entrenamiento), 0.8 * nrow(datos_entrenamiento))  # 80% para entrenamiento
datos_entrenamiento_split <- datos_entrenamiento[train_indices, ]
datos_validacion <- datos_entrenamiento[-train_indices, ]

# Oversampling
proportion <- 0.28
datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ ., data = datos_entrenamiento_split, p = proportion)$data

# Inicializar variables para almacenar los resultados
mejor_auc <- 0
mejor_modelo <- NULL

# Definir combinaciones de hiperparámetros
params <- expand.grid(size = c(15), decay = c(0.01,0.5,0.001), maxit = c(225), skip = c(0, 1, 2))

# Iterar sobre las combinaciones de hiperparámetros
for (param in 1:nrow(params)) {
  size <- params$size[param]
  decay <- params$decay[param]
  maxit <- params$maxit[param]
  skip <- params$skip[param]
  
  # Entrenar el modelo con la combinación de hiperparámetros actual
  modelo <- nnet(formula, data = datos_entrenamiento_oversampled, size = size, decay = decay, maxit = maxit, skip = skip, trace = FALSE)
  
  # Realizar predicciones en datos de validación
  predicciones_validacion <- predict(modelo, newdata = datos_validacion, type = "raw")
  
  # Calcular AUC para la combinación actual
  roc_obj <- roc(ifelse(datos_validacion$ESTADO_ACTUAL == "SI", 1, 0), ifelse(predicciones_validacion == "SI", 1, 0))
  auc_actual <- auc(roc_obj)
  
  # Actualizar el mejor modelo y su AUC si se encuentra un modelo mejor
  if (auc_actual > mejor_auc) {
    mejor_auc <- auc_actual
    mejor_modelo <- modelo
  }
}

# Imprimir el mejor modelo y su AUC
print(mejor_modelo)

end_time <- Sys.time()
execution_time <- end_time - start_time
execution_time <- as.numeric(execution_time, units = "secs")
cat("Execution time:", execution_time,  "seconds \n")

```
::: {.panel-tabset}

### Predictions


```{r}

# Calcular las predicciones para el conjunto de test
predicciones_test <- predict(mejor_modelo, newdata = datos_test, type = "raw")
clases_predichas_test <- ifelse(predicciones_test >= 0.5, "SI", "NO")

mostrar_metricas(datos_test$ESTADO_ACTUAL,clases_predichas_test)

```


### Roc curve and auc

```{r}


# Calcular la curva ROC y el AUC para el conjunto de test
plot_roc_curve(predicciones_test,datos_test$ESTADO_ACTUAL)

```
:::
The performance is not bad, it can be improved with a vasiable selector method though.

## 5x2 Cross Validation Filter

We are going to perform a 5x2 cross validation to ensure the robustness of the model.
First, we create the outer folds and then we iterate over them to create the inner folds and perform the model selection process.


```{r,warning=FALSE,message=FALSE,eval=FALSE}
start_time <- Sys.time()
# Establecer la fórmula base para el modelo logístico
formula_base <- formula(ESTADO_ACTUAL ~ 1)  # Fórmula inicial sin predictoras

# Obtener nombres de variables predictoras
variables <- names(data)[-which(names(data) == "ESTADO_ACTUAL")]

# Obtener todas las combinaciones posibles de variables predictoras
combinaciones_variables <- unlist(lapply(1:length(variables), function(x) combn(variables, x, simplify = FALSE)), recursive = FALSE)


# Configurar la validación cruzada 5x2
k_outer <- 5  # Número de pliegues externos
k_inner <- 2  # Número de pliegues internos
folds_outer <- createFolds(data$ESTADO_ACTUAL, k = k_outer, list = FALSE)  # Obtener índices de pliegues externos

# Lista para almacenar resultados
resultados_auc <- numeric(k_outer)
resultados_sensibilidad <- numeric(k_outer)
resultados_accuracy <- numeric(k_outer)
indices <- 1:nrow(data)

# Inicializar matrices para almacenar predicciones y objetivos
predicciones_totales <- matrix(NA, nrow = nrow(data), ncol = k_outer)
targets_totales <- numeric(nrow(data))

# Iterar sobre cada pliegue externo y realizar el proceso de modelado
for (i_outer in 1:k_outer) {

  # Obtener los índices para el pliegue externo actual
  test_indices_outer <- indices[folds_outer==i_outer]  # Índices correspondientes al pliegue externo de test
  train_indices_outer <- setdiff(1:nrow(data), test_indices_outer)  # Índices correspondientes al conjunto de entrenamiento
  
  # Dividir datos en entrenamiento y test para el pliegue externo
  datos_entrenamiento_outer <- data[train_indices_outer, ]
  datos_test_outer <- data[test_indices_outer, ]
  

  # Configurar la validación cruzada interna
  folds_inner <- createFolds(datos_entrenamiento_outer$ESTADO_ACTUAL, k = k_inner, list = FALSE)  # Obtener índices de pliegues internos
  
  # Almacenar predicciones y test para formar una curva ROC general
  predicciones_auc <- numeric(nrow(datos_entrenamiento_outer))
  targets_auc <- numeric(nrow(datos_entrenamiento_outer))
  
  # Iterar sobre cada pliegue interno para selección de modelo
  indices_inner <- 1:nrow(datos_entrenamiento_outer)
  resultados_auc_inner <- numeric(k_inner)
  resultados_sensibilidad_inner <- numeric(k_inner)
  resultados_accuracy_inner <- numeric(k_inner)
  for (i_inner in 1:k_inner) {
    ###############################
    #### Aqui el filtrado##########
    ###############################
    filter_data <- filter_dependency(datos_entrenamiento_outer,"ESTADO_ACTUAL",colnames(datos_entrenamiento_outer),0.05)
    if ("ESTADO_ACTUAL.1" %in% colnames(filter_data)) {
      filter_data <- filter_data[, !colnames(filter_data) %in% "ESTADO_ACTUAL.1", drop = FALSE]
    }
#con poca variables solo unaa la vez o filtrado o wrapper
    
    
    
    # Obtener los índices para el pliegue interno actual
    test_indices_inner <- indices_inner[folds_inner==i_inner]  # Índices correspondientes al pliegue interno de test
    train_indices_inner <- setdiff(1:nrow(datos_entrenamiento_outer), test_indices_inner)  # Índices correspondientes al conjunto de entrenamiento interno
    
    # Dividir datos en entrenamiento y test para el pliegue interno
    datos_entrenamiento_inner <- datos_entrenamiento_outer[train_indices_inner, ]
    datos_validacion <- datos_entrenamiento_outer[test_indices_inner, ]
    
 
    # Aplicar oversampling en el conjunto de entrenamiento interno
    datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ .,
                                            data = datos_entrenamiento_inner,
                                            p = 0.28)$data
    
    # Inicializar variables para almacenar los resultados
    mejor_auc <- 0
    mejor_modelo <- NULL
    
    # Definir combinaciones de hiperparámetros
    params <- expand.grid(size = c(15), decay = c(0.01,0.5,0.001), maxit = c(225), skip = c(0, 1, 2))
        
    # Iterar sobre las combinaciones de hiperparámetros
    for (param in 1:nrow(params)) {
      size <- params$size[param]
      decay <- params$decay[param]
      maxit <- params$maxit[param]
      skip <- params$skip[param]
      
      # Entrenar el modelo con la combinación de hiperparámetros actual
      modelo <- nnet(formula, data = datos_entrenamiento_oversampled, size = size, decay = decay, maxit = maxit, skip = skip, trace = FALSE)
      
      # Realizar predicciones en datos de validación
      predicciones_validacion <- predict(modelo, newdata = datos_validacion, type = "raw")
      
      # Calcular AUC para la combinación actual
      roc_obj <- roc(ifelse(datos_validacion$ESTADO_ACTUAL == "SI", 1, 0), ifelse(predicciones_validacion == "SI", 1, 0))
      auc_actual <- auc(roc_obj)
      
      # Actualizar el mejor modelo y su AUC si se encuentra un modelo mejor
      if (auc_actual > mejor_auc) {
        mejor_auc <- auc_actual
        mejor_modelo <- modelo
      }
    }
    ########################
    predicciones_test <- predict(mejor_modelo, newdata = datos_validacion, type = "raw")
    clases_predichas_test <- ifelse(predicciones_test >= 0.5, "SI", "NO")
    
    ###########################
    # Realizar predicciones en el conjunto de entrenamiento externo para el AUC general
    predicciones_auc[test_indices_inner] <- predict(mejor_modelo_auc, newdata = datos_validacion, type = "response")
    predicciones_clasificadas <- ifelse(predicciones_auc[test_indices_inner] >= 0.5, "SI", "NO")
  
    resultados_sensibilidad_inner[i_inner] <- sum(clases_predichas_test == "SI" & datos_validacion[["ESTADO_ACTUAL"]] == "SI") / sum(datos_validacion[["ESTADO_ACTUAL"]] == "SI")
    resultados_accuracy_inner[i_inner] <- mean(clases_predichas_test == datos_validacion[["ESTADO_ACTUAL"]])

    targets_auc[test_indices_inner] <- as.numeric(datos_validacion$ESTADO_ACTUAL) - 1

  }
  
  # Calcular el AUC final para este fold externo
  auc_fold_externo <- pROC::roc(targets_auc, predicciones_auc)$auc
  accuracy_fold_externo <- mean(resultados_accuracy_inner)
  sensibilidad_fold_externo <- mean(resultados_sensibilidad_inner)
 
  
  # Almacenar el AUC del fold externo actual
  resultados_auc[i_outer] <- auc_fold_externo
  resultados_accuracy[i_outer] <- accuracy_fold_externo
  resultados_sensibilidad[i_outer] <- sensibilidad_fold_externo
  
  # Predecir en los datos de test para este pliegue externo y almacenar predicciones
  predicciones_test <- predict(mejor_modelo_auc, newdata = datos_test_outer, type = "response")
  predicciones_totales[test_indices_outer, i_outer] <- predicciones_test
  targets_totales[test_indices_outer] <- as.numeric(datos_test_outer$ESTADO_ACTUAL) - 1
 
}

# Calcular AUC promedio
auc_promedio <- mean(resultados_auc)
accuracy_promedio <- mean(resultados_accuracy)
sensibilidad_promedio <- mean(resultados_sensibilidad)


# Crear tabla con todas las métricas
tabla_metricas <- data.frame(
  Pliegue = 1:k_outer,
  AUC = sapply(resultados_auc, round, digits = 3),
  Accuracy = sapply(resultados_accuracy, round, digits = 3),
  Recall = sapply(resultados_sensibilidad, round, digits = 3)
)


# Agregar fila con promedio de métricas
tabla_metricas_nnet_filter <- rbind(tabla_metricas, c("Mean", round(auc_promedio,3), round(accuracy_promedio,3), round(sensibilidad_promedio,3)))

end_time <- Sys.time()
execution_time_nnet_filter <- end_time - start_time
execution_time_nnet_filter <- as.numeric(execution_time_nnet_filter, units = "secs")
save(tabla_metricas_nnet_filter, file = "tabla_metricas_nnet_filter.RData")
save(execution_time_nnet_filter, file = "execution_time_nnet_filter.RData")
```

```{r}
load("tabla_metricas_nnet_filter.RData")
load("execution_time_nnet_filter.RData")
cat("Execution time:", execution_time_nnet_filter,  "seconds \n")
```


::: {.panel-tabset}

# Metrics Table
```{r}

# Imprimir tabla formateada con kableExtra
kable(tabla_metricas_nnet_filter, caption = "Cross Validation Metrics", align = "c") %>%
  kable_styling(full_width = FALSE)
```

# Metrics Plot
```{r}
crear_plot_metricas_per_fold(tabla_metricas_nnet_filter)
```

:::
The algorith is much more stable than the previous one.

## 5x2 Cross Validation Embedded


```{r,warning=FALSE,message=FALSE, eval=FALSE}
start_time <- Sys.time()
# Establecer la fórmula base para el modelo logístico
formula_base <- formula(ESTADO_ACTUAL ~ 1)  # Fórmula inicial sin predictoras

# Obtener nombres de variables predictoras
variables <- names(data)[-which(names(data) == "ESTADO_ACTUAL")]

# Obtener todas las combinaciones posibles de variables predictoras
combinaciones_variables <- unlist(lapply(1:length(variables), function(x) combn(variables, x, simplify = FALSE)), recursive = FALSE)


# Configurar la validación cruzada 5x2
k_outer <- 5  # Número de pliegues externos
k_inner <- 2  # Número de pliegues internos
folds_outer <- createFolds(data$ESTADO_ACTUAL, k = k_outer, list = FALSE)  # Obtener índices de pliegues externos

# Lista para almacenar resultados
resultados_auc <- numeric(k_outer)
resultados_sensibilidad <- numeric(k_outer)
resultados_accuracy <- numeric(k_outer)
indices <- 1:nrow(data)

# Inicializar matrices para almacenar predicciones y objetivos
predicciones_totales <- matrix(NA, nrow = nrow(data), ncol = k_outer)
targets_totales <- numeric(nrow(data))

# Iterar sobre cada pliegue externo y realizar el proceso de modelado
for (i_outer in 1:k_outer) {

  # Obtener los índices para el pliegue externo actual
  test_indices_outer <- indices[folds_outer==i_outer]  # Índices correspondientes al pliegue externo de test
  train_indices_outer <- setdiff(1:nrow(data), test_indices_outer)  # Índices correspondientes al conjunto de entrenamiento
  
  # Dividir datos en entrenamiento y test para el pliegue externo
  datos_entrenamiento_outer <- data[train_indices_outer, ]
  datos_test_outer <- data[test_indices_outer, ]
  

  # Configurar la validación cruzada interna
  folds_inner <- createFolds(datos_entrenamiento_outer$ESTADO_ACTUAL, k = k_inner, list = FALSE)  # Obtener índices de pliegues internos
  
  # Almacenar predicciones y test para formar una curva ROC general
  predicciones_auc <- numeric(nrow(datos_entrenamiento_outer))
  targets_auc <- numeric(nrow(datos_entrenamiento_outer))
  
  # Iterar sobre cada pliegue interno para selección de modelo
  indices_inner <- 1:nrow(datos_entrenamiento_outer)
  resultados_auc_inner <- numeric(k_inner)
  resultados_sensibilidad_inner <- numeric(k_inner)
  resultados_accuracy_inner <- numeric(k_inner)
  for (i_inner in 1:k_inner) {
    ###############################
    #### Aqui el filtrado##########
    ###############################
      variables_importantes <- seleccionar_variables_importantes(datos_entrenamiento_outer,"ESTADO_ACTUAL",6)
    filter_data <- datos_entrenamiento_outer[,c("ESTADO_ACTUAL",variables_importantes)]
    # Obtener los índices para el pliegue interno actual
    test_indices_inner <- indices_inner[folds_inner==i_inner]  # Índices correspondientes al pliegue interno de test
    train_indices_inner <- setdiff(1:nrow(filter_data), test_indices_inner)  # Índices correspondientes al conjunto de entrenamiento interno
    
    # Dividir datos en entrenamiento y test para el pliegue interno
    datos_entrenamiento_inner <- filter_data[train_indices_inner, ]
    datos_validacion <- filter_data[test_indices_inner, ]
 
    # Aplicar oversampling en el conjunto de entrenamiento interno
    datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ .,
                                            data = datos_entrenamiento_inner,
                                            p = 0.28)$data
    
    # Inicializar variables para almacenar los resultados
    mejor_auc <- 0
    mejor_modelo <- NULL
    
    # Definir combinaciones de hiperparámetros
    params <- expand.grid(size = c(15), decay = c(0.01,0.5,0.001), maxit = c(225), skip = c(0, 1, 2))
        
    # Iterar sobre las combinaciones de hiperparámetros

    for (param in 1:nrow(params)) {
      size <- params$size[param]
      decay <- params$decay[param]
      maxit <- params$maxit[param]
      skip <- params$skip[param]
      
      # Entrenar el modelo con la combinación de hiperparámetros actual
      modelo <- nnet(formula, data = datos_entrenamiento_oversampled, size = size, decay = decay, maxit = maxit, skip = skip, trace = FALSE)
      
      # Realizar predicciones en datos de validación
      predicciones_validacion <- predict(modelo, newdata = datos_validacion, type = "raw")
      
      # Calcular AUC para la combinación actual
      roc_obj <- roc(ifelse(datos_validacion$ESTADO_ACTUAL == "SI", 1, 0), ifelse(predicciones_validacion == "SI", 1, 0))
      auc_actual <- auc(roc_obj)
      
      # Actualizar el mejor modelo y su AUC si se encuentra un modelo mejor
      if (auc_actual > mejor_auc) {
        mejor_auc <- auc_actual
        mejor_modelo <- modelo
      }
    }
     
    ########################
    predicciones_test <- predict(mejor_modelo, newdata = datos_validacion, type = "raw")
    clases_predichas_test <- ifelse(predicciones_test >= 0.5, "SI", "NO")
    
    ###########################
    # Realizar predicciones en el conjunto de entrenamiento externo para el AUC general
    predicciones_auc[test_indices_inner] <- predict(mejor_modelo_auc, newdata = datos_validacion, type = "response")
    predicciones_clasificadas <- ifelse(predicciones_auc[test_indices_inner] >= 0.5, "SI", "NO")
  
    resultados_sensibilidad_inner[i_inner] <- sum(clases_predichas_test == "SI" & datos_validacion[["ESTADO_ACTUAL"]] == "SI") / sum(datos_validacion[["ESTADO_ACTUAL"]] == "SI")
    resultados_accuracy_inner[i_inner] <- mean(clases_predichas_test == datos_validacion[["ESTADO_ACTUAL"]])

    targets_auc[test_indices_inner] <- as.numeric(datos_validacion$ESTADO_ACTUAL) - 1

  }
  
  # Calcular el AUC final para este fold externo
  auc_fold_externo <- pROC::roc(targets_auc, predicciones_auc)$auc
  accuracy_fold_externo <- mean(resultados_accuracy_inner)
  sensibilidad_fold_externo <- mean(resultados_sensibilidad_inner)
 
  
  # Almacenar el AUC del fold externo actual
  resultados_auc[i_outer] <- auc_fold_externo
  resultados_accuracy[i_outer] <- accuracy_fold_externo
  resultados_sensibilidad[i_outer] <- sensibilidad_fold_externo
  
  # Predecir en los datos de test para este pliegue externo y almacenar predicciones
  predicciones_test <- predict(mejor_modelo_auc, newdata = datos_test_outer, type = "response")
  predicciones_totales[test_indices_outer, i_outer] <- predicciones_test
  targets_totales[test_indices_outer] <- as.numeric(datos_test_outer$ESTADO_ACTUAL) - 1
 
}

# Calcular AUC promedio
auc_promedio <- mean(resultados_auc)
accuracy_promedio <- mean(resultados_accuracy)
sensibilidad_promedio <- mean(resultados_sensibilidad)


# Crear tabla con todas las métricas
tabla_metricas <- data.frame(
  Pliegue = 1:k_outer,
  AUC = sapply(resultados_auc, round, digits = 3),
  Accuracy = sapply(resultados_accuracy, round, digits = 3),
  Recall = sapply(resultados_sensibilidad, round, digits = 3)
)


# Agregar fila con promedio de métricas
tabla_metricas_nnet_embeded <- rbind(tabla_metricas, c("Mean", round(auc_promedio,3), round(accuracy_promedio,3), round(sensibilidad_promedio,3)))

end_time <- Sys.time()
execution_time_nnet_embeded <- end_time - start_time
execution_time_nnet_embeded <- as.numeric(execution_time_nnet_embeded, units = "secs")
save(tabla_metricas_nnet_embeded, file = "tabla_metricas_nnet_embeded.RData")
save(execution_time_nnet_embeded, file = "execution_time_nnet_embeded.RData")
```

```{r}
load("tabla_metricas_nnet_embeded.RData")
load("execution_time_nnet_embeded.RData")
cat("Execution time:", execution_time_nnet_embeded,  "seconds \n")
```


::: {.panel-tabset}

# Metrics Table
```{r}

# Imprimir tabla formateada con kableExtra
kable(tabla_metricas_nnet_embeded, caption = "Cross Validation Metrics", align = "c") %>%
  kable_styling(full_width = FALSE)
```

# Metrics Plot
```{r}
crear_plot_metricas_per_fold(tabla_metricas_nnet_embeded)
```

:::
 
It seems that there are not a lot of redundant variables, so it works better with more variables.


## 5x2 Cross Validation Wrapper

Specially in this case, it is important to limit the hyperparameters due to the high computational cost of the wrapper method.


```{r,warning=FALSE,message=FALSE, eval=FALSE}
start_time <- Sys.time()
# Establecer la fórmula base para el modelo logístico
formula_base <- formula(ESTADO_ACTUAL ~ 1)  # Fórmula inicial sin predictoras

# Obtener nombres de variables predictoras
variables <- names(data)[-which(names(data) == "ESTADO_ACTUAL")]

# Obtener todas las combinaciones posibles de variables predictoras
combinaciones_variables <- unlist(lapply(1:length(variables), function(x) combn(variables, x, simplify = FALSE)), recursive = FALSE)


# Configurar la validación cruzada 5x2
k_outer <- 5  # Número de pliegues externos
k_inner <- 2  # Número de pliegues internos
folds_outer <- createFolds(data$ESTADO_ACTUAL, k = k_outer, list = FALSE)  # Obtener índices de pliegues externos

# Lista para almacenar resultados
resultados_auc <- numeric(k_outer)
resultados_sensibilidad <- numeric(k_outer)
resultados_accuracy <- numeric(k_outer)
indices <- 1:nrow(data)

# Inicializar matrices para almacenar predicciones y objetivos
predicciones_totales <- matrix(NA, nrow = nrow(data), ncol = k_outer)
targets_totales <- numeric(nrow(data))

# Iterar sobre cada pliegue externo y realizar el proceso de modelado
for (i_outer in 1:k_outer) {

  # Obtener los índices para el pliegue externo actual
  test_indices_outer <- indices[folds_outer==i_outer]  # Índices correspondientes al pliegue externo de test
  train_indices_outer <- setdiff(1:nrow(data), test_indices_outer)  # Índices correspondientes al conjunto de entrenamiento
  
  # Dividir datos en entrenamiento y test para el pliegue externo
  datos_entrenamiento_outer <- data[train_indices_outer, ]
  datos_test_outer <- data[test_indices_outer, ]
  

  # Configurar la validación cruzada interna
  folds_inner <- createFolds(datos_entrenamiento_outer$ESTADO_ACTUAL, k = k_inner, list = FALSE)  # Obtener índices de pliegues internos
  
  # Almacenar predicciones y test para formar una curva ROC general
  predicciones_auc <- numeric(nrow(datos_entrenamiento_outer))
  targets_auc <- numeric(nrow(datos_entrenamiento_outer))
  
  # Iterar sobre cada pliegue interno para selección de modelo
  indices_inner <- 1:nrow(datos_entrenamiento_outer)
  resultados_auc_inner <- numeric(k_inner)
  resultados_sensibilidad_inner <- numeric(k_inner)
  resultados_accuracy_inner <- numeric(k_inner)
  for (i_inner in 1:k_inner) {
   ###############################
    #### Aqui el filtrado##########
    ###############################
#con poca variables solo unaa la vez o filtrado o wrapper
    
    
    
    # Obtener los índices para el pliegue interno actual
    test_indices_inner <- indices_inner[folds_inner==i_inner]  # Índices correspondientes al pliegue interno de test
    train_indices_inner <- setdiff(1:nrow(datos_entrenamiento_outer), test_indices_inner)  # Índices correspondientes al conjunto de entrenamiento interno
    
    # Dividir datos en entrenamiento y test para el pliegue interno
    datos_entrenamiento_inner <- datos_entrenamiento_outer[train_indices_inner, ]
    datos_validacion <- datos_entrenamiento_outer[test_indices_inner, ]
    
 
    # Aplicar oversampling en el conjunto de entrenamiento interno
    datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ .,
                                            data = datos_entrenamiento_inner,
                                            p = 0.28)$data
    
    # Inicializar variables para almacenar los resultados
    mejor_auc <- 0
    mejor_modelo <- NULL
    
    # Definir combinaciones de hiperparámetros
    params <- expand.grid(size = c(15), decay = c(0.01,0.5,0.001), maxit = c(225), skip = c(0, 1, 2))
    
    for (combinacion in combinaciones_variables) {
      # Construir la fórmula con las variables actuales
      formula_actual <- as.formula(paste("ESTADO_ACTUAL", "~", paste(combinacion, collapse = "+")))
  
      # Iterar sobre las combinaciones de hiperparámetros
      for (param in 1:nrow(params)) {
        size <- params$size[param]
        decay <- params$decay[param]
        maxit <- params$maxit[param]
        skip <- params$skip[param]
        
        # Entrenar el modelo con la combinación de hiperparámetros actual
        modelo <- nnet(formula, data = datos_entrenamiento_oversampled, size = size, decay = decay, maxit = maxit, skip = skip, trace = FALSE)
        
        # Realizar predicciones en datos de validación
        predicciones_validacion <- predict(modelo, newdata = datos_validacion, type = "raw")
        
        # Calcular AUC para la combinación actual
        roc_obj <- roc(ifelse(datos_validacion$ESTADO_ACTUAL == "SI", 1, 0), ifelse(predicciones_validacion == "SI", 1, 0))
        auc_actual <- auc(roc_obj)
        
        # Actualizar el mejor modelo y su AUC si se encuentra un modelo mejor
        if (auc_actual > mejor_auc) {
          mejor_auc <- auc_actual
          mejor_modelo <- modelo
        }
      }
    }
    ########################
    predicciones_test <- predict(mejor_modelo, newdata = datos_validacion, type = "raw")
    clases_predichas_test <- ifelse(predicciones_test >= 0.5, "SI", "NO")
    
    ###########################
    # Realizar predicciones en el conjunto de entrenamiento externo para el AUC general
    predicciones_auc[test_indices_inner] <- predict(mejor_modelo_auc, newdata = datos_validacion, type = "response")
    predicciones_clasificadas <- ifelse(predicciones_auc[test_indices_inner] >= 0.5, "SI", "NO")
  
    resultados_sensibilidad_inner[i_inner] <- sum(clases_predichas_test == "SI" & datos_validacion[["ESTADO_ACTUAL"]] == "SI") / sum(datos_validacion[["ESTADO_ACTUAL"]] == "SI")
    resultados_accuracy_inner[i_inner] <- mean(clases_predichas_test == datos_validacion[["ESTADO_ACTUAL"]])

    targets_auc[test_indices_inner] <- as.numeric(datos_validacion$ESTADO_ACTUAL) - 1

  }
  
  # Calcular el AUC final para este fold externo
  auc_fold_externo <- pROC::roc(targets_auc, predicciones_auc)$auc
  accuracy_fold_externo <- mean(resultados_accuracy_inner)
  sensibilidad_fold_externo <- mean(resultados_sensibilidad_inner)
 
  
  # Almacenar el AUC del fold externo actual
  resultados_auc[i_outer] <- auc_fold_externo
  resultados_accuracy[i_outer] <- accuracy_fold_externo
  resultados_sensibilidad[i_outer] <- sensibilidad_fold_externo
  
  # Predecir en los datos de test para este pliegue externo y almacenar predicciones
  predicciones_test <- predict(mejor_modelo_auc, newdata = datos_test_outer, type = "response")
  predicciones_totales[test_indices_outer, i_outer] <- predicciones_test
  targets_totales[test_indices_outer] <- as.numeric(datos_test_outer$ESTADO_ACTUAL) - 1
 
}

# Calcular AUC promedio
auc_promedio <- mean(resultados_auc)
accuracy_promedio <- mean(resultados_accuracy)
sensibilidad_promedio <- mean(resultados_sensibilidad)


# Crear tabla con todas las métricas
tabla_metricas <- data.frame(
  Pliegue = 1:k_outer,
  AUC = sapply(resultados_auc, round, digits = 3),
  Accuracy = sapply(resultados_accuracy, round, digits = 3),
  Recall = sapply(resultados_sensibilidad, round, digits = 3)
)


# Agregar fila con promedio de métricas
tabla_metricas_nnet_wrapper <- rbind(tabla_metricas, c("Mean", round(auc_promedio,3), round(accuracy_promedio,3), round(sensibilidad_promedio,3)))

end_time <- Sys.time()
execution_time_nnet_wrapper <- end_time - start_time
execution_time_nnet_wrapper <- as.numeric(execution_time_nnet_wrapper, units = "secs")
save(tabla_metricas_nnet_wrapper, file = "tabla_metricas_nnet_wrapper.RData")
save(execution_time_nnet_wrapper, file = "execution_time_nnet_wrapper.RData")
```

```{r}
load("tabla_metricas_nnet_wrapper.RData")
load("execution_time_nnet_wrapper.RData")
cat("Execution time:", execution_time_nnet_wrapper,  "seconds \n")
```


::: {.panel-tabset}

# Metrics Table
```{r}

# Imprimir tabla formateada con kableExtra
kable(tabla_metricas_nnet_wrapper, caption = "Cross Validation Metrics", align = "c") %>%
  kable_styling(full_width = FALSE)
```

# Metrics Plot
```{r}
crear_plot_metricas_per_fold(tabla_metricas_nnet_wrapper)
```

:::

The performance and constancy of the model are good.


## Hyperparameters Resume

This is a resume of the model performance with every hyperparameters combination. The next table shows the top 10 and bottom 10 models by AUC.


```{r,message=FALSE,warning=FALSE}
start_time <- Sys.time()

# Dividir el conjunto de datos en entrenamiento y test
set.seed(123)  # Establecer una semilla para reproducibilidad
indices_entrenamiento <- sample(nrow(data), 0.75 * nrow(data))  # 75% para entrenamiento
datos_entrenamiento <- data[indices_entrenamiento, ]
datos_test <- data[-indices_entrenamiento, ]

# Dividir datos de entrenamiento en entrenamiento y validación
train_indices <- sample(nrow(datos_entrenamiento), 0.8 * nrow(datos_entrenamiento))  # 80% para entrenamiento
datos_entrenamiento_split <- datos_entrenamiento[train_indices, ]
datos_validacion <- datos_entrenamiento[-train_indices, ]

# Oversampling
proportion <- 0.28
datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ ., data = datos_entrenamiento_split, p = proportion, seed = 123)$data

# Inicializar variables para almacenar los resultados
mejor_auc <- 0
mejor_modelo <- NULL
resultados <- data.frame(size = integer(), decay = numeric(), maxit = integer(), skip = integer(), auc = numeric(), stringsAsFactors = FALSE)

# Definir combinaciones de hiperparámetros
params <- expand.grid(size = c(15, 40), decay = c(0.01, 0.5, 0.001), maxit = c(225, 1000, 5), skip = c(0, 1, 2))

# Fórmula con todas las variables predictoras
formula <- ESTADO_ACTUAL ~ .

# Iterar sobre las combinaciones de hiperparámetros
for (param in 1:nrow(params)) {
  size <- params$size[param]
  decay <- params$decay[param]
  maxit <- params$maxit[param]
  skip <- params$skip[param]
  
  # Entrenar el modelo con la combinación de hiperparámetros actual
  modelo <- nnet(formula, data = datos_entrenamiento_oversampled, size = size, decay = decay, maxit = maxit, skip = skip, trace = FALSE)
  
  # Realizar predicciones en datos de prueba
  predicciones_test <- predict(modelo, newdata = datos_test, type = "raw")
  
  # Calcular AUC para la combinación actual en datos de prueba
  roc_obj <- roc(ifelse(datos_test$ESTADO_ACTUAL == "SI", 1, 0), predicciones_test)
  auc_actual <- as.numeric(auc(roc_obj))  # Extraer el valor numérico del AUC
  
  # Guardar los resultados en la tabla
  resultados <- resultados %>%
    add_row(size = size, decay = decay, maxit = maxit, skip = skip, auc = auc_actual)
  
  # Actualizar el mejor modelo y su AUC si se encuentra un modelo mejor
  if (auc_actual > mejor_auc) {
    mejor_auc <- auc_actual
    mejor_modelo <- modelo
  }
}

# Ordenar los resultados por AUC en orden descendente
resultados_ordenados <- resultados %>% arrange(desc(auc))



# Mostrar la tabla con los resultados de todas las combinaciones de hiperparámetros
save(resultados_ordenados, file = "resultados_ordenados.RData")

end_time <- Sys.time()
execution_time <- end_time - start_time
execution_time <- as.numeric(execution_time, units = "secs")
save(execution_time, file = "execution_time_nnet.RData")

```

```{r}
# Cargar los datos guardados y mostrar las tablas
load("resultados_ordenados.RData")
load("execution_time_nnet.RData")
cat("Execution time:", execution_time, "seconds \n")
# Seleccionar los mejores 10 modelos
mejores_10 <- resultados_ordenados %>% head(10)

# Seleccionar los peores 10 modelos
peores_10 <- resultados_ordenados %>% tail(10)
# Imprimir los mejores y peores modelos en tablas bonitas

mejores_10 %>%
  kbl(caption = "Top 10 Models by AUC") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

peores_10 %>%
  kbl(caption = "Bottom 10 Models by AUC") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```
```{r}
 # Instalar y cargar la librería plotly si no está instalada
if (!require(plotly)) {
  install.packages("plotly")
}
library(plotly)

# Cargar los resultados
load("resultados_ordenados.RData")

# Crear la gráfica 3D con plotly
fig <- plot_ly(
  data = resultados_ordenados,
  x = ~size,
  y = ~decay,
  z = ~auc,
  color = ~factor(maxit),
  colors = c("#3C5B6F", "steelblue", "lightblue"),
  symbol = ~factor(skip),
  type = "scatter3d",
  mode = "markers",
  marker = list(size = 5)
)

fig <- fig %>%
  layout(
    title = "AUC for each combination of hyperparameters",
    scene = list(
      xaxis = list(title = "Size"),
      yaxis = list(title = "Decay"),
      zaxis = list(title = "AUC")
    )
  )

# Mostrar la gráfica
fig

```

It seem that the best models have a size of 15 and a decay of 0.01. The skip parameter does not seem to have a big impact on the model performance. These perrormance measuraments are extremely optimistic and do not correspond to the real performance of the model but it can help us to understand the hyperparameters impact on the model.


:::


# Support Vectors Machine {#suport-vectors-machine}

::: {.panel-tabset style="background-color: #f0f0f0;box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);border-radius: 5px;"}

## Hold out

In this section we are going to build a SVM model using different hyperparameters and evaluate it using a hold out validation.


```{r,warning=FALSE,message=FALSE, eval=FALSE}
start_time <- Sys.time()

# Parámetros a cambiar:
proportion <- 0.28
sensitivity_weight <- 0.8

# Dividir el conjunto de datos en entrenamiento y test
set.seed(123)  # Establecer una semilla para reproducibilidad
indices_entrenamiento <- sample(nrow(data), 0.75 * nrow(data))  # 75% para entrenamiento
datos_entrenamiento <- data[indices_entrenamiento, ]
datos_test <- data[-indices_entrenamiento, ]

# Dividir datos de entrenamiento en entrenamiento y validación
train_indices <- sample(nrow(datos_entrenamiento), 0.8 * nrow(datos_entrenamiento))  # 80% para entrenamiento
datos_entrenamiento_split <- datos_entrenamiento[train_indices, ]
datos_validacion <- datos_entrenamiento[-train_indices, ]

# Realizar oversampling
datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ ., data = datos_entrenamiento_split, seed = 123, p = proportion)$data

# Definir la fórmula para el modelo
formula <- ESTADO_ACTUAL ~ .


# Inicializar variables para almacenar los resultados
mejor_auc <- 0
mejor_modelo <- NULL

# Definir hiperparámetros a probar
params <- expand.grid(cost = c(100, 1000, 10000, 50000, 1000000),
                      gamma = c(0.1, 1, 10),
                      kernel = c("linear", "radial", "sigmoid"))

# Bucle para probar diferentes combinaciones de hiperparámetros
for (i in 1:nrow(params)) {
  cost <- params$cost[i]
  gamma <- params$gamma[i]
  kernel <- params$kernel[i]

  # Entrenar el modelo SVM con la combinación de hiperparámetros actual
  modelo <- svm(formula, data = datos_entrenamiento_oversampled, type = "C-classification", kernel = kernel, cost = cost, gamma = gamma, probability = TRUE)
  ##############################
  # Realizar predicciones con probabilidades en datos de validación
  predicciones_validacion <- predict(modelo, newdata = datos_validacion, probability = TRUE)
  predicciones_validacion_prob <- attr(predicciones_validacion, "probabilities")[, "SI"]  # Asumiendo que "SI" es la clase positiva
  
  # Calcular las predicciones finales basadas en probabilidades
  umbral <- 0.5
  predicciones_finales <- ifelse(predicciones_validacion_prob >= umbral, "SI", "NO")
  
  # Calcular la sensibilidad y la precisión
  TP <- sum(predicciones_finales == "SI" & datos_validacion[["ESTADO_ACTUAL"]] == "SI")
  FN <- sum(predicciones_finales == "NO" & datos_validacion[["ESTADO_ACTUAL"]] == "SI")
  FP <- sum(predicciones_finales == "SI" & datos_validacion[["ESTADO_ACTUAL"]] == "NO")
  TN <- sum(predicciones_finales == "NO" & datos_validacion[["ESTADO_ACTUAL"]] == "NO")
  
  recall_actual <- TP / (TP + FN)
  accuracy_actual <- (TP + TN) / (TP + FN + FP + TN)
  
  # Calcular el custom metric para la combinación actual
  custom_metric_actual <- compute_custom_metric(recall_actual, accuracy_actual, sensitivity_weight)
  
  # Actualizar el mejor modelo y su rendimiento si se encuentra un modelo mejor
  if (auc_actual > mejor_auc) {
    mejor_auc <- auc_actual
    mejor_modelo <- modelo
  }
}
#########################33
mejor_modelo_svm <- mejor_modelo
end_time <- Sys.time()
execution_time_svm <- end_time - start_time
execution_time_svm <- as.numeric(execution_time_svm, units = "secs")
save(mejor_modelo_svm, file = "mejor_modelo_svm.RData")
save(execution_time_svm, file = "execution_time_svm.RData")
```

```{r}
load("mejor_modelo_svm.RData")
load("execution_time_svm.RData")
summary(mejor_modelo_svm)

cat("Execution time:", execution_time_svm,  "seconds \n")
```
::: {.panel-tabset}

### Predictions

We are going to predict on test in order to get a estimation of the performance of the model.

```{r}

# Calcular las predicciones para el conjunto de test
predicciones_test <- predict(mejor_modelo_svm, newdata = datos_test, probability = TRUE)

predicciones_test <- attr(predicciones_test, "probabilities")[, "SI"]  # Asumiendo que "SI" es la clase positiva

clases_predichas_test <- ifelse(predicciones_test >= 0.5, "SI", "NO")

mostrar_metricas(datos_test$ESTADO_ACTUAL,clases_predichas_test)
```

Even though the model has a good auc, the recall is not good enough. This is because the model is not able to detect the positive cases. This is unacceptable in a medical context, we are going to see if the metrics are robust using a 5x2 cross validation and if it is not we are going to try to improve the model with variable selection techniques.

### Roc curve and auc

```{r}
# Calcular la curva ROC y el AUC para el conjunto de test
plot_roc_curve(predicciones_test,datos_test$ESTADO_ACTUAL)

```

:::

## 5x2 Cross Validation Filter


```{r,warning=FALSE,message=FALSE, eval=FALSE}
start_time <- Sys.time()
# Establecer la fórmula base para el modelo logístico
formula_base <- formula(ESTADO_ACTUAL ~ 1)  # Fórmula inicial sin predictoras

# Obtener nombres de variables predictoras
variables <- names(data)[-which(names(data) == "ESTADO_ACTUAL")]

# Obtener todas las combinaciones posibles de variables predictoras
combinaciones_variables <- unlist(lapply(1:length(variables), function(x) combn(variables, x, simplify = FALSE)), recursive = FALSE)


# Configurar la validación cruzada 5x2
k_outer <- 5  # Número de pliegues externos
k_inner <- 2  # Número de pliegues internos
set.seed(123)  # Establecer semilla para reproducibilidad
folds_outer <- createFolds(data$ESTADO_ACTUAL, k = k_outer, list = FALSE)  # Obtener índices de pliegues externos

# Lista para almacenar resultados
resultados_auc <- numeric(k_outer)
resultados_sensibilidad <- numeric(k_outer)
resultados_accuracy <- numeric(k_outer)
indices <- 1:nrow(data)

# Inicializar matrices para almacenar predicciones y objetivos
predicciones_totales <- matrix(NA, nrow = nrow(data), ncol = k_outer)
targets_totales <- numeric(nrow(data))

# Iterar sobre cada pliegue externo y realizar el proceso de modelado
for (i_outer in 1:k_outer) {

  # Obtener los índices para el pliegue externo actual
  test_indices_outer <- indices[folds_outer==i_outer]  # Índices correspondientes al pliegue externo de test
  train_indices_outer <- setdiff(1:nrow(data), test_indices_outer)  # Índices correspondientes al conjunto de entrenamiento
  
  # Dividir datos en entrenamiento y test para el pliegue externo
  datos_entrenamiento_outer <- data[train_indices_outer, ]
  datos_test_outer <- data[test_indices_outer, ]
  

  # Configurar la validación cruzada interna
  folds_inner <- createFolds(datos_entrenamiento_outer$ESTADO_ACTUAL, k = k_inner, list = FALSE)  # Obtener índices de pliegues internos
  
  # Almacenar predicciones y test para formar una curva ROC general
  predicciones_auc <- numeric(nrow(datos_entrenamiento_outer))
  targets_auc <- numeric(nrow(datos_entrenamiento_outer))
  
  # Iterar sobre cada pliegue interno para selección de modelo
  indices_inner <- 1:nrow(datos_entrenamiento_outer)
  resultados_auc_inner <- numeric(k_inner)
  resultados_sensibilidad_inner <- numeric(k_inner)
  resultados_accuracy_inner <- numeric(k_inner)
  for (i_inner in 1:k_inner) {
    ###############################
    #### Aqui el filtrado##########
    ###############################
    filter_data <- filter_dependency(datos_entrenamiento_outer,"ESTADO_ACTUAL",colnames(datos_entrenamiento_outer),0.05)
    if ("ESTADO_ACTUAL.1" %in% colnames(filter_data)) {
      filter_data <- filter_data[, !colnames(filter_data) %in% "ESTADO_ACTUAL.1", drop = FALSE]
    }
#con poca variables solo unaa la vez o filtrado o wrapper
    
    
    
    # Obtener los índices para el pliegue interno actual
    test_indices_inner <- indices_inner[folds_inner==i_inner]  # Índices correspondientes al pliegue interno de test
    train_indices_inner <- setdiff(1:nrow(filter_data), test_indices_inner)  # Índices correspondientes al conjunto de entrenamiento interno
    
    # Dividir datos en entrenamiento y test para el pliegue interno
    datos_entrenamiento_inner <- filter_data[train_indices_inner, ]
    datos_validacion <- filter_data[test_indices_inner, ]
    
 
    # Aplicar oversampling en el conjunto de entrenamiento interno
    datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ .,
                                            data = datos_entrenamiento_inner,
                                            seed = 123,
                                            p = 0.28)$data
    
        # Definir la fórmula para el modelo
    formula <- ESTADO_ACTUAL ~ .
    
    # Definir la función para calcular el custom metric
    compute_custom_metric <- function(recall, accuracy, sensitivity_weight = 0.8) {
      custom_metric <- (sensitivity_weight * recall) + ((1 - sensitivity_weight) * accuracy)
      return(custom_metric)
    }
    
    # Inicializar variables para almacenar los resultados
    mejor_custom_metric <- 0
    mejor_modelo <- NULL
    
    # Definir hiperparámetros a probar
    params <- expand.grid(cost = c(100, 1000, 10000, 50000, 1000000),
                          gamma = c(0.1, 1, 10),
                          kernel = c("linear", "radial", "sigmoid"))
    
    # Bucle para probar diferentes combinaciones de hiperparámetros
    for (i in 1:nrow(params)) {
      cost <- params$cost[i]
      gamma <- params$gamma[i]
      kernel <- params$kernel[i]
    
      # Entrenar el modelo SVM con la combinación de hiperparámetros actual
      modelo <- svm(formula, data = datos_entrenamiento_oversampled, type = "C-classification", kernel = kernel, cost = cost, gamma = gamma, probability = TRUE)
      
      # Realizar predicciones con probabilidades en datos de validación
      predicciones_validacion <- predict(modelo, newdata = datos_validacion, probability = TRUE)
      predicciones_validacion_prob <- attr(predicciones_validacion, "probabilities")[, "SI"]  # Asumiendo que "SI" es la clase positiva
      
      # Calcular las predicciones finales basadas en probabilidades
      umbral <- 0.5
      predicciones_finales <- ifelse(predicciones_validacion_prob >= umbral, "SI", "NO")
      
      # Calcular la sensibilidad y la precisión
      TP <- sum(predicciones_finales == "SI" & datos_validacion[["ESTADO_ACTUAL"]] == "SI")
      FN <- sum(predicciones_finales == "NO" & datos_validacion[["ESTADO_ACTUAL"]] == "SI")
      FP <- sum(predicciones_finales == "SI" & datos_validacion[["ESTADO_ACTUAL"]] == "NO")
      TN <- sum(predicciones_finales == "NO" & datos_validacion[["ESTADO_ACTUAL"]] == "NO")
      
      recall_actual <- TP / (TP + FN)
      accuracy_actual <- (TP + TN) / (TP + FN + FP + TN)
      
      # Calcular el custom metric para la combinación actual
      custom_metric_actual <- compute_custom_metric(recall_actual, accuracy_actual, sensitivity_weight)
      
      # Actualizar el mejor modelo y su rendimiento si se encuentra un modelo mejor
      if (custom_metric_actual > mejor_custom_metric) {
        mejor_custom_metric <- custom_metric_actual
        mejor_modelo <- modelo
      }
    }
    ########################
    # Calcular las predicciones para el conjunto de test
    predicciones_test <- predict(mejor_modelo, newdata = datos_test, probability = TRUE)
    predicciones_test <- attr(predicciones_test, "probabilities")[, "SI"]  # Asumiendo que "SI" es la clase positiva
    
    ###########################
    
    # Realizar predicciones en el conjunto de entrenamiento externo para el AUC general
    predicciones_auc[test_indices_inner] <- predicciones_test
    predicciones_clasificadas <- ifelse(predicciones_auc[test_indices_inner] >= 0.5, "SI", "NO")
  
    resultados_sensibilidad_inner[i_inner] <- sum(clases_predichas_test == "SI" & datos_validacion[["ESTADO_ACTUAL"]] == "SI") / sum(datos_validacion[["ESTADO_ACTUAL"]] == "SI")
    resultados_accuracy_inner[i_inner] <- mean(clases_predichas_test == datos_validacion[["ESTADO_ACTUAL"]])

    targets_auc[test_indices_inner] <- as.numeric(datos_validacion$ESTADO_ACTUAL) - 1

  }
  
  # Calcular el AUC final para este fold externo
  auc_fold_externo <- pROC::roc(targets_auc, predicciones_auc)$auc
  accuracy_fold_externo <- mean(resultados_accuracy_inner)
  sensibilidad_fold_externo <- mean(resultados_sensibilidad_inner)
 
  
  # Almacenar el AUC del fold externo actual
  resultados_auc[i_outer] <- auc_fold_externo
  resultados_accuracy[i_outer] <- accuracy_fold_externo
  resultados_sensibilidad[i_outer] <- sensibilidad_fold_externo
  
  # Predecir en los datos de test para este pliegue externo y almacenar predicciones
  predicciones_test <- predict(mejor_modelo, newdata = datos_test_outer, probability = TRUE)
  predicciones_totales[test_indices_outer, i_outer] <- predicciones_test
  targets_totales[test_indices_outer] <- as.numeric(datos_test_outer$ESTADO_ACTUAL) - 1
 
}

# Calcular AUC promedio
auc_promedio <- mean(resultados_auc)
accuracy_promedio <- mean(resultados_accuracy)
sensibilidad_promedio <- mean(resultados_sensibilidad)


# Crear tabla con todas las métricas
tabla_metricas <- data.frame(
  Pliegue = 1:k_outer,
  AUC = sapply(resultados_auc, round, digits = 3),
  Accuracy = sapply(resultados_accuracy, round, digits = 3),
  Recall = sapply(resultados_sensibilidad, round, digits = 3)
)


# Agregar fila con promedio de métricas
tabla_metricas_svm_filter <- rbind(tabla_metricas, c("Mean", round(auc_promedio,3), round(accuracy_promedio,3), round(sensibilidad_promedio,3)))

end_time <- Sys.time()
execution_time_svm_filter <- end_time - start_time
execution_time_svm_filter <- as.numeric(execution_time_svm_filter, units = "secs")
save(tabla_metricas_svm_filter, file = "tabla_metricas_svm_filter.RData")
save(execution_time_svm_filter, file = "execution_time_svm_filter.RData")
```

```{r}
load("tabla_metricas_svm_filter.RData")
load("execution_time_svm_filter.RData")

cat("Execution time:", execution_time_svm_filter,  "seconds \n")
```

::: {.panel-tabset}

# Metrics Table
```{r}

# Imprimir tabla formateada con kableExtra
kable(tabla_metricas_svm_filter, caption = "Cross Validation Metrics", align = "c") %>%
  kable_styling(full_width = FALSE)
```

# Metrics Plot
```{r}
crear_plot_metricas_per_fold(tabla_metricas_svm_filter)

```
:::
As we suspected the model is not robust and the metrics are not good. To avoid unnecesary complexity we are not going to implement any variable selection technique since the model does not have a good performance.

## Hyperparameters Resume

This is a resume of the model performance with every hyperparameters combination. The next table shows the top 10 and bottom 10 models by AUC.

```{r,message=FALSE,warning=FALSE,eval=FALSE}

start_time <- Sys.time()

# Dividir el conjunto de datos en entrenamiento y test
set.seed(123)  # Establecer una semilla para reproducibilidad
indices_entrenamiento <- sample(nrow(data), 0.75 * nrow(data))  # 75% para entrenamiento
datos_entrenamiento <- data[indices_entrenamiento, ]
datos_test <- data[-indices_entrenamiento, ]

# Dividir datos de entrenamiento en entrenamiento y validación
train_indices <- sample(nrow(datos_entrenamiento), 0.8 * nrow(datos_entrenamiento))  # 80% para entrenamiento
datos_entrenamiento_split <- datos_entrenamiento[train_indices, ]
datos_validacion <- datos_entrenamiento[-train_indices, ]

# Oversampling
proportion <- 0.28
datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ ., data = datos_entrenamiento_split, p = proportion, seed = 123)$data

# Inicializar variables para almacenar los resultados
mejor_auc <- 0
mejor_modelo <- NULL
resultados <- data.frame(kernel = character(), cost = numeric(), gamma = numeric(), auc = numeric(), stringsAsFactors = FALSE)

# Definir hiperparámetros a probar
params <- expand.grid(cost = c(100, 1000, 10000, 50000, 1000000),
                      gamma = c(0.1, 1, 10),
                      kernel = c("linear", "radial", "sigmoid"))

# Bucle para probar diferentes combinaciones de hiperparámetros
for (i in 1:nrow(params)) {
  cost <- params$cost[i]
  gamma <- params$gamma[i]
  kernel <- params$kernel[i]

  # Entrenar el modelo SVM con la combinación de hiperparámetros actual
  modelo <- svm(ESTADO_ACTUAL ~ ., data = datos_entrenamiento_oversampled, type = "C-classification", kernel = kernel, cost = cost, gamma = gamma, probability = TRUE)

  # Realizar predicciones en datos de prueba
  predicciones_test <- predict(modelo, newdata = datos_test, probability = TRUE)
  
  # Calcular AUC para la combinación actual en datos de prueba
  roc_obj <- roc(ifelse(datos_test$ESTADO_ACTUAL == "SI", 1, 0), attr(predicciones_test, "probabilities")[, 2])
  auc_actual <- as.numeric(auc(roc_obj))  # Extraer el valor numérico del AUC
  
  # Guardar los resultados en la tabla
  resultados <- resultados %>%
    add_row(kernel = kernel, cost = cost, gamma = gamma, auc = auc_actual)
  
  # Actualizar el mejor modelo y su AUC si se encuentra un modelo mejor
  if (auc_actual > mejor_auc) {
    mejor_auc <- auc_actual
    mejor_modelo <- modelo
  }
}

# Ordenar los resultados por AUC en orden descendente
resultados_ordenados_svm <- resultados %>% arrange(desc(auc))


# Mostrar la tabla con los resultados de todas las combinaciones de hiperparámetros
save(resultados_ordenados_svm, file = "resultados_ordenados_svm.RData")

end_time <- Sys.time()

```

```{r}
# Cargar los datos guardados y mostrar las tablas
load("resultados_ordenados_svm.RData")


# Seleccionar los mejores 10 modelos
mejores_10 <- resultados_ordenados_svm %>% head(10)

# Seleccionar los peores 10 modelos
peores_10 <- resultados_ordenados_svm %>% tail(10)

# Imprimir los mejores y peores modelos en tablas bonitas
mejores_10 %>%
  kbl(caption = "Top 10 Models by AUC") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

peores_10 %>%
  kbl(caption = "Bottom 10 Models by AUC") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```
It seem that the best models have a cost of 100 and a gamma of 0.1. But the most importatnt hyperparameter is linear kernel. These performance measuraments are extremely optimistic and do not correspond to the real performance of the model but it can help us to understand the hyperparameters impact on the model. We have to ve careful with some hyperparameters that can lead to overfitting.

```{r}

library(plotly)

# Cargar los resultados
load("resultados_ordenados_svm.RData")

# Crear la gráfica 3D con plotly
fig <- plot_ly(
  data = resultados_ordenados_svm,
  x = ~cost,
  y = ~gamma,
  z = ~auc,
  color = ~kernel,
  colors = c("#3C5B6F", "steelblue", "lightblue"),
  type = "scatter3d",
  mode = "markers",
  marker = list(size = 5)
)


fig <- fig %>%
  layout(
    title = "AUC for every combination of hyperparameters",
    scene = list(
      xaxis = list(title = "Cost", type = "log"),
      yaxis = list(title = "Gamma", type = "log"),
      zaxis = list(title = "AUC")
    )
  )

# Mostrar la gráfica
fig

```

:::


# Decision tree {#decision-tree}

::: {.panel-tabset style="background-color: #f0f0f0;box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);border-radius: 5px;"}

## Hold out

In this section we are going to build a decision tree model using different hyperparameters like cp, minsplit and minbucket and evaluate it using a hold out validation.

```{r}

# Iniciar tiempo de ejecución
start_time <- Sys.time()

# Parámetros a cambiar
proportion <- 0.28

# Dividir el conjunto de datos en entrenamiento y test
indices_entrenamiento <- sample(nrow(data), 0.75 * nrow(data))  # 75% para entrenamiento
datos_entrenamiento <- data[indices_entrenamiento, ]
datos_test <- data[-indices_entrenamiento, ]

# Dividir datos de entrenamiento en entrenamiento y validación
train_indices <- sample(nrow(datos_entrenamiento), 0.8 * nrow(datos_entrenamiento))  # 80% para entrenamiento
datos_entrenamiento_split <- datos_entrenamiento[train_indices, ]
datos_validacion <- datos_entrenamiento[-train_indices, ]

# Sobremuestrear los datos de entrenamiento
datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ ., data = datos_entrenamiento_split, p = proportion)$data

# Definir la fórmula para el modelo
formula <- ESTADO_ACTUAL ~ .

# Inicializar variables para almacenar los resultados
mejor_auc <- 0
mejor_modelo <- NULL

# Definir combinaciones de hiperparámetros a probar
param_grid <- expand.grid(cp = c(0.01, 0.05, 0.1), 
                          minsplit = c(5, 10, 20), 
                          minbucket = c(5, 10, 20))

# Bucle para probar diferentes combinaciones de hiperparámetros
for (params in 1:nrow(param_grid)) {
  cp <- param_grid$cp[params]
  minsplit <- param_grid$minsplit[params]
  minbucket <- param_grid$minbucket[params]
  
  # Entrenar el modelo de árbol de decisión con la combinación de hiperparámetros actual
  modelo <- rpart(formula, data = datos_entrenamiento_oversampled, 
                  control = rpart.control(cp = cp, minsplit = minsplit, minbucket = minbucket), 
                  method = "class")
  
  # Realizar predicciones en datos de validación
  prob_pred_validacion <- predict(modelo, newdata = datos_validacion, type = "prob")[,2]
  
  # Calcular el AUC
  roc_obj <- roc(datos_validacion[["ESTADO_ACTUAL"]], prob_pred_validacion, levels = rev(levels(datos_validacion[["ESTADO_ACTUAL"]])))
  auc_actual <- auc(roc_obj)
  
  # Actualizar el mejor modelo y su rendimiento si se encuentra un modelo mejor
  if (auc_actual > mejor_auc) {
    mejor_auc <- auc_actual
    mejor_modelo <- modelo
  }
}

# Graficar el mejor modelo
rpart.plot(mejor_modelo)

# Detener el tiempo de ejecución
end_time <- Sys.time()
execution_time <- end_time - start_time

# Imprimir el tiempo de ejecución y el mejor AUC
cat("Execution time:", execution_time,  "seconds \n")

```
::: {.panel-tabset}

### Predictions



```{r}
# Calcular las predicciones para el conjunto de test
predicciones_test <- predict(mejor_modelo, newdata = datos_test, probability = TRUE)
predicciones_test <- predicciones_test[, "SI"]  # Asumiendo que "SI" es la clase positiva
clases_predichas_test <- ifelse(predicciones_test >= 0.5, "SI", "NO")

mostrar_metricas(datos_test$ESTADO_ACTUAL,clases_predichas_test)

```


### Roc curve and auc

```{r}
# Calcular la curva ROC y el AUC para el conjunto de test
plot_roc_curve(predicciones_test,datos_test$ESTADO_ACTUAL)

```
:::
The performance is bad compared to the previous models. We are going to see if the metrics are robust using a 5x2 cross validation.

## 5x2 Cross Validation Filter


```{r,warning=FALSE,message=FALSE, eval=FALSE}

# Iniciar tiempo de ejecución
start_time <- Sys.time()

# Parámetros a cambiar
proportion <- 0.28

# Configurar la validación cruzada 5x3
k_outer <- 5  # Número de pliegues externos
k_inner <- 3  # Número de pliegues internos
folds_outer <- createFolds(data$ESTADO_ACTUAL, k = k_outer, list = FALSE)  # Obtener índices de pliegues externos

# Lista para almacenar resultados
resultados_auc <- numeric(k_outer)
resultados_accuracy <- numeric(k_outer)
resultados_sensibilidad <- numeric(k_outer)
indices <- 1:nrow(data)

# Inicializar matrices para almacenar predicciones y objetivos
predicciones_totales <- matrix(NA, nrow = nrow(data), ncol = k_outer)
targets_totales <- numeric(nrow(data))

# Iterar sobre cada pliegue externo y realizar el proceso de modelado
for (i_outer in 1:k_outer) {

  # Obtener los índices para el pliegue externo actual
  test_indices_outer <- indices[folds_outer == i_outer]  # Índices correspondientes al pliegue externo de test
  train_indices_outer <- setdiff(1:nrow(data), test_indices_outer)  # Índices correspondientes al conjunto de entrenamiento
  
  # Dividir datos en entrenamiento y test para el pliegue externo
  datos_entrenamiento_outer <- data[train_indices_outer, ]
  datos_test_outer <- data[test_indices_outer, ]
  
  # Configurar la validación cruzada interna
  folds_inner <- createFolds(datos_entrenamiento_outer$ESTADO_ACTUAL, k = k_inner, list = FALSE)  # Obtener índices de pliegues internos
  
  # Almacenar predicciones y objetivos para el AUC general
  predicciones_auc <- numeric(nrow(datos_entrenamiento_outer))
  targets_auc <- numeric(nrow(datos_entrenamiento_outer))
  
  # Inicializar variables para almacenar resultados internos
  resultados_auc_inner <- numeric(k_inner)
  resultados_accuracy_inner <- numeric(k_inner)
  resultados_sensibilidad_inner <- numeric(k_inner)
  
  # Iterar sobre cada pliegue interno para selección de modelo
  indices_inner <- 1:nrow(datos_entrenamiento_outer)
  for (i_inner in 1:k_inner) {
    ###############################
    #### Aqui el filtrado##########
    filter_data <- filter_dependency(datos_entrenamiento_outer,"ESTADO_ACTUAL",colnames(datos_entrenamiento_outer),0.05)
    if ("ESTADO_ACTUAL.1" %in% colnames(filter_data)) {
      filter_data <- filter_data[, !colnames(filter_data) %in% "ESTADO_ACTUAL.1", drop = FALSE]
    }
#con poca variables solo unaa la vez o filtrado o wrapper
    
    
    
    # Obtener los índices para el pliegue interno actual
    test_indices_inner <- indices_inner[folds_inner==i_inner]  # Índices correspondientes al pliegue interno de test
    train_indices_inner <- setdiff(1:nrow(filter_data), test_indices_inner)  # Índices correspondientes al conjunto de entrenamiento interno
    
    # Dividir datos en entrenamiento y test para el pliegue interno
    datos_entrenamiento_inner <- filter_data[train_indices_inner, ]
    datos_validacion <- filter_data[test_indices_inner, ]
    
    # Aplicar oversampling en el conjunto de entrenamiento interno
    datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ ., data = datos_entrenamiento_inner, p = proportion)$data
    
    # Definir la fórmula para el modelo
    formula <- ESTADO_ACTUAL ~ .
    
    # Inicializar variables para almacenar el mejor modelo y su AUC
    mejor_auc <- 0
    mejor_modelo <- NULL
    
    # Definir combinaciones de hiperparámetros a probar
    param_grid <- expand.grid(cp = c(0.01, 0.05, 0.1), 
                              minsplit = c(5, 10, 20), 
                              minbucket = c(5, 10, 20))
    
    # Bucle para probar diferentes combinaciones de hiperparámetros
    for (params in 1:nrow(param_grid)) {
      cp <- param_grid$cp[params]
      minsplit <- param_grid$minsplit[params]
      minbucket <- param_grid$minbucket[params]
      
      # Entrenar el modelo de árbol de decisión con la combinación de hiperparámetros actual
      modelo <- rpart(formula, data = datos_entrenamiento_oversampled, 
                      control = rpart.control(cp = cp, minsplit = minsplit, minbucket = minbucket), 
                      method = "class")
      
      # Realizar predicciones con probabilidades en datos de validación
      predicciones_validacion_prob <- predict(modelo, newdata = datos_validacion, type = "prob")[,2]
      
      # Calcular el AUC
      roc_obj <- roc(datos_validacion[["ESTADO_ACTUAL"]], predicciones_validacion_prob, levels = rev(levels(datos_validacion[["ESTADO_ACTUAL"]])))
      auc_actual <- auc(roc_obj)
      
      # Actualizar el mejor modelo y su rendimiento si se encuentra un modelo mejor
      if (auc_actual > mejor_auc) {
        mejor_auc <- auc_actual
        mejor_modelo <- modelo
      }
    }
    
    # Calcular las predicciones para el conjunto de validación interno
    predicciones_validacion_prob <- predict(mejor_modelo, newdata = datos_validacion, type = "prob")[,2]
    
    # Almacenar las predicciones y objetivos
    predicciones_auc[test_indices_inner] <- predicciones_validacion_prob
    targets_auc[test_indices_inner] <- as.numeric(datos_validacion$ESTADO_ACTUAL) - 1
    
    # Calcular la sensibilidad y la precisión
    umbral <- 0.5
    predicciones_finales <- ifelse(predicciones_validacion_prob >= umbral, "SI", "NO")
    TP <- sum(predicciones_finales == "SI" & datos_validacion[["ESTADO_ACTUAL"]] == "SI")
    FN <- sum(predicciones_finales == "NO" & datos_validacion[["ESTADO_ACTUAL"]] == "SI")
    FP <- sum(predicciones_finales == "SI" & datos_validacion[["ESTADO_ACTUAL"]] == "NO")
    TN <- sum(predicciones_finales == "NO" & datos_validacion[["ESTADO_ACTUAL"]] == "NO")
    
    recall_actual <- TP / (TP + FN)
    accuracy_actual <- (TP + TN) / (TP + FN + FP + TN)
    
    # Almacenar resultados internos
    resultados_sensibilidad_inner[i_inner] <- recall_actual
    resultados_accuracy_inner[i_inner] <- accuracy_actual
    resultados_auc_inner[i_inner] <- auc_actual
  }
  
  # Calcular el AUC final para este pliegue externo
  auc_fold_externo <- mean(resultados_auc_inner)
  accuracy_fold_externo <- mean(resultados_accuracy_inner)
  sensibilidad_fold_externo <- mean(resultados_sensibilidad_inner)
  
  # Almacenar el AUC del fold externo actual
  resultados_auc[i_outer] <- auc_fold_externo
  resultados_accuracy[i_outer] <- accuracy_fold_externo
  resultados_sensibilidad[i_outer] <- sensibilidad_fold_externo
  
  # Predecir en los datos de test para este pliegue externo y almacenar predicciones
  predicciones_test_outer_prob <- predict(mejor_modelo, newdata = datos_test_outer, type = "prob")[,2]
  predicciones_totales[test_indices_outer, i_outer] <- predicciones_test_outer_prob
  targets_totales[test_indices_outer] <- as.numeric(datos_test_outer$ESTADO_ACTUAL) - 1
}

# Calcular AUC promedio
auc_promedio <- mean(resultados_auc)
accuracy_promedio <- mean(resultados_accuracy)
sensibilidad_promedio <- mean(resultados_sensibilidad)

# Crear tabla con todas las métricas
tabla_metricas <- data.frame(
  Pliegue = 1:k_outer,
  AUC = sapply(resultados_auc, round, digits = 3),
  Accuracy = sapply(resultados_accuracy, round, digits = 3),
  Recall = sapply(resultados_sensibilidad, round, digits = 3)
)

# Agregar fila con promedio de métricas
tabla_metricas_dtree_filter <- rbind(tabla_metricas, c("Mean", round(auc_promedio, 3), round(accuracy_promedio, 3), round(sensibilidad_promedio, 3)))


# Detener el tiempo de ejecución
end_time <- Sys.time()
execution_time_dtree_filter <- end_time - start_time
execution_time_dtree_filter <- as.numeric(execution_time_dtree_filter, units = "secs")
save(tabla_metricas_dtree_filter, file = "tabla_metricas_dtree_filter.RData")
save(execution_time_dtree_filter, file = "execution_time_dtree_filter.RData")
```

```{r}
load("tabla_metricas_dtree_filter.RData")
load("execution_time_dtree_filter.RData")
# Imprimir el tiempo de ejecución
cat("Execution time:", execution_time_dtree_filter,  "seconds \n")

```


::: {.panel-tabset}

# Metrics Table
```{r}

# Imprimir tabla formateada con kableExtra
kable(tabla_metricas_dtree_filter, caption = "Cross Validation Metrics", align = "c") %>%
  kable_styling(full_width = FALSE)
```

# Metrics Plot
```{r}
crear_plot_metricas_per_fold(tabla_metricas_dtree_filter)

```
:::

The model is instable and the performance metrics are not good. In the next section we are going to implement a random forest model to see if we can improve the performance.
:::
# Random Forest {#random-forest}

::: {.panel-tabset style="background-color: #f0f0f0;box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);border-radius: 5px;"}
## Hold out

The objetive of this section is to better the decision tree model using a random forest model. We are going to use the same hyperparameters as the decision tree model and evaluate it using a hold out validation. The hyperparameters are cp, minsplit and minbucket.


```{r}
start_time <- Sys.time()
# Parámetros a cambiar
proportion <- 0.28
sensitivity_weight <- 0.8

# Dividir el conjunto de datos en entrenamiento y test
set.seed(123)  # Establecer una semilla para reproducibilidad
indices_entrenamiento <- sample(nrow(data), 0.75 * nrow(data))  # 75% para entrenamiento
datos_entrenamiento <- data[indices_entrenamiento, ]
datos_test <- data[-indices_entrenamiento, ]

# Dividir datos de entrenamiento en entrenamiento y validación
train_indices <- sample(nrow(datos_entrenamiento), 0.8 * nrow(datos_entrenamiento))  # 80% para entrenamiento
datos_entrenamiento_split <- datos_entrenamiento[train_indices, ]
datos_validacion <- datos_entrenamiento[-train_indices, ]

# Realizar sobremuestreo en los datos de entrenamiento
datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ ., data = datos_entrenamiento_split, seed = 123, p = proportion)$data

# Definir la función para calcular el métrico personalizado
compute_custom_metric <- function(recall, accuracy, sensitivity_weight = 1) {
  custom_metric <- (sensitivity_weight * recall) + ((1 - sensitivity_weight) * accuracy)
  return(custom_metric)
}

# Definir la fórmula para el modelo
formula <- ESTADO_ACTUAL ~ .

# Inicializar variables para almacenar los resultados
mejor_custom_metric <- 0
mejor_modelo <- NULL

# Definir combinaciones de hiperparámetros a probar
param_grid <- expand.grid(mtry = c(2, 3, 4), 
                          ntree = c(100, 200, 300),
                          nodesize = c(5, 10, 15))

# Bucle para probar diferentes combinaciones de hiperparámetros
for (params in 1:nrow(param_grid)) {
  mtry <- param_grid$mtry[params]
  ntree <- param_grid$ntree[params]
  nodesize <- param_grid$nodesize[params]
  
  # Entrenar el modelo Random Forest con la combinación de hiperparámetros actual
  modelo <- randomForest(formula, data = datos_entrenamiento_oversampled,
                         mtry = mtry, ntree = ntree, nodesize = nodesize)
  
  # Realizar predicciones en datos de validación
  predicciones_validacion <- predict(modelo, newdata = datos_validacion, type = "response")
  
  # Calcular la sensibilidad y la precisión
  TP <- sum(predicciones_validacion == "SI" & datos_validacion[["ESTADO_ACTUAL"]] == "SI")
  FN <- sum(predicciones_validacion == "NO" & datos_validacion[["ESTADO_ACTUAL"]] == "SI")
  FP <- sum(predicciones_validacion == "SI" & datos_validacion[["ESTADO_ACTUAL"]] == "NO")
  TN <- sum(predicciones_validacion == "NO" & datos_validacion[["ESTADO_ACTUAL"]] == "NO")
  
  recall_actual <- TP / (TP + FN)
  accuracy_actual <- (TP + TN) / (TP + FN + FP + TN)
  
  # Calcular el métrico personalizado para la combinación actual
  custom_metric_actual <- compute_custom_metric(recall_actual, accuracy_actual, sensitivity_weight)
  
  # Actualizar el mejor modelo y su rendimiento si se encuentra un modelo mejor
  if (custom_metric_actual > mejor_custom_metric) {
    mejor_custom_metric <- custom_metric_actual
    mejor_modelo <- modelo
  }
}

# Evaluar el mejor modelo en el conjunto de prueba
predicciones_test <- predict(mejor_modelo, newdata = datos_test, type = "response")

summary(mejor_modelo)
end_time <- Sys.time()
execution_time <- end_time - start_time
execution_time <- as.numeric(execution_time, units = "secs")
cat("Execution time:", execution_time,  "seconds \n")

```
::: {.panel-tabset}

### Predictions


```{r}

# Calcular las predicciones para el conjunto de test
predicciones_test <- predict(mejor_modelo, newdata = datos_test, type = "prob")
predicciones_test <- predicciones_test[, "SI"]  # Asumiendo que "SI" es la clase positiva
clases_predichas_test <- ifelse(predicciones_test >= 0.5, "SI", "NO")
mostrar_metricas(datos_test$ESTADO_ACTUAL,clases_predichas_test)
```



### Roc curve and auc

```{r}
# Calcular la curva ROC y el AUC para el conjunto de test
plot_roc_curve(predicciones_test,datos_test$ESTADO_ACTUAL)

```



:::

We obtain much better results than the decision tree model. The next step is to ensure that the model is robust using a 5x2 cross validation.



## 5x2 Cross Validation Filter

We are going to perform a 5x2 cross validation to ensure the robustness of the model.
First, we create the outer folds and then we iterate over them to create the inner folds and perform the model selection process.


```{r,warning=FALSE,message=FALSE, eval=FALSE}
# Iniciar tiempo de ejecución
start_time <- Sys.time()

# Parámetros a cambiar
proportion <- 0.28

# Configurar la validación cruzada 5x3
k_outer <- 5  # Número de pliegues externos
k_inner <- 3  # Número de pliegues internos
set.seed(123)  # Establecer semilla para reproducibilidad
folds_outer <- createFolds(data$ESTADO_ACTUAL, k = k_outer, list = FALSE)  # Obtener índices de pliegues externos

# Lista para almacenar resultados
resultados_auc <- numeric(k_outer)
resultados_accuracy <- numeric(k_outer)
resultados_sensibilidad <- numeric(k_outer)
indices <- 1:nrow(data)

# Inicializar matrices para almacenar predicciones y objetivos
predicciones_totales <- matrix(NA, nrow = nrow(data), ncol = k_outer)
targets_totales <- numeric(nrow(data))

# Iterar sobre cada pliegue externo y realizar el proceso de modelado
for (i_outer in 1:k_outer) {

  # Obtener los índices para el pliegue externo actual
  test_indices_outer <- indices[folds_outer == i_outer]  # Índices correspondientes al pliegue externo de test
  train_indices_outer <- setdiff(1:nrow(data), test_indices_outer)  # Índices correspondientes al conjunto de entrenamiento
  
  # Dividir datos en entrenamiento y test para el pliegue externo
  datos_entrenamiento_outer <- data[train_indices_outer, ]
  datos_test_outer <- data[test_indices_outer, ]
  
  # Configurar la validación cruzada interna
  folds_inner <- createFolds(datos_entrenamiento_outer$ESTADO_ACTUAL, k = k_inner, list = FALSE)  # Obtener índices de pliegues internos
  
  # Almacenar predicciones y objetivos para el AUC general
  predicciones_auc <- numeric(nrow(datos_entrenamiento_outer))
  targets_auc <- numeric(nrow(datos_entrenamiento_outer))
  
  # Inicializar variables para almacenar resultados internos
  resultados_auc_inner <- numeric(k_inner)
  resultados_accuracy_inner <- numeric(k_inner)
  resultados_sensibilidad_inner <- numeric(k_inner)
  
  # Iterar sobre cada pliegue interno para selección de modelo
  indices_inner <- 1:nrow(datos_entrenamiento_outer)
  for (i_inner in 1:k_inner) {
    # Obtener los índices para el pliegue interno actual
    test_indices_inner <- indices_inner[folds_inner == i_inner]  # Índices correspondientes al pliegue interno de test
    train_indices_inner <- setdiff(1:nrow(datos_entrenamiento_outer), test_indices_inner)  # Índices correspondientes al conjunto de entrenamiento interno
    
    # Dividir datos en entrenamiento y test para el pliegue interno
    datos_entrenamiento_inner <- datos_entrenamiento_outer[train_indices_inner, ]
    datos_validacion <- datos_entrenamiento_outer[test_indices_inner, ]
    
    # Aplicar oversampling en el conjunto de entrenamiento interno
    datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ ., data = datos_entrenamiento_inner, seed = 123, p = proportion)$data
    
    # Definir la fórmula para el modelo
    formula <- ESTADO_ACTUAL ~ .
    
    # Inicializar variables para almacenar el mejor modelo y su AUC
    mejor_auc <- 0
    mejor_modelo <- NULL
    
    # Definir combinaciones de hiperparámetros a probar
    param_grid <- expand.grid(mtry = c(2, 3, 4), 
                              ntree = c(100, 200, 500),
                              maxnodes = c(10, 15, 20))
    
    # Bucle para probar diferentes combinaciones de hiperparámetros
    for (params in 1:nrow(param_grid)) {
      mtry <- param_grid$mtry[params]
      ntree <- param_grid$ntree[params]
      maxnodes <- param_grid$maxnodes[params]
      
      # Entrenar el modelo de bosque aleatorio con la combinación de hiperparámetros actual
      modelo <- randomForest(formula, data = datos_entrenamiento_oversampled, 
                             mtry = mtry, ntree = ntree, maxnodes = maxnodes)
      
      # Realizar predicciones con probabilidades en datos de validación
      predicciones_validacion_prob <- predict(modelo, newdata = datos_validacion, type = "prob")[,2]
      
      # Calcular el AUC
      roc_obj <- roc(datos_validacion[["ESTADO_ACTUAL"]], predicciones_validacion_prob, levels = rev(levels(datos_validacion[["ESTADO_ACTUAL"]])))
      auc_actual <- auc(roc_obj)
      
      # Actualizar el mejor modelo y su rendimiento si se encuentra un modelo mejor
      if (auc_actual > mejor_auc) {
        mejor_auc <- auc_actual
        mejor_modelo <- modelo
      }
    }
    
    # Calcular las predicciones para el conjunto de validación interno
    predicciones_validacion_prob <- predict(mejor_modelo, newdata = datos_validacion, type = "prob")[,2]
    
    # Almacenar las predicciones y objetivos
    predicciones_auc[test_indices_inner] <- predicciones_validacion_prob
    targets_auc[test_indices_inner] <- as.numeric(datos_validacion$ESTADO_ACTUAL) - 1
    
    # Calcular la sensibilidad y la precisión
    umbral <- 0.5
    predicciones_finales <- ifelse(predicciones_validacion_prob >= umbral, "SI", "NO")
    TP <- sum(predicciones_finales == "SI" & datos_validacion[["ESTADO_ACTUAL"]] == "SI")
    FN <- sum(predicciones_finales == "NO" & datos_validacion[["ESTADO_ACTUAL"]] == "SI")
    FP <- sum(predicciones_finales == "SI" & datos_validacion[["ESTADO_ACTUAL"]] == "NO")
    TN <- sum(predicciones_finales == "NO" & datos_validacion[["ESTADO_ACTUAL"]] == "NO")
    
    recall_actual <- TP / (TP + FN)
    accuracy_actual <- (TP + TN) / (TP + FN + FP + TN)
    
    # Almacenar resultados internos
    resultados_sensibilidad_inner[i_inner] <- recall_actual
    resultados_accuracy_inner[i_inner] <- accuracy_actual
    resultados_auc_inner[i_inner] <- auc_actual
  }
  
  # Calcular el AUC final para este pliegue externo
  auc_fold_externo <- mean(resultados_auc_inner)
  accuracy_fold_externo <- mean(resultados_accuracy_inner)
  sensibilidad_fold_externo <- mean(resultados_sensibilidad_inner)
  
  # Almacenar el AUC del fold externo actual
  resultados_auc[i_outer] <- auc_fold_externo
  resultados_accuracy[i_outer] <- accuracy_fold_externo
  resultados_sensibilidad[i_outer] <- sensibilidad_fold_externo
  
  # Predecir en los datos de test para este pliegue externo y almacenar predicciones
  predicciones_test_outer_prob <- predict(mejor_modelo, newdata = datos_test_outer, type = "prob")[,2]
  predicciones_totales[test_indices_outer, i_outer] <- predicciones_test_outer_prob
  targets_totales[test_indices_outer] <- as.numeric(datos_test_outer$ESTADO_ACTUAL) - 1
}

# Calcular AUC promedio
auc_promedio <- mean(resultados_auc)
accuracy_promedio <- mean(resultados_accuracy)
sensibilidad_promedio <- mean(resultados_sensibilidad)

# Crear tabla con todas las métricas
tabla_metricas <- data.frame(
  Pliegue = 1:k_outer,
  AUC = sapply(resultados_auc, round, digits = 3),
  Accuracy = sapply(resultados_accuracy, round, digits = 3),
  Recall = sapply(resultados_sensibilidad, round, digits = 3)
)

# Agregar fila con promedio de métricas
tabla_metricas_random_forest <- rbind(tabla_metricas, c("Mean", round(auc_promedio, 3), round(accuracy_promedio, 3), round(sensibilidad_promedio, 3)))



# Detener el tiempo de ejecución
end_time <- Sys.time()
execution_time_random_forest <- end_time - start_time
execution_time_random_forest <- as.numeric(execution_time_random_forest, units = "secs")
save(tabla_metricas_random_forest, file = "tabla_metricas_random_forest.RData")
save(execution_time_random_forest, file = "execution_time_random_forest.RData")


```

```{r}
load("tabla_metricas_random_forest.RData")
load("execution_time_random_forest.RData")
# Imprimir el tiempo de ejecución
cat("Execution time:", execution_time_random_forest,  "seconds \n")

```


::: {.panel-tabset}

# Metrics Table
```{r}

# Imprimir tabla formateada con kableExtra
kable(tabla_metricas_random_forest, caption = "Cross Validation Metrics", align = "c") %>%
  kable_styling(full_width = FALSE)
```

# Metrics Plot
```{r}
crear_plot_metricas_per_fold(tabla_metricas_random_forest)

```
:::

The model is robust and the performance metrics are good. But there there is no hope to surpass the performance of neural network so we are not going to use variable selection in the random forest model.

:::

# Naive Bayes {#naive-bayes}

::: {.panel-tabset style="background-color: #f0f0f0;box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);border-radius: 5px;"}

## Hold out

In this section we are going to implement a Naive Bayes model. The hyperparameters to optimize are usekernel and laplace.

```{r}

start_time <- Sys.time()

# Parámetros a cambiar
proportion <- 0.28

# Dividir el conjunto de datos en entrenamiento y test
set.seed(123)  # Establecer una semilla para reproducibilidad
indices_entrenamiento <- sample(nrow(data), 0.75 * nrow(data))  # 75% para entrenamiento
datos_entrenamiento <- data[indices_entrenamiento, ]
datos_test <- data[-indices_entrenamiento, ]

# Dividir datos de entrenamiento en entrenamiento y validación
train_indices <- sample(nrow(datos_entrenamiento), 0.8 * nrow(datos_entrenamiento))  # 80% para entrenamiento
datos_entrenamiento_split <- datos_entrenamiento[train_indices, ]
datos_validacion <- datos_entrenamiento[-train_indices, ]

# Realizar sobremuestreo en los datos de entrenamiento
datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ ., data = datos_entrenamiento_split, seed = 123, p = proportion)$data

# Definir la fórmula para el modelo
formula <- ESTADO_ACTUAL ~ .

# Inicializar variables para almacenar los resultados
mejor_auc <- 0
mejor_modelo <- NULL

# Definir combinaciones de hiperparámetros a probar
param_grid <- expand.grid(usekernel = c(FALSE, TRUE), 
                          laplace = c(FALSE, TRUE))

# Bucle para probar diferentes combinaciones de hiperparámetros
for (params in 1:nrow(param_grid)) {
  usekernel <- param_grid$usekernel[params]
  laplace <- param_grid$laplace[params]
  
  # Entrenar el modelo Naive Bayes con la combinación de hiperparámetros actual
  modelo <- naiveBayes(formula, data = datos_entrenamiento_oversampled, kernel = usekernel, laplace = laplace)
  
  # Realizar predicciones en datos de validación
  predicciones_validacion <- predict(modelo, newdata = datos_validacion, type = "raw")
  
  # Calcular AUC para la combinación actual
  roc_obj <- roc(ifelse(datos_validacion$ESTADO_ACTUAL == "SI", 1, 0), predicciones_validacion[, "SI"])
  auc_actual <- as.numeric(auc(roc_obj))  # Extraer el valor numérico del AUC
  
  # Actualizar el mejor modelo y su AUC si se encuentra un modelo mejor
  if (auc_actual > mejor_auc) {
    mejor_auc <- auc_actual
    mejor_modelo <- modelo
  }
}

# Resumen del mejor modelo
summary(mejor_modelo)

end_time <- Sys.time()
execution_time <- end_time - start_time
execution_time <- as.numeric(execution_time, units = "secs")
cat("Execution time:", execution_time, "seconds \n")

```
::: {.panel-tabset}

### Predictions

We are going to predict on test in order to get a estimation of the performance of the model.

```{r}

# Calcular las predicciones para el conjunto de test
predicciones_test <- predict(mejor_modelo, newdata = datos_test, type = "raw")
predicciones_test <- predicciones_test[, "SI"]  # Asumiendo que "SI" es la clase positiva
clases_predichas_test <- ifelse(predicciones_test >= 0.5, "SI", "NO")
mostrar_metricas(datos_test$ESTADO_ACTUAL,clases_predichas_test)
```

The performance of the model is definitively acceptable.
The last step is to ensure that the performance measurements are robust.

### Roc curve and auc

```{r}
# Calcular la curva ROC y el AUC para el conjunto de test
plot_roc_curve(predicciones_test,datos_test$ESTADO_ACTUAL)

```
The model is good in first instance. The next step is to ensure that the model is robust using a 5x2 cross validation.
:::

## 5x2 Cross Validation Filter


```{r,warning=FALSE,message=FALSE, eval=FALSE}


# Iniciar tiempo de ejecución
start_time <- Sys.time()

# Parámetros a cambiar
proportion <- 0.28

# Configurar la validación cruzada 5x3
k_outer <- 5  # Número de pliegues externos
k_inner <- 3  # Número de pliegues internos
set.seed(123)  # Establecer semilla para reproducibilidad
folds_outer <- createFolds(data$ESTADO_ACTUAL, k = k_outer, list = FALSE)  # Obtener índices de pliegues externos

# Lista para almacenar resultados
resultados_auc <- numeric(k_outer)
resultados_accuracy <- numeric(k_outer)
resultados_sensibilidad <- numeric(k_outer)
indices <- 1:nrow(data)

# Inicializar matrices para almacenar predicciones y objetivos
predicciones_totales <- matrix(NA, nrow = nrow(data), ncol = k_outer)
targets_totales <- numeric(nrow(data))

# Iterar sobre cada pliegue externo y realizar el proceso de modelado
for (i_outer in 1:k_outer) {

  # Obtener los índices para el pliegue externo actual
  test_indices_outer <- indices[folds_outer == i_outer]  # Índices correspondientes al pliegue externo de test
  train_indices_outer <- setdiff(1:nrow(data), test_indices_outer)  # Índices correspondientes al conjunto de entrenamiento
  
  # Dividir datos en entrenamiento y test para el pliegue externo
  datos_entrenamiento_outer <- data[train_indices_outer, ]
  datos_test_outer <- data[test_indices_outer, ]
  
  # Configurar la validación cruzada interna
  folds_inner <- createFolds(datos_entrenamiento_outer$ESTADO_ACTUAL, k = k_inner, list = FALSE)  # Obtener índices de pliegues internos
  
  # Almacenar predicciones y objetivos para el AUC general
  predicciones_auc <- numeric(nrow(datos_entrenamiento_outer))
  targets_auc <- numeric(nrow(datos_entrenamiento_outer))
  
  # Inicializar variables para almacenar resultados internos
  resultados_auc_inner <- numeric(k_inner)
  resultados_accuracy_inner <- numeric(k_inner)
  resultados_sensibilidad_inner <- numeric(k_inner)
  
  # Iterar sobre cada pliegue interno para selección de modelo
  indices_inner <- 1:nrow(datos_entrenamiento_outer)
  for (i_inner in 1:k_inner) {
    # Obtener los índices para el pliegue interno actual
    test_indices_inner <- indices_inner[folds_inner == i_inner]  # Índices correspondientes al pliegue interno de test
    train_indices_inner <- setdiff(1:nrow(datos_entrenamiento_outer), test_indices_inner)  # Índices correspondientes al conjunto de entrenamiento interno
    
    # Dividir datos en entrenamiento y test para el pliegue interno
    datos_entrenamiento_inner <- datos_entrenamiento_outer[train_indices_inner, ]
    datos_validacion <- datos_entrenamiento_outer[test_indices_inner, ]
    
    # Aplicar oversampling en el conjunto de entrenamiento interno
    datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ ., data = datos_entrenamiento_inner, seed = 123, p = proportion)$data
    for (params in 1:nrow(param_grid)) {
      usekernel <- param_grid$usekernel[params]
      laplace <- param_grid$laplace[params]
      
      # Entrenar el modelo Naive Bayes con la combinación de hiperparámetros actual
      modelo <- naiveBayes(formula, data = datos_entrenamiento_oversampled, kernel = usekernel, laplace = laplace)
      
      # Realizar predicciones en datos de validación
      predicciones_validacion <- predict(modelo, newdata = datos_validacion, type = "raw")
      
      # Calcular AUC para la combinación actual
      roc_obj <- roc(ifelse(datos_validacion$ESTADO_ACTUAL == "SI", 1, 0), predicciones_validacion[, "SI"])
      auc_actual <- as.numeric(auc(roc_obj))  # Extraer el valor numérico del AUC
      
      # Actualizar el mejor modelo y su AUC si se encuentra un modelo mejor
      if (auc_actual > mejor_auc) {
        mejor_auc <- auc_actual
        mejor_modelo <- modelo
      }
    }
    # Almacenar las predicciones y objetivos
    predicciones_auc[test_indices_inner] <- predicciones_validacion_prob
    targets_auc[test_indices_inner] <- as.numeric(datos_validacion$ESTADO_ACTUAL) - 1
    
    # Calcular la sensibilidad y la precisión
    umbral <- 0.5
    predicciones_finales <- ifelse(predicciones_validacion_prob >= umbral, "SI", "NO")
    TP <- sum(predicciones_finales == "SI" & datos_validacion[["ESTADO_ACTUAL"]] == "SI")
    FN <- sum(predicciones_finales == "NO" & datos_validacion[["ESTADO_ACTUAL"]] == "SI")
    FP <- sum(predicciones_finales == "SI" & datos_validacion[["ESTADO_ACTUAL"]] == "NO")
    TN <- sum(predicciones_finales == "NO" & datos_validacion[["ESTADO_ACTUAL"]] == "NO")
    
    recall_actual <- TP / (TP + FN)
    accuracy_actual <- (TP + TN) / (TP + FN + FP + TN)
    
    # Almacenar resultados internos
    resultados_sensibilidad_inner[i_inner] <- recall_actual
    resultados_accuracy_inner[i_inner] <- accuracy_actual
    resultados_auc_inner[i_inner] <- auc_actual
  }
  
  # Calcular el AUC final para este pliegue externo
  auc_fold_externo <- mean(resultados_auc_inner)
  accuracy_fold_externo <- mean(resultados_accuracy_inner)
  sensibilidad_fold_externo <- mean(resultados_sensibilidad_inner)
  
  # Almacenar el AUC del fold externo actual
  resultados_auc[i_outer] <- auc_fold_externo
  resultados_accuracy[i_outer] <- accuracy_fold_externo
  resultados_sensibilidad[i_outer] <- sensibilidad_fold_externo
  
  # Predecir en los datos de test para este pliegue externo y almacenar predicciones
  predicciones_test_outer_prob <- predict(modelo, newdata = datos_test_outer, type = "raw")[,2]
  predicciones_totales[test_indices_outer, i_outer] <- predicciones_test_outer_prob
  targets_totales[test_indices_outer] <- as.numeric(datos_test_outer$ESTADO_ACTUAL) - 1
}

# Calcular AUC promedio
auc_promedio <- mean(resultados_auc)
accuracy_promedio <- mean(resultados_accuracy)
sensibilidad_promedio <- mean(resultados_sensibilidad)

# Crear tabla con todas las métricas
tabla_metricas <- data.frame(
  Pliegue = 1:k_outer,
  AUC = sapply(resultados_auc, round, digits = 3),
  Accuracy = sapply(resultados_accuracy, round, digits = 3),
  Recall = sapply(resultados_sensibilidad, round, digits = 3)
)

# Agregar fila con promedio de métricas
tabla_metricas_naive <- rbind(tabla_metricas, c("Mean", round(auc_promedio, 3), round(accuracy_promedio, 3), round(sensibilidad_promedio, 3)))

# Detener el tiempo de ejecución
end_time <- Sys.time()
execution_time_naive <- end_time - start_time
execution_time_naive <- as.numeric(execution_time_naive, units = "secs")
save(tabla_metricas_naive, file = "tabla_metricas_naive_filter.RData")
save(execution_time_naive, file = "execution_time_naive_filter.RData")

```

```{r}
load("tabla_metricas_naive_filter.RData")
load("execution_time_naive_filter.RData")
# Imprimir el tiempo de ejecución
cat("Execution time:", execution_time_naive,  "seconds \n")



```

::: {.panel-tabset}

# Metrics Table
```{r}

# Imprimir tabla formateada con kableExtra
kable(tabla_metricas_naive, caption = "Cross Validation Metrics", align = "c") %>%
  kable_styling(full_width = FALSE)
```

# Metrics Plot
```{r}
crear_plot_metricas_per_fold(tabla_metricas_naive)

```
:::

The model is one of the best so far. The performance metrics are good and the model is robust. The next step is to use a variable selection method to improve the model. We are going to use brute force wrapper to select the best variables.

## 5x2 Cross Validation Wrapper

```{r,warning=FALSE,message=FALSE, eval=FALSE}


# Iniciar tiempo de ejecución
start_time <- Sys.time()

# Parámetros a cambiar
proportion <- 0.28

# Configurar la validación cruzada 5x2
k_outer <- 5  # Número de pliegues externos
k_inner <- 2  # Número de pliegues internos

# Lista para almacenar resultados
resultados_auc <- numeric(k_outer)
resultados_accuracy <- numeric(k_outer)
resultados_sensibilidad <- numeric(k_outer)
indices <- 1:nrow(data)

# Inicializar matrices para almacenar predicciones y objetivos
predicciones_totales <- matrix(NA, nrow = nrow(data), ncol = k_outer)
targets_totales <- numeric(nrow(data))

# Iterar sobre cada pliegue externo y realizar el proceso de modelado
for (i_outer in 1:k_outer) {
  
  # Obtener los índices para el pliegue externo actual
  test_indices_outer <- indices[folds_outer == i_outer]  # Índices correspondientes al pliegue externo de test
  train_indices_outer <- setdiff(1:nrow(data), test_indices_outer)  # Índices correspondientes al conjunto de entrenamiento
  
  # Dividir datos en entrenamiento y test para el pliegue externo
  datos_entrenamiento_outer <- data[train_indices_outer, ]
  datos_test_outer <- data[test_indices_outer, ]
  
  # Configurar la validación cruzada interna
  folds_inner <- createFolds(datos_entrenamiento_outer$ESTADO_ACTUAL, k = k_inner, list = FALSE)  # Obtener índices de pliegues internos
  
  # Almacenar predicciones y objetivos para el AUC general
  predicciones_auc <- numeric(nrow(datos_entrenamiento_outer))
  targets_auc <- numeric(nrow(datos_entrenamiento_outer))
  
  # Inicializar variables para almacenar resultados internos
  resultados_auc_inner <- numeric(k_inner)
  resultados_accuracy_inner <- numeric(k_inner)
  resultados_sensibilidad_inner <- numeric(k_inner)
  
  # Iterar sobre cada pliegue interno para selección de modelo
  indices_inner <- 1:nrow(datos_entrenamiento_outer)
  for (i_inner in 1:k_inner) {
    # Obtener los índices para el pliegue interno actual
    test_indices_inner <- indices_inner[folds_inner == i_inner]  # Índices correspondientes al pliegue interno de test
    train_indices_inner <- setdiff(1:nrow(datos_entrenamiento_outer), test_indices_inner)  # Índices correspondientes al conjunto de entrenamiento interno
    
    # Dividir datos en entrenamiento y test para el pliegue interno
    datos_entrenamiento_inner <- datos_entrenamiento_outer[train_indices_inner, ]
    datos_validacion <- datos_entrenamiento_outer[test_indices_inner, ]
    
    # Almacenar predicciones y objetivos para el AUC interno
    predicciones_auc <- numeric(nrow(datos_validacion))
    targets_auc <- numeric(nrow(datos_validacion))
    
    # Definir todas las combinaciones de variables (sustituye esto por tus combinaciones reales)

    # Iterar sobre cada combinación de variables para selección de modelo
    for (combinacion in combinaciones_variables) {
      
      # Aplicar oversampling en el conjunto de entrenamiento interno
      datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ ., data = datos_entrenamiento_inner, p = proportion)$data
      
      # Definir la fórmula para el modelo
      formula_actual <- as.formula(paste("ESTADO_ACTUAL", "~", paste(combinacion, collapse = "+")))
      
      # Entrenar el modelo Naive Bayes
      modelo <- naiveBayes(formula_actual, data = datos_entrenamiento_oversampled)
      
      # Realizar predicciones con probabilidades en datos de validación
      predicciones_validacion_prob <- predict(modelo, newdata = datos_validacion, type = "raw")[,2]
      
      # Calcular el AUC
      roc_obj <- roc(datos_validacion$ESTADO_ACTUAL, predicciones_validacion_prob, levels = rev(levels(datos_validacion$ESTADO_ACTUAL)))
      auc_actual <- auc(roc_obj)
      
      # Almacenar las predicciones y objetivos
      predicciones_auc[test_indices_inner] <- predicciones_validacion_prob
      targets_auc[test_indices_inner] <- as.numeric(datos_validacion$ESTADO_ACTUAL) - 1
      
      # Calcular la sensibilidad y la precisión
      umbral <- 0.5
      predicciones_finales <- ifelse(predicciones_validacion_prob >= umbral, "SI", "NO")
      TP <- sum(predicciones_finales == "SI" & datos_validacion$ESTADO_ACTUAL == "SI")
      FN <- sum(predicciones_finales == "NO" & datos_validacion$ESTADO_ACTUAL == "SI")
      FP <- sum(predicciones_finales == "SI" & datos_validacion$ESTADO_ACTUAL == "NO")
      TN <- sum(predicciones_finales == "NO" & datos_validacion$ESTADO_ACTUAL == "NO")
      
      recall_actual <- TP / (TP + FN)
      accuracy_actual <- (TP + TN) / (TP + FN + FP + TN)
      
      # Almacenar resultados internos
      resultados_sensibilidad_inner[i_inner] <- recall_actual
      resultados_accuracy_inner[i_inner] <- accuracy_actual
      resultados_auc_inner[i_inner] <- auc_actual
    }
  }
  
  # Calcular el AUC final para este pliegue externo
  auc_fold_externo <- mean(resultados_auc_inner)
  accuracy_fold_externo <- mean(resultados_accuracy_inner)
  sensibilidad_fold_externo <- mean(resultados_sensibilidad_inner)
  
  # Almacenar el AUC del fold externo actual
  resultados_auc[i_outer] <- auc_fold_externo
  resultados_accuracy[i_outer] <- accuracy_fold_externo
  resultados_sensibilidad[i_outer] <- sensibilidad_fold_externo
  
  # Predecir en los datos de test para este pliegue externo y almacenar predicciones
  # (Aquí deberías hacer la predicción usando el mejor modelo encontrado en el bucle interno)
}

# Calcular AUC promedio
auc_promedio <- mean(resultados_auc)
accuracy_promedio <- mean(resultados_accuracy)
sensibilidad_promedio <- mean(resultados_sensibilidad)

tabla_metricas <- data.frame(
  Pliegue = 1:k_outer,
  AUC = sapply(resultados_auc, round, digits = 3),
  Accuracy = sapply(resultados_accuracy, round, digits = 3),
  Recall = sapply(resultados_sensibilidad, round, digits = 3)
)

# Agregar fila con promedio de métricas
tabla_metricas_naive_wrapper <- rbind(tabla_metricas, c("Mean", round(auc_promedio, 3), round(accuracy_promedio, 3), round(sensibilidad_promedio, 3)))

# Detener el tiempo de ejecución
end_time <- Sys.time()
execution_time_naive <- end_time - start_time
execution_time_naive_wrapper <- as.numeric(execution_time_naive, units = "secs")
save(tabla_metricas_naive_wrapper, file = "tabla_metricas_naive_wrapper.RData")
save(execution_time_naive_wrapper, file = "execution_time_naive_wrapper.RData")

```

```{r}
load("tabla_metricas_naive_wrapper.RData")
load("execution_time_naive_wrapper.RData")
# Imprimir el tiempo de ejecución
cat("Execution time:", execution_time_naive_wrapper,  "seconds \n")



```

::: {.panel-tabset}

# Metrics Table
```{r}

# Imprimir tabla formateada con kableExtra
kable(tabla_metricas_naive_wrapper, caption = "Cross Validation Metrics", align = "c") %>%
  kable_styling(full_width = FALSE)
```

# Metrics Plot
```{r}
crear_plot_metricas_per_fold(tabla_metricas_naive_wrapper)

```
:::
The naive bayes model with wrapper variable selection is the best model so far. The performance metrics are good and the model is robust.
:::


# K-Nearest Neighbors {#k-nearest-neighbors}

::: {.panel-tabset style="background-color: #f0f0f0;box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);border-radius: 5px;"}

## Hold out

```{r,warning=FALSE,message=FALSE}
start_time <- Sys.time()
# Parámetros a cambiar
proportion <- 0.28
sensitivity_weight <- 0.8

# Dividir el conjunto de datos en entrenamiento y test
indices_entrenamiento <- sample(nrow(data), 0.75 * nrow(data))  # 75% para entrenamiento
datos_entrenamiento <- data[indices_entrenamiento, ]
datos_test <- data[-indices_entrenamiento, ]

# Dividir datos de entrenamiento en entrenamiento y validación
train_indices <- sample(nrow(datos_entrenamiento), 0.8 * nrow(datos_entrenamiento))  # 80% para entrenamiento
datos_entrenamiento_split <- datos_entrenamiento[train_indices, ]
datos_validacion <- datos_entrenamiento[-train_indices, ]

# Realizar sobremuestreo en los datos de entrenamiento
datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ ., data = datos_entrenamiento_split, p = proportion)$data


# Inicializar variables para almacenar los resultados
mejor_auc <- 0
mejor_k <- NULL
mejor_l <- NULL

# Codificación One-Hot Encoding para variables categóricas
encoded_data_entrenamiento <- as.data.frame(model.matrix(~.-1, datos_entrenamiento_oversampled[, -which(names(datos_entrenamiento_oversampled) == "ESTADO_ACTUAL")]))
encoded_data_test <- as.data.frame(model.matrix(~.-1, datos_test[, -which(names(datos_test) == "ESTADO_ACTUAL")]))
#Este código realiza la codificación One-Hot Encoding para las variables categóricas en los conjuntos de datos datos_entrenamiento_oversampled y datos_test. Primero, identifica la columna que contiene la variable objetivo "ESTADO_ACTUAL" y la excluye del proceso. Luego, utiliza la función model.matrix para crear una matriz de diseño que representa las variables categóricas codificadas en forma binaria. Finalmente, convierte esta matriz en un data frame.

param_grid <- expand.grid(k = seq(1, 20, by = 1),
                           l = seq(1, 10, by = 1))

# Bucle para probar diferentes combinaciones de hiperparámetros
for (i in 1:nrow(param_grid)) {
  k <- param_grid$k[i]
  l <- param_grid$l[i]
  # Realizar predicciones en datos de validación
  predicciones_validacion <- knn(train = encoded_data_entrenamiento, 
                                  test = encoded_data_test, 
                                  cl = datos_entrenamiento_oversampled$ESTADO_ACTUAL, 
                                  k = k, 
                                  use.all = TRUE,
                                  prob = TRUE,
                                  l = l)
  
  predicciones_validacion <- attr(predicciones_validacion, "prob")  # Asumiendo que "SI" es la clase positiva
  
  # Calcular AUC para la combinación actual
  roc_obj <- roc(ifelse(datos_test$ESTADO_ACTUAL == "SI", 1, 0), predicciones_validacion)
  auc_actual <- as.numeric(auc(roc_obj))  # Extraer el valor numérico del AUC
  
  # Actualizar el mejor modelo y su rendimiento si se encuentra un modelo mejor
  if (auc_actual > mejor_auc) {
    mejor_auc <- auc_actual
    mejor_k <- k
    mejor_l <- l
  }
}

end_time <- Sys.time()
execution_time <- end_time - start_time
execution_time <- as.numeric(execution_time, units = "secs")
cat("Execution time:", execution_time,  "seconds \n")
cat("Best k:", mejor_k, "\n")
cat("Best l:", mejor_l, "\n")
```
::: {.panel-tabset}

### Predictions

We are going to predict on test in order to get a estimation of the performance of the model.

```{r}
# Ejecutar la parte del código que involucra la función knn
predicciones_test <- knn(train = encoded_data_entrenamiento, 
                         test = encoded_data_test, 
                         cl = datos_entrenamiento_oversampled$ESTADO_ACTUAL, 
                         k = mejor_k, 
                         use.all = TRUE,
                         prob = TRUE)
predicciones_test <- attr(predicciones_test, "prob")  # Asumiendo que "SI" es la clase positiva

# Calcular las predicciones para el conjunto de test
clases_predichas_test <- ifelse(predicciones_test >= 0.5, "SI", "NO")
mostrar_metricas(datos_test$ESTADO_ACTUAL,clases_predichas_test)

```



### Roc curve and auc

```{r}
# Calcular la curva ROC y el AUC para el conjunto de test
plot_roc_curve(predicciones_test,datos_test$ESTADO_ACTUAL)

```
:::
The algorithm performance is really bad for this problem.


## 5x2 Cross Validation Filter

```{r,warning=FALSE,message=FALSE, eval=FALSE}
start_time <- Sys.time()

# Configurar la validación cruzada 5x2
k_outer <- 5  # Número de pliegues externos
k_inner <- 2  # Número de pliegues internos
folds_outer <- createFolds(data$ESTADO_ACTUAL, k = k_outer, list = FALSE)  # Obtener índices de pliegues externos

# Lista para almacenar resultados
resultados_auc <- numeric(k_outer)
resultados_sensibilidad <- numeric(k_outer)
resultados_accuracy <- numeric(k_outer)
indices <- 1:nrow(data)

# Inicializar matrices para almacenar predicciones y objetivos
predicciones_totales <- matrix(NA, nrow = nrow(data), ncol = k_outer)
targets_totales <- numeric(nrow(data))

# Iterar sobre cada pliegue externo y realizar el proceso de modelado
for (i_outer in 1:k_outer) {

  # Obtener los índices para el pliegue externo actual
  test_indices_outer <- indices[folds_outer == i_outer]  # Índices correspondientes al pliegue externo de test
  train_indices_outer <- setdiff(1:nrow(data), test_indices_outer)  # Índices correspondientes al conjunto de entrenamiento
  
  # Dividir datos en entrenamiento y test para el pliegue externo
  datos_entrenamiento_outer <- data[train_indices_outer, ]
  datos_test_outer <- data[test_indices_outer, ]
  

  # Configurar la validación cruzada interna
  folds_inner <- createFolds(datos_entrenamiento_outer$ESTADO_ACTUAL, k = k_inner, list = FALSE)  # Obtener índices de pliegues internos
  
  # Almacenar predicciones y test para formar una curva ROC general
  predicciones_auc <- numeric(nrow(datos_entrenamiento_outer))
  targets_auc <- numeric(nrow(datos_entrenamiento_outer))
  
  # Iterar sobre cada pliegue interno para selección de modelo
  indices_inner <- 1:nrow(datos_entrenamiento_outer)
  resultados_auc_inner <- numeric(k_inner)
  resultados_sensibilidad_inner <- numeric(k_inner)
  resultados_accuracy_inner <- numeric(k_inner)
  for (i_inner in 1:k_inner) {
    ###############################
    #### Aquí el filtrado #########
    ###############################
    filter_data <- filter_dependency(datos_entrenamiento_outer, "ESTADO_ACTUAL", colnames(datos_entrenamiento_outer), 0.05)
    if ("ESTADO_ACTUAL.1" %in% colnames(filter_data)) {
      filter_data <- filter_data[, !colnames(filter_data) %in% "ESTADO_ACTUAL.1", drop = FALSE]
    }
    
    # Obtener los índices para el pliegue interno actual
    test_indices_inner <- indices_inner[folds_inner == i_inner]  # Índices correspondientes al pliegue interno de test
    train_indices_inner <- setdiff(1:nrow(datos_entrenamiento_outer), test_indices_inner)  # Índices correspondientes al conjunto de entrenamiento interno
    
    # Dividir datos en entrenamiento y test para el pliegue interno
    datos_entrenamiento_inner <- datos_entrenamiento_outer[train_indices_inner, ]
    datos_validacion <- datos_entrenamiento_outer[test_indices_inner, ]
    
    # Aplicar oversampling en el conjunto de entrenamiento interno
    datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ .,
                                            data = datos_entrenamiento_inner,
                                            p = 0.28)$data
    
    # Inicializar variables para almacenar los resultados
    mejor_auc <- 0
    mejor_k <- NULL
    mejor_l <- NULL
    
    # Codificación One-Hot Encoding para variables categóricas
    encoded_data_entrenamiento <- as.data.frame(model.matrix(~.-1, datos_entrenamiento_oversampled[, -which(names(datos_entrenamiento_oversampled) == "ESTADO_ACTUAL")]))
    encoded_data_validacion <- as.data.frame(model.matrix(~.-1, datos_validacion[, -which(names(datos_validacion) == "ESTADO_ACTUAL")]))
   
    param_grid <- expand.grid(k = seq(1, 20, by = 1),
                               l = seq(1, 10, by = 1))

    # Iterar sobre las combinaciones de hiperparámetros
    for (i in 1:nrow(param_grid)) {
      k <- param_grid$k[i]
      l <- param_grid$l[i]
      
      # Realizar predicciones en datos de validación
      predicciones_validacion <- knn(train = encoded_data_entrenamiento, 
                                     test = encoded_data_validacion, 
                                     cl = datos_entrenamiento_oversampled$ESTADO_ACTUAL, 
                                     k = k, 
                                     use.all = TRUE,
                                     prob = TRUE,
                                     l = l)
      predicciones_validacion <- attr(predicciones_validacion, "prob")  # Asumiendo que "SI" es la clase positiva

      # Calcular AUC para la combinación actual
      roc_obj <- roc(ifelse(datos_validacion$ESTADO_ACTUAL == "SI", 1, 0), predicciones_validacion)
      auc_actual <- as.numeric(auc(roc_obj))  # Extraer el valor numérico del AUC
      
      # Actualizar el mejor modelo y su rendimiento si se encuentra un modelo mejor
      if (auc_actual > mejor_auc) {
        mejor_auc <- auc_actual
        mejor_k <- k
        mejor_l <- l
      }
    }
    
    ########################
    
    encoded_data_test <- as.data.frame(model.matrix(~.-1, datos_test_outer[, -which(names(datos_test_outer) == "ESTADO_ACTUAL")]))
    # Realizar predicciones en datos de test
    predicciones_test <- knn(train = encoded_data_entrenamiento, 
                             test = encoded_data_test, 
                             cl = datos_entrenamiento_oversampled$ESTADO_ACTUAL, 
                             k = mejor_k, 
                             use.all = TRUE,
                             prob = TRUE,
                             l = mejor_l)
   
  
    predicciones_test <- attr(predicciones_test, "prob")  # Asumiendo que "SI" es la clase positiva
  
    clases_predichas_test <- ifelse(predicciones_test >= 0.5, "SI", "NO")
    
    ###########################
    # Realizar predicciones en el conjunto de entrenamiento externo para el AUC general
    predicciones_auc[test_indices_inner] <- predicciones_test
    predicciones_clasificadas <- clases_predichas_test
  
    resultados_sensibilidad_inner[i_inner] <- sum(clases_predichas_test == "SI" & datos_validacion[["ESTADO_ACTUAL"]] == "SI") / sum(datos_validacion[["ESTADO_ACTUAL"]] == "SI")
    resultados_accuracy_inner[i_inner] <- mean(clases_predichas_test == datos_validacion[["ESTADO_ACTUAL"]])

    targets_auc[test_indices_inner] <- as.numeric(datos_validacion$ESTADO_ACTUAL) - 1

  }
  
  # Calcular el AUC final para este fold externo
  auc_fold_externo <- pROC::roc(targets_auc, predicciones_auc)$auc
  accuracy_fold_externo <- mean(resultados_accuracy_inner)
  sensibilidad_fold_externo <- mean(resultados_sensibilidad_inner)
 
  
  # Almacenar el AUC del fold externo actual
  resultados_auc[i_outer] <- auc_fold_externo
  resultados_accuracy[i_outer] <- accuracy_fold_externo
  resultados_sensibilidad[i_outer] <- sensibilidad_fold_externo
  
  # Predecir en los datos de test para este pliegue externo y almacenar predicciones
  predicciones_test <- knn(train = encoded_data_entrenamiento, 
                           test = encoded_data_test, 
                           cl = datos_entrenamiento_oversampled$ESTADO_ACTUAL, 
                           k = mejor_k, 
                           use.all = TRUE,
                           prob = TRUE,
                           l = mejor_l)
  
  predicciones_test <- attr(predicciones_test, "prob")  # Asumiendo que "SI" es la clase positiva
  clases_predichas_test <- ifelse(predicciones_test >= 0.5, "SI", "NO")
  
  # Almacenar predicciones de este fold externo
  predicciones_totales[test_indices_outer, i_outer] <- predicciones_test
  targets_totales[test_indices_outer] <- as.numeric(datos_test_outer$ESTADO_ACTUAL) - 1
}

# Calcular AUC promedio
auc_promedio <- mean(resultados_auc)
accuracy_promedio <- mean(resultados_accuracy)
sensibilidad_promedio <- mean(resultados_sensibilidad)

# Crear tabla con todas las métricas
tabla_metricas <- data.frame(
  Pliegue = 1:k_outer,
  AUC = sapply(resultados_auc, round, digits = 3),
  Accuracy = sapply(resultados_accuracy, round, digits = 3),
  Recall = sapply(resultados_sensibilidad, round, digits = 3)
)

# Agregar fila con promedio de métricas
tabla_metricas_knn <- rbind(tabla_metricas, c("Mean", round(auc_promedio, 3), round(accuracy_promedio, 3), round(sensibilidad_promedio, 3)))

end_time <- Sys.time()

execution_time_knn <- end_time - start_time
execution_time_knn <- as.numeric(execution_time_knn, units = "secs")

# Guardar resultados
save(tabla_metricas_knn, file = "tabla_metricas_knn.RData")
save(execution_time_knn, file = "execution_time_knn.RData")
```

```{r}
load("tabla_metricas_knn.RData")
load("execution_time_knn.RData")
# Imprimir el tiempo de ejecución
cat("Execution time:", execution_time_knn,  "seconds \n")
```
::: {.panel-tabset}

# Metrics Table
```{r}

# Imprimir tabla formateada con kableExtra
kable(tabla_metricas_knn, caption = "Cross Validation Metrics", align = "c") %>%
  kable_styling(full_width = FALSE)
```

# Metrics Plot
```{r}
crear_plot_metricas_per_fold(tabla_metricas_knn)

```

:::
The model does not have a good performance, in fact, it only predicts True Methastasis.
:::
# AdaBoost {#adaboost}
::: {.panel-tabset style="background-color: #f0f0f0;box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);border-radius: 5px;"}

## Hold out
```{r}
start_time <- Sys.time()
# Parámetros a cambiar
proportion <- 0.28
sensitivity_weight <- 0.8

# Dividir el conjunto de datos en entrenamiento y test
set.seed(123)  # Establecer una semilla para reproducibilidad
indices_entrenamiento <- sample(nrow(data), 0.75 * nrow(data))  # 75% para entrenamiento
datos_entrenamiento <- data[indices_entrenamiento, ]
datos_test <- data[-indices_entrenamiento, ]

# Dividir datos de entrenamiento en entrenamiento y validación
train_indices <- sample(nrow(datos_entrenamiento), 0.8 * nrow(datos_entrenamiento))  # 80% para entrenamiento
datos_entrenamiento_split <- datos_entrenamiento[train_indices, ]
datos_validacion <- datos_entrenamiento[-train_indices, ]

# Realizar sobremuestreo en los datos de entrenamiento
datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ ., data = datos_entrenamiento_split, seed = 123, p = proportion)$data

# Definir la función para calcular el métrico personalizado
compute_custom_metric <- function(recall, accuracy, sensitivity_weight = 1) {
  custom_metric <- (sensitivity_weight * recall) + ((1 - sensitivity_weight) * accuracy)
  return(custom_metric)
}

# Inicializar variables para almacenar los resultados
mejor_auc <- 0
mejor_modelo <- NULL
mejores_iteraciones <- 0

# Bucle para probar diferentes combinaciones de hiperparámetros
for (iteraciones in seq(50, 500, 50)) {
  # Entrenar el modelo glmboost
  modelo <- glmboost(ESTADO_ACTUAL ~ ., data = datos_entrenamiento_oversampled, family = Binomial(), control = boost_control(mstop = iteraciones))
  
  # Realizar predicciones en datos de validación
  predicciones_validacion <- predict(modelo, newdata = datos_validacion, type = "response")
  
  # Calcular el AUC
  pred <- prediction(predicciones_validacion, datos_validacion$ESTADO_ACTUAL)
  perf <- performance(pred, measure = "auc")
  auc_actual <- as.numeric(perf@y.values)
  
  # Actualizar el mejor modelo y su rendimiento si se encuentra un modelo mejor
  if (auc_actual > mejor_auc) {
    mejor_auc <- auc_actual
    mejor_modelo <- modelo
    mejores_iteraciones <- iteraciones
  }
}

summary(mejor_modelo)
end_time <- Sys.time()
execution_time <- end_time - start_time
execution_time <- as.numeric(execution_time, units = "secs")
cat("Execution time:", execution_time,  "seconds \n")
```

::: {.panel-tabset}

### Predictions

We are going to predict on test in order to get a estimation of the performance of the model.

```{r}
# Evaluar el mejor modelo en el conjunto de prueba
predicciones_test <- predict(mejor_modelo, newdata = datos_test, type = "response")
clases_predichas_test <- ifelse(predicciones_test > 0.5, "SI", "NO")
mostrar_metricas(datos_test$ESTADO_ACTUAL,clases_predichas_test)

```


### Roc curve and auc

```{r}
plot_roc_curve(predicciones_test,datos_test$ESTADO_ACTUAL)

```
:::
The performance of the model is encouraging. The model has a good performance in the test set.

## 5x2 Cross Validation Filter



```{r,warning=FALSE,message=FALSE, eval=FALSE}
start_time <- Sys.time()

# Configurar la validación cruzada 5x2
k_outer <- 5  # Número de pliegues externos
k_inner <- 2  # Número de pliegues internos
folds_outer <- createFolds(data$ESTADO_ACTUAL, k = k_outer, list = FALSE)  # Obtener índices de pliegues externos

# Lista para almacenar resultados
resultados_auc <- numeric(k_outer)
resultados_sensibilidad <- numeric(k_outer)
resultados_accuracy <- numeric(k_outer)
indices <- 1:nrow(data)

# Inicializar matrices para almacenar predicciones y objetivos
predicciones_totales <- matrix(NA, nrow = nrow(data), ncol = k_outer)
targets_totales <- numeric(nrow(data))

# Iterar sobre cada pliegue externo y realizar el proceso de modelado
for (i_outer in 1:k_outer) {

  # Obtener los índices para el pliegue externo actual
  test_indices_outer <- indices[folds_outer==i_outer]  # Índices correspondientes al pliegue externo de test
  train_indices_outer <- setdiff(1:nrow(data), test_indices_outer)  # Índices correspondientes al conjunto de entrenamiento
  
  # Dividir datos en entrenamiento y test para el pliegue externo
  datos_entrenamiento_outer <- data[train_indices_outer, ]
  datos_test_outer <- data[test_indices_outer, ]
  

  # Configurar la validación cruzada interna
  folds_inner <- createFolds(datos_entrenamiento_outer$ESTADO_ACTUAL, k = k_inner, list = FALSE)  # Obtener índices de pliegues internos
  
  # Almacenar predicciones y test para formar una curva ROC general
  predicciones_auc <- numeric(nrow(datos_entrenamiento_outer))
  targets_auc <- numeric(nrow(datos_entrenamiento_outer))
  
  # Iterar sobre cada pliegue interno para selección de modelo
  indices_inner <- 1:nrow(datos_entrenamiento_outer)
  resultados_auc_inner <- numeric(k_inner)
  resultados_sensibilidad_inner <- numeric(k_inner)
  resultados_accuracy_inner <- numeric(k_inner)
  for (i_inner in 1:k_inner) {
    ###############################
    #### Aqui el filtrado##########
    ###############################
    filter_data <- filter_dependency(datos_entrenamiento_outer,"ESTADO_ACTUAL",colnames(datos_entrenamiento_outer),0.05)
    if ("ESTADO_ACTUAL.1" %in% colnames(filter_data)) {
      filter_data <- filter_data[, !colnames(filter_data) %in% "ESTADO_ACTUAL.1", drop = FALSE]
    }
#con poca variables solo unaa la vez o filtrado o wrapper
    
    
    
    # Obtener los índices para el pliegue interno actual
    test_indices_inner <- indices_inner[folds_inner==i_inner]  # Índices correspondientes al pliegue interno de test
    train_indices_inner <- setdiff(1:nrow(datos_entrenamiento_outer), test_indices_inner)  # Índices correspondientes al conjunto de entrenamiento interno
    
    # Dividir datos en entrenamiento y test para el pliegue interno
    datos_entrenamiento_inner <- datos_entrenamiento_outer[train_indices_inner, ]
    datos_validacion <- datos_entrenamiento_outer[test_indices_inner, ]
    
 
    # Aplicar oversampling en el conjunto de entrenamiento interno
    datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ .,
                                            data = datos_entrenamiento_inner,
                                            p = 0.28)$data
    
    # Inicializar variables para almacenar los resultados
    mejor_auc <- 0
    mejor_modelo <- NULL
    mejores_iteraciones <- 0
      
  
    # Iterar sobre las combinaciones de hiperparámetros
    for (iteraciones in seq(50, 500, 50)) {
      # Entrenar el modelo glmboost
      modelo <- glmboost(ESTADO_ACTUAL ~ ., data = datos_entrenamiento_oversampled, family = Binomial(), control = boost_control(mstop = iteraciones))
  
      predicciones_validacion <- predict(modelo, newdata = datos_validacion, type = "response")
    
    
      # Calcular el AUC
      pred <- prediction(predicciones_validacion, datos_validacion$ESTADO_ACTUAL)
      perf <- performance(pred, measure = "auc")
      auc_actual <- as.numeric(perf@y.values)
      
      # Actualizar el mejor modelo y su AUC si se encuentra un modelo mejor
      if (auc_actual > mejor_auc) {
          mejor_auc <- auc_actual
          mejor_modelo <- modelo
          mejores_iteraciones <- iteraciones
        }
      }
    ########################
    
    predicciones_test <- predict(mejor_modelo, newdata = datos_validacion, type = "response")
    clases_predichas_test <- ifelse(predicciones_test >= 0.5, "SI", "NO")
    
    ###########################
    # Realizar predicciones en el conjunto de entrenamiento externo para el AUC general
    predicciones_auc[test_indices_inner] <- predicciones_test
    predicciones_clasificadas <- clases_predichas_test
  
    resultados_sensibilidad_inner[i_inner] <- sum(clases_predichas_test == "SI" & datos_validacion[["ESTADO_ACTUAL"]] == "SI") / sum(datos_validacion[["ESTADO_ACTUAL"]] == "SI")
    resultados_accuracy_inner[i_inner] <- mean(clases_predichas_test == datos_validacion[["ESTADO_ACTUAL"]])

    targets_auc[test_indices_inner] <- as.numeric(datos_validacion$ESTADO_ACTUAL) - 1

  }
  
  # Calcular el AUC final para este fold externo
  auc_fold_externo <- pROC::roc(targets_auc, predicciones_auc)$auc
  accuracy_fold_externo <- mean(resultados_accuracy_inner)
  sensibilidad_fold_externo <- mean(resultados_sensibilidad_inner)
 
  
  # Almacenar el AUC del fold externo actual
  resultados_auc[i_outer] <- auc_fold_externo
  resultados_accuracy[i_outer] <- accuracy_fold_externo
  resultados_sensibilidad[i_outer] <- sensibilidad_fold_externo
  
  # Predecir en los datos de test para este pliegue externo y almacenar predicciones
  predicciones_test <- predict(mejor_modelo, newdata = datos_test_outer, type = "response")
  predicciones_totales[test_indices_outer, i_outer] <- predicciones_test
  targets_totales[test_indices_outer] <- as.numeric(datos_test_outer$ESTADO_ACTUAL) - 1
 
}

# Calcular AUC promedio
auc_promedio <- mean(resultados_auc)
accuracy_promedio <- mean(resultados_accuracy)
sensibilidad_promedio <- mean(resultados_sensibilidad)


# Crear tabla con todas las métricas
tabla_metricas <- data.frame(
  Pliegue = 1:k_outer,
  AUC = sapply(resultados_auc, round, digits = 3),
  Accuracy = sapply(resultados_accuracy, round, digits = 3),
  Recall = sapply(resultados_sensibilidad, round, digits = 3)
)


# Agregar fila con promedio de métricas
tabla_metricas_ada <- rbind(tabla_metricas, c("Mean", round(auc_promedio,3), round(accuracy_promedio,3), round(sensibilidad_promedio,3)))

end_time <- Sys.time()

execution_time_ada <- end_time - start_time
execution_time_ada <- as.numeric(execution_time_ada, units = "secs")
save(tabla_metricas_ada, file = "tabla_metricas_ada_filter.RData")
save(execution_time_ada, file = "execution_time_ada_filter.RData")

```

```{r}
load("tabla_metricas_ada_filter.RData")
load("execution_time_ada_filter.RData")
# Imprimir el tiempo de ejecución
cat("Execution time:", execution_time_ada,  "seconds \n")


```



::: {.panel-tabset}

# Metrics Table
```{r}

# Imprimir tabla formateada con kableExtra
kable(tabla_metricas_ada, caption = "Cross Validation Metrics", align = "c") %>%
  kable_styling(full_width = FALSE)
```

# Metrics Plot
```{r}
crear_plot_metricas_per_fold(tabla_metricas_ada)

```

:::
The performance is good but not the best and it is so instable.
:::
:::

# Models Comparison {#models-comparison}

::: {.panel-tabset style="background-color: #f0f0f0;box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);border-radius: 5px;"}
 
## Metrics Comparison

For the comparison of the models, we are going to plot the metrics of each model in a bar plot. Specifically, we are going to compare the AUC, Accuracy, and Recall metrics of the best variables selected for each model.
```{r}
library(tidyverse)

crear_comparacion_metricas <- function(lista_tablas_metricas, nombres_tablas) {
  # Verificar que la longitud de la lista de tablas y los nombres coincida
  if (length(lista_tablas_metricas) != length(nombres_tablas)) {
    stop("La cantidad de tablas y de nombres debe ser la misma.")
  }
  
  # Añadir una columna con el nombre del modelo a cada tabla y combinarlas
  lista_tablas_metricas <- lapply(seq_along(lista_tablas_metricas), function(i) {
    tabla <- lista_tablas_metricas[[i]]
    tabla <- tabla %>%
      filter(Pliegue != "Mean") %>%  # Filtrar las filas de medias si no deseas incluirlas
      mutate(Modelo = nombres_tablas[i])
    # Convertir las columnas de métricas a numéricas
    tabla <- tabla %>%
      mutate(across(c(AUC, Accuracy, Recall), as.numeric))
    return(tabla)
  })
  
  # Combinar todas las tablas en una sola
  tabla_metricas_combinada <- bind_rows(lista_tablas_metricas)
  
  # Transformar la tabla a un formato largo
  tabla_metricas_long <- pivot_longer(tabla_metricas_combinada, 
                                      cols = c(AUC, Accuracy, Recall), 
                                      names_to = "Metrica", 
                                      values_to = "Valor")
  
  # Crear el gráfico
  plot <- ggplot(tabla_metricas_long, aes(x = Modelo, y = Valor, fill = Metrica)) +
    geom_bar(stat = "identity", position = "dodge") +
    labs(title = "Metrics comparison between models", x = "Model", y = "Value") +
    theme_minimal() +
    scale_fill_manual(values = c("AUC" = "steelblue", "Accuracy" = "lightblue", "Recall" = "#3C5B6F"), name = "Metric") +
    theme(plot.background = element_rect(color = rgb(226, 231, 236, maxColorValue = 255), size = 1.5)) +
    scale_y_continuous(limits = c(0, 1), expand = c(0, 0))  # Ajustar el eje y de 0 a 1 y sin espacio adicional
  
  return(plot)
}

lista_tablas_metricas <- list(tabla_metricas_logistic_wrapper, tabla_metricas_nnet_wrapper,tabla_metricas_svm_filter,tabla_metricas_dtree_filter,tabla_metricas_random_forest,tabla_metricas_naive_wrapper,tabla_metricas_ada,tabla_metricas_knn)

# Nombres de las tablas
nombres_tablas <- c("Logistic", "NN","SVM", "D.Tree", "R.Forest", "N.Bayes","AdaBoost","KNN")

# Llamar a la función
plot <- crear_comparacion_metricas(lista_tablas_metricas, nombres_tablas)
print(plot)


```

## Execution Time Comparison


```{r}

# Crear el vector de tiempos de entrenamiento
modelos_tiempos <- c(Logistic = execution_time_logistic_wrapper, 
                     Neural_Network = execution_time_nnet_wrapper,
                     Decision_Tree = execution_time_dtree_filter,
                     Random_Forest = execution_time_random_forest,
                     Naive_Bayes = execution_time_naive_wrapper,
                     SVM = execution_time_svm_filter,
                     AdaBoost = execution_time_ada,
                     KNN = execution_time_knn)


# Definir la función para comparar tiempos de entrenamiento
comparar_tiempos_entrenamiento <- function(modelos_tiempos) {
  # Verificar que los tiempos estén en un vector numérico
  if (!is.numeric(modelos_tiempos)) {
    stop("Los tiempos de entrenamiento deben ser un vector numérico.")
  }
  
  # Crear un dataframe con los nombres de los modelos y sus tiempos de entrenamiento
  modelos_df <- data.frame(
    modelo = names(modelos_tiempos),
    tiempo = modelos_tiempos
  )
  
  # Ordenar el dataframe por tiempo de entrenamiento
  modelos_df <- modelos_df[order(modelos_df$tiempo, decreasing = TRUE), ]
  
  # Crear el gráfico de barras con etiquetas de texto dentro de cada barra
  ggplot(modelos_df, aes(x = reorder(modelo, tiempo), y = tiempo, fill = tiempo)) +
    geom_bar(stat = "identity") +
    geom_text(aes(label = round(tiempo, 1)), hjust = 1.2, color = "white") + # Ajustar etiquetas dentro de las barras
    coord_flip() + # Girar el gráfico para que los modelos se vean en el eje y
    scale_fill_gradientn(colors = c("lightblue", "steelblue", "#3C5B6F","purple")) + # Gradiente de azul a rojo pasando por naranja
        labs(
      title = "Execution Time Comparison between Models",
      x = "Models",
      y = "Execution time (seconds)"
    ) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5)) +# Centrar el título
    theme(plot.background = element_rect(color = rgb(226, 231, 236, maxColorValue = 255), size = 1.5)) 

}

# Usar la función para comparar los tiempos de entrenamiento
comparar_tiempos_entrenamiento(modelos_tiempos)

```

:::
Results show that the best model for this problem is the naive bayes model. It has high AUC, accuracy and recall. Also, it is very stable regarding the hyperparameters. We can trust that the performance of the model is going to be very similar in the future.

# Final Model {#final-model}

The objetive of this section is to find the best hyperparameters and variable combiantion for the naive bayes model. The hyperparameter that we are going to tune is the Laplace smoothing, a technique used to avoid zero probabilities in the model. We are not going to tune the usekernel hyperparameter because our variables are not normally distributed(the function of usekernel is to asume that the variables are normally distributed or not).



::: {.panel-tabset style="background-color: #f0f0f0;box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);border-radius: 5px;"}

## Best Hyperparameters
```{r}

start_time <- Sys.time()

# Dividir el conjunto de datos en entrenamiento y test
indices_entrenamiento <- sample(nrow(data), 0.75 * nrow(data))  # 75% para entrenamiento
datos_entrenamiento <- data[indices_entrenamiento, ]
datos_test <- data[-indices_entrenamiento, ]

# Dividir datos de entrenamiento en entrenamiento y validación
train_indices <- sample(nrow(datos_entrenamiento), 0.8 * nrow(datos_entrenamiento))  # 80% para entrenamiento
datos_entrenamiento_split <- datos_entrenamiento[train_indices, ]
datos_validacion <- datos_entrenamiento[-train_indices, ]

# Oversampling
proportion <- 0.28
datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ ., data = datos_entrenamiento_split, p = proportion)$data

# Inicializar variables para almacenar los resultados
mejor_auc <- 0
mejor_modelo <- NULL
resultados <- data.frame(fL = numeric(), auc = numeric(), stringsAsFactors = FALSE)

# Definir hiperparámetros a probar
params <- expand.grid(fL = c(0, 0.5, 1))

# Bucle para probar diferentes combinaciones de hiperparámetros
for (i in 1:nrow(params)) {
  fL <- params$fL[i]
  
  
  # Entrenar el modelo Naive Bayes con la combinación de hiperparámetros actual
  modelo <- naiveBayes(ESTADO_ACTUAL ~ ., data = datos_entrenamiento_oversampled, fL = fL)
  
  # Realizar predicciones en datos de prueba
  predicciones_test <- predict(modelo, newdata = datos_test, type = "raw")
  
  # Calcular AUC para la combinación actual en datos de prueba
  roc_obj <- roc(ifelse(datos_test$ESTADO_ACTUAL == "SI", 1, 0), predicciones_test[, 2])
  auc_actual <- as.numeric(auc(roc_obj))  # Extraer el valor numérico del AUC
  
  # Guardar los resultados en la tabla
  resultados <- resultados %>%
    add_row(fL = fL, auc = auc_actual)
  
  # Actualizar el mejor modelo y su AUC si se encuentra un modelo mejor
  if (auc_actual > mejor_auc) {
    mejor_auc <- auc_actual
    mejor_modelo <- modelo
  }
}

# Ordenar los resultados por AUC en orden descendente
resultados_ordenados_nb <- resultados %>% arrange(desc(auc))

# Mostrar la tabla con los resultados de todas las combinaciones de hiperparámetros
save(resultados_ordenados_nb, file = "resultados_ordenados_nb.RData")

end_time <- Sys.time()
# Cargar los datos guardados y mostrar las tablas
load("resultados_ordenados_nb.RData")

# Seleccionar los mejores 10 modelos
mejores_10 <- resultados_ordenados_nb %>% head(10)


# Imprimir los mejores y peores modelos en tablas bonitas
mejores_10 %>%
  kbl(caption = "Top 10 Models by AUC") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))


```

## Visualization
```{r}

ggplot(mejores_10, aes(x = factor(fL), y = auc)) +
  geom_bar(stat = "identity", fill = "#3C5B6F") +
  labs(title = "AUC for each Laplace smoothing value",
       x = "fL",
       y = "AUC") +
  ylim(0, 1) +             # Establecer el límite del eje y
  theme_minimal() 
```
:::

There is no difference between the hyperparameters because Laplace smoothing is only necessary when the data is very sparse. 

## Variable Selection for the Final Model

The objetive of this section is to find the best variables for the final model. We are going to obtain the performance of every combination of variables in a repeated hold out and then we are going to select variables that are more frequent in the combinations with high AUC(threshold of 0.75).

::: {.panel-tabset style="background-color: #f0f0f0;box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);border-radius: 5px;"}
### Variables cominations and auc
```{r,results='asis',warning=FALSE,message=FALSE, eval=FALSE}
library(caret)

# Definir el número de repeticiones para el repeated holdout
n_repeats <- 50

# Crear particiones de datos
index <- createDataPartition(data$ESTADO_ACTUAL, times = n_repeats, p = 0.75, list = TRUE)

# Inicializar variables para almacenar los resultados
resultados <- data.frame(Variables = character(), auc = numeric(), stringsAsFactors = FALSE)

start_time <- Sys.time()

for (r in seq_along(index)) {
  # Obtener índices de entrenamiento y prueba para esta repetición
  train_indices <- unlist(index[-r])
  test_indices <- index[[r]]

  # Dividir el conjunto de datos en entrenamiento y test
  datos_entrenamiento <- data[train_indices, ]
  datos_test <- data[test_indices, ]

  # Oversampling
  proportion <- 0.28
  datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ ., data = datos_entrenamiento, p = proportion)$data

  for (comb in combinaciones_variables) {
    # Entrenar el modelo Naive Bayes con la combinación de variables actual
    modelo <- naiveBayes(ESTADO_ACTUAL ~ ., data = datos_entrenamiento_oversampled[, c(comb, "ESTADO_ACTUAL")])

    # Realizar predicciones en datos de prueba
    predicciones_test <- predict(modelo, newdata = datos_test, type = "raw")

    # Calcular AUC para la combinación actual en datos de prueba
    roc_obj <- roc(ifelse(datos_test$ESTADO_ACTUAL == "SI", 1, 0), predicciones_test[, 2])
    auc_actual <- as.numeric(auc(roc_obj))  # Extraer el valor numérico del AUC

    # Convertir la combinación en una cadena única
    comb_str <- paste(comb, collapse = ",")

    # Guardar los resultados en la tabla
    resultados <- resultados %>%
      add_row(Variables = comb_str, auc = auc_actual)
  }
}

# Calcular el AUC promedio para cada combinación de variables
resultados_promedio <- resultados %>%
  group_by(Variables) %>%
  summarise(auc_promedio = mean(auc))

# Ordenar los resultados por AUC promedio en orden descendente
resultados_ordenados_variables <- resultados_promedio %>% arrange(desc(auc_promedio))
save(resultados_ordenados_variables, file = "resultados_ordenados_variables.RData")


end_time <- Sys.time()
execution_time <- end_time - start_time

execution_time_30 <- as.numeric(execution_time, units = "secs")
save(execution_time_30, file = "execution_time_30.RData")

```



```{r,results='asis'}
load("resultados_ordenados_variables.RData")
load("execution_time_30.RData")


# Mostrar la tabla con los resultados de todas las combinaciones de variables
tabla <- kable(resultados_ordenados_variables, format = "html", align = "ccc", caption = "Tabla de Resultados", row.names = FALSE, booktabs = TRUE) %>%
  kable_styling(bootstrap_options = "striped")

# Mostrar la tabla con scroll
tabla_con_scroll <- scroll_box(tabla, width = "100%", height = "400px")

# Mostrar la tabla
print(tabla_con_scroll)

end_time <- Sys.time()

```


### Variable frequency in better combinations 
```{r}
# Definir el umbral para el AUC
umbral_auc <- 0.75

# Filtrar combinaciones con AUC mayor que el umbral
combinaciones_altas <- resultados_ordenados_variables[resultados_ordenados_variables$auc_promedio > umbral_auc, ]

# Crear un vector para almacenar las variables individuales
variables_individuales <- unlist(strsplit(combinaciones_altas$Variables, ","))

# Calcular la frecuencia de cada variable
frecuencia_variables <- table(variables_individuales)

# Crear la tabla de frecuencia de variables
tabla_frecuencia_variables <- data.frame(Variable = names(frecuencia_variables), Frecuencia = as.numeric(frecuencia_variables))

# Ordenar la tabla por frecuencia en orden descendente
tabla_frecuencia_variables <- tabla_frecuencia_variables[order(-tabla_frecuencia_variables$Frecuencia), ]

# Mostrar la tabla de frecuencia de variables
kable(tabla_frecuencia_variables, format = "html", align = "ccc", caption = "Variable Frequency in Best Combinations", row.names = FALSE, booktabs = TRUE) %>%
  kable_styling(bootstrap_options = "striped")




```
:::
The variables that are more frequent in the best combinations are ESTADO_ACTUAL, REst, Fenotipo, Grado and Edad. We are going to use these variables for the final model.

## Final model

Final model summary:
```{r}
# Modelo para producción
datafinal <- data[c("ESTADO_ACTUAL", "REst", "Fenotipo", "Grado", "Edad")]
modelo_final <- naiveBayes(ESTADO_ACTUAL ~ ., data = datafinal)
save(modelo_final, file = "modelo_final.RData")
summary(modelo_final)


```
## Cross validation comparation

The objetive of this section is to validate the final model with a cross validation.
We are going to compare different numbers of folds in the cross validation to see how this can affect the performance of the model.

::: {.panel-tabset style="background-color: #f0f0f0;box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);border-radius: 5px;"}

### 5x2 Cross Validation
```{r,warning=FALSE,message=FALSE, eval=FALSE}
# Iniciar tiempo de ejecución
start_time <- Sys.time()

# Parámetros a cambiar
proportion <- 0.28

# Configurar la validación cruzada 5x2
k_outer <- 5  # Número de pliegues externos
k_inner <- 2  # Número de pliegues internos

# Lista para almacenar resultados
resultados_auc <- numeric(k_outer)
resultados_accuracy <- numeric(k_outer)
resultados_sensibilidad <- numeric(k_outer)
indices <- 1:nrow(data)

# Inicializar matrices para almacenar predicciones y objetivos
predicciones_totales <- matrix(NA, nrow = nrow(data), ncol = k_outer)
targets_totales <- numeric(nrow(data))

# Iterar sobre cada pliegue externo y realizar el proceso de modelado
for (i_outer in 1:k_outer) {
  
  # Obtener los índices para el pliegue externo actual
  test_indices_outer <- indices[folds_outer == i_outer]  # Índices correspondientes al pliegue externo de test
  train_indices_outer <- setdiff(1:nrow(data), test_indices_outer)  # Índices correspondientes al conjunto de entrenamiento
  
  # Dividir datos en entrenamiento y test para el pliegue externo
  datos_entrenamiento_outer <- datafinal[train_indices_outer, ]
  datos_test_outer <- datafinal[test_indices_outer, ]
  
  # Configurar la validación cruzada interna
  folds_inner <- createFolds(datos_entrenamiento_outer$ESTADO_ACTUAL, k = k_inner, list = FALSE)  # Obtener índices de pliegues internos
  
  # Almacenar predicciones y objetivos para el AUC general
  predicciones_auc <- numeric(nrow(datos_entrenamiento_outer))
  targets_auc <- numeric(nrow(datos_entrenamiento_outer))
  
  # Inicializar variables para almacenar resultados internos
  resultados_auc_inner <- numeric(k_inner)
  resultados_accuracy_inner <- numeric(k_inner)
  resultados_sensibilidad_inner <- numeric(k_inner)
  
  # Iterar sobre cada pliegue interno para selección de modelo
  indices_inner <- 1:nrow(datos_entrenamiento_outer)
  for (i_inner in 1:k_inner) {
    # Obtener los índices para el pliegue interno actual
    test_indices_inner <- indices_inner[folds_inner == i_inner]  # Índices correspondientes al pliegue interno de test
    train_indices_inner <- setdiff(1:nrow(datos_entrenamiento_outer), test_indices_inner)  # Índices correspondientes al conjunto de entrenamiento interno
    
    # Dividir datos en entrenamiento y test para el pliegue interno
    datos_entrenamiento_inner <- datos_entrenamiento_outer[train_indices_inner, ]
    datos_validacion <- datos_entrenamiento_outer[test_indices_inner, ]
    
    # Almacenar predicciones y objetivos para el AUC interno
    predicciones_auc <- numeric(nrow(datos_validacion))
    targets_auc <- numeric(nrow(datos_validacion))
    
    # Definir todas las combinaciones de variables (sustituye esto por tus combinaciones reales)

    
    
    # Aplicar oversampling en el conjunto de entrenamiento interno
    datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ ., data = datos_entrenamiento_inner, p = proportion)$data
    
    
    # Entrenar el modelo Naive Bayes
    modelo <- naiveBayes(ESTADO_ACTUAL ~ ., data = datos_entrenamiento_oversampled)
    
    # Realizar predicciones con probabilidades en datos de validación
    predicciones_validacion_prob <- predict(modelo, newdata = datos_validacion, type = "raw")[,2]
    
    # Calcular el AUC
    roc_obj <- roc(datos_validacion$ESTADO_ACTUAL, predicciones_validacion_prob, levels = rev(levels(datos_validacion$ESTADO_ACTUAL)))
    auc_actual <- auc(roc_obj)
    
    # Almacenar las predicciones y objetivos
    predicciones_auc[test_indices_inner] <- predicciones_validacion_prob
    targets_auc[test_indices_inner] <- as.numeric(datos_validacion$ESTADO_ACTUAL) - 1
    
    # Calcular la sensibilidad y la precisión
    umbral <- 0.5
    predicciones_finales <- ifelse(predicciones_validacion_prob >= umbral, "SI", "NO")
    TP <- sum(predicciones_finales == "SI" & datos_validacion$ESTADO_ACTUAL == "SI")
    FN <- sum(predicciones_finales == "NO" & datos_validacion$ESTADO_ACTUAL == "SI")
    FP <- sum(predicciones_finales == "SI" & datos_validacion$ESTADO_ACTUAL == "NO")
    TN <- sum(predicciones_finales == "NO" & datos_validacion$ESTADO_ACTUAL == "NO")
    
    recall_actual <- TP / (TP + FN)
    accuracy_actual <- (TP + TN) / (TP + FN + FP + TN)
    
    # Almacenar resultados internos
    resultados_sensibilidad_inner[i_inner] <- recall_actual
    resultados_accuracy_inner[i_inner] <- accuracy_actual
    resultados_auc_inner[i_inner] <- auc_actual
  
  }
  
  # Calcular el AUC final para este pliegue externo
  auc_fold_externo <- mean(resultados_auc_inner)
  accuracy_fold_externo <- mean(resultados_accuracy_inner)
  sensibilidad_fold_externo <- mean(resultados_sensibilidad_inner)
  
  # Almacenar el AUC del fold externo actual
  resultados_auc[i_outer] <- auc_fold_externo
  resultados_accuracy[i_outer] <- accuracy_fold_externo
  resultados_sensibilidad[i_outer] <- sensibilidad_fold_externo
  
  # Predecir en los datos de test para este pliegue externo y almacenar predicciones
  # (Aquí deberías hacer la predicción usando el mejor modelo encontrado en el bucle interno)
}

# Calcular AUC promedio
auc_promedio <- mean(resultados_auc)
accuracy_promedio <- mean(resultados_accuracy)
sensibilidad_promedio <- mean(resultados_sensibilidad)

tabla_metricas <- data.frame(
  Pliegue = 1:k_outer,
  AUC = sapply(resultados_auc, round, digits = 3),
  Accuracy = sapply(resultados_accuracy, round, digits = 3),
  Recall = sapply(resultados_sensibilidad, round, digits = 3)
)

# Agregar fila con promedio de métricas
tabla_metricas_naive_52 <- rbind(tabla_metricas, c("Mean", round(auc_promedio, 3), round(accuracy_promedio, 3), round(sensibilidad_promedio, 3)))

# Detener el tiempo de ejecución
end_time <- Sys.time()
execution_time_naive <- end_time - start_time
execution_time_naive52 <- as.numeric(execution_time_naive, units = "secs")
save(tabla_metricas_naive_52, file = "tabla_metricas_naive_52.RData")
save(execution_time_naive52, file = "execution_time_naive_filter52.RData")

```

```{r}
load("tabla_metricas_naive_52.RData")
load("execution_time_naive_filter52.RData")
# Imprimir el tiempo de ejecución

cat("Execution time:", execution_time_naive52,  "seconds \n")



```

::: {.panel-tabset}

# Metrics Table
```{r}

# Imprimir tabla formateada con kableExtra
kable(tabla_metricas_naive_52, caption = "Cross Validation Metrics", align = "c") %>%
  kable_styling(full_width = FALSE)
```

# Metrics Plot
```{r}
crear_plot_metricas_per_fold(tabla_metricas_naive_52)

```
:::

### 10x2 Cross Validation
```{r,warning=FALSE,message=FALSE, eval=FALSE}
# Iniciar tiempo de ejecución
start_time <- Sys.time()

# Parámetros a cambiar
proportion <- 0.28

# Configurar la validación cruzada 5x2
k_outer <- 10  # Número de pliegues externos
k_inner <- 2  # Número de pliegues internos

# Lista para almacenar resultados
resultados_auc <- numeric(k_outer)
resultados_accuracy <- numeric(k_outer)
resultados_sensibilidad <- numeric(k_outer)
indices <- 1:nrow(data)

# Inicializar matrices para almacenar predicciones y objetivos
predicciones_totales <- matrix(NA, nrow = nrow(data), ncol = k_outer)
targets_totales <- numeric(nrow(data))

# Iterar sobre cada pliegue externo y realizar el proceso de modelado
for (i_outer in 1:k_outer) {
  
  # Obtener los índices para el pliegue externo actual
  test_indices_outer <- indices[folds_outer == i_outer]  # Índices correspondientes al pliegue externo de test
  train_indices_outer <- setdiff(1:nrow(data), test_indices_outer)  # Índices correspondientes al conjunto de entrenamiento
  
  # Dividir datos en entrenamiento y test para el pliegue externo
  datos_entrenamiento_outer <- datafinal[train_indices_outer, ]
  datos_test_outer <- datafinal[test_indices_outer, ]
  
  # Configurar la validación cruzada interna
  folds_inner <- createFolds(datos_entrenamiento_outer$ESTADO_ACTUAL, k = k_inner, list = FALSE)  # Obtener índices de pliegues internos
  
  # Almacenar predicciones y objetivos para el AUC general
  predicciones_auc <- numeric(nrow(datos_entrenamiento_outer))
  targets_auc <- numeric(nrow(datos_entrenamiento_outer))
  
  # Inicializar variables para almacenar resultados internos
  resultados_auc_inner <- numeric(k_inner)
  resultados_accuracy_inner <- numeric(k_inner)
  resultados_sensibilidad_inner <- numeric(k_inner)
  
  # Iterar sobre cada pliegue interno para selección de modelo
  indices_inner <- 1:nrow(datos_entrenamiento_outer)
  for (i_inner in 1:k_inner) {
    # Obtener los índices para el pliegue interno actual
    test_indices_inner <- indices_inner[folds_inner == i_inner]  # Índices correspondientes al pliegue interno de test
    train_indices_inner <- setdiff(1:nrow(datos_entrenamiento_outer), test_indices_inner)  # Índices correspondientes al conjunto de entrenamiento interno
    
    # Dividir datos en entrenamiento y test para el pliegue interno
    datos_entrenamiento_inner <- datos_entrenamiento_outer[train_indices_inner, ]
    datos_validacion <- datos_entrenamiento_outer[test_indices_inner, ]
    
    # Almacenar predicciones y objetivos para el AUC interno
    predicciones_auc <- numeric(nrow(datos_validacion))
    targets_auc <- numeric(nrow(datos_validacion))
    
    # Definir todas las combinaciones de variables (sustituye esto por tus combinaciones reales)

    
    
    # Aplicar oversampling en el conjunto de entrenamiento interno
    datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ ., data = datos_entrenamiento_inner, p = proportion)$data
    
    
    # Entrenar el modelo Naive Bayes
    modelo <- naiveBayes(ESTADO_ACTUAL ~ ., data = datos_entrenamiento_oversampled)
    
    # Realizar predicciones con probabilidades en datos de validación
    predicciones_validacion_prob <- predict(modelo, newdata = datos_validacion, type = "raw")[,2]
    
    # Calcular el AUC
    roc_obj <- roc(datos_validacion$ESTADO_ACTUAL, predicciones_validacion_prob, levels = rev(levels(datos_validacion$ESTADO_ACTUAL)))
    auc_actual <- auc(roc_obj)
    
    # Almacenar las predicciones y objetivos
    predicciones_auc[test_indices_inner] <- predicciones_validacion_prob
    targets_auc[test_indices_inner] <- as.numeric(datos_validacion$ESTADO_ACTUAL) - 1
    
    # Calcular la sensibilidad y la precisión
    umbral <- 0.5
    predicciones_finales <- ifelse(predicciones_validacion_prob >= umbral, "SI", "NO")
    TP <- sum(predicciones_finales == "SI" & datos_validacion$ESTADO_ACTUAL == "SI")
    FN <- sum(predicciones_finales == "NO" & datos_validacion$ESTADO_ACTUAL == "SI")
    FP <- sum(predicciones_finales == "SI" & datos_validacion$ESTADO_ACTUAL == "NO")
    TN <- sum(predicciones_finales == "NO" & datos_validacion$ESTADO_ACTUAL == "NO")
    
    recall_actual <- TP / (TP + FN)
    accuracy_actual <- (TP + TN) / (TP + FN + FP + TN)
    
    # Almacenar resultados internos
    resultados_sensibilidad_inner[i_inner] <- recall_actual
    resultados_accuracy_inner[i_inner] <- accuracy_actual
    resultados_auc_inner[i_inner] <- auc_actual
  
  }
  
  # Calcular el AUC final para este pliegue externo
  auc_fold_externo <- mean(resultados_auc_inner)
  accuracy_fold_externo <- mean(resultados_accuracy_inner)
  sensibilidad_fold_externo <- mean(resultados_sensibilidad_inner)
  
  # Almacenar el AUC del fold externo actual
  resultados_auc[i_outer] <- auc_fold_externo
  resultados_accuracy[i_outer] <- accuracy_fold_externo
  resultados_sensibilidad[i_outer] <- sensibilidad_fold_externo
  
  # Predecir en los datos de test para este pliegue externo y almacenar predicciones
  # (Aquí deberías hacer la predicción usando el mejor modelo encontrado en el bucle interno)
}

# Calcular AUC promedio
auc_promedio <- mean(resultados_auc)
accuracy_promedio <- mean(resultados_accuracy)
sensibilidad_promedio <- mean(resultados_sensibilidad)

tabla_metricas <- data.frame(
  Pliegue = 1:k_outer,
  AUC = sapply(resultados_auc, round, digits = 3),
  Accuracy = sapply(resultados_accuracy, round, digits = 3),
  Recall = sapply(resultados_sensibilidad, round, digits = 3)
)
# Agregar fila con promedio de métricas
tabla_metricas_naive12 <- rbind(tabla_metricas, c("Mean", round(auc_promedio, 3), round(accuracy_promedio, 3), round(sensibilidad_promedio, 3)))

# Detener el tiempo de ejecución
end_time <- Sys.time()
execution_time_naive <- end_time - start_time

execution_time_naive12 <- as.numeric(execution_time_naive, units = "secs")
save(tabla_metricas_naive12, file = "tabla_metricas_naive_filter12.RData")
save(execution_time_naive12, file = "execution_time_naive_filter12.RData")

```

```{r}
load("tabla_metricas_naive_filter12.RData")
load("execution_time_naive_filter12.RData")
# Imprimir el tiempo de ejecución
cat("Execution time:", execution_time_naive12,  "seconds \n")



```

::: {.panel-tabset}

# Metrics Table
```{r}

# Imprimir tabla formateada con kableExtra
kable(tabla_metricas_naive12, caption = "Cross Validation Metrics", align = "c") %>%
  kable_styling(full_width = FALSE)
```

# Metrics Plot
```{r}
crear_plot_metricas_per_fold(tabla_metricas_naive12)

```
:::

### 10x5 Cross Validation
```{r,warning=FALSE,message=FALSE, eval=FALSE}
# Iniciar tiempo de ejecución
start_time <- Sys.time()

# Parámetros a cambiar
proportion <- 0.28

# Configurar la validación cruzada 5x2
k_outer <- 10  # Número de pliegues externos
k_inner <- 5  # Número de pliegues internos

# Lista para almacenar resultados
resultados_auc <- numeric(k_outer)
resultados_accuracy <- numeric(k_outer)
resultados_sensibilidad <- numeric(k_outer)
indices <- 1:nrow(data)

# Inicializar matrices para almacenar predicciones y objetivos
predicciones_totales <- matrix(NA, nrow = nrow(data), ncol = k_outer)
targets_totales <- numeric(nrow(data))

# Iterar sobre cada pliegue externo y realizar el proceso de modelado
for (i_outer in 1:k_outer) {
  
  # Obtener los índices para el pliegue externo actual
  test_indices_outer <- indices[folds_outer == i_outer]  # Índices correspondientes al pliegue externo de test
  train_indices_outer <- setdiff(1:nrow(data), test_indices_outer)  # Índices correspondientes al conjunto de entrenamiento
  
  # Dividir datos en entrenamiento y test para el pliegue externo
  datos_entrenamiento_outer <- datafinal[train_indices_outer, ]
  datos_test_outer <- datafinal[test_indices_outer, ]
  
  # Configurar la validación cruzada interna
  folds_inner <- createFolds(datos_entrenamiento_outer$ESTADO_ACTUAL, k = k_inner, list = FALSE)  # Obtener índices de pliegues internos
  
  # Almacenar predicciones y objetivos para el AUC general
  predicciones_auc <- numeric(nrow(datos_entrenamiento_outer))
  targets_auc <- numeric(nrow(datos_entrenamiento_outer))
  
  # Inicializar variables para almacenar resultados internos
  resultados_auc_inner <- numeric(k_inner)
  resultados_accuracy_inner <- numeric(k_inner)
  resultados_sensibilidad_inner <- numeric(k_inner)
  
  # Iterar sobre cada pliegue interno para selección de modelo
  indices_inner <- 1:nrow(datos_entrenamiento_outer)
  for (i_inner in 1:k_inner) {
    # Obtener los índices para el pliegue interno actual
    test_indices_inner <- indices_inner[folds_inner == i_inner]  # Índices correspondientes al pliegue interno de test
    train_indices_inner <- setdiff(1:nrow(datos_entrenamiento_outer), test_indices_inner)  # Índices correspondientes al conjunto de entrenamiento interno
    
    # Dividir datos en entrenamiento y test para el pliegue interno
    datos_entrenamiento_inner <- datos_entrenamiento_outer[train_indices_inner, ]
    datos_validacion <- datos_entrenamiento_outer[test_indices_inner, ]
    
    # Almacenar predicciones y objetivos para el AUC interno
    predicciones_auc <- numeric(nrow(datos_validacion))
    targets_auc <- numeric(nrow(datos_validacion))
    
    # Definir todas las combinaciones de variables (sustituye esto por tus combinaciones reales)

    
    
    # Aplicar oversampling en el conjunto de entrenamiento interno
    datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ ., data = datos_entrenamiento_inner, p = proportion)$data
    
    
    # Entrenar el modelo Naive Bayes
    modelo <- naiveBayes(ESTADO_ACTUAL ~ ., data = datos_entrenamiento_oversampled)
    
    # Realizar predicciones con probabilidades en datos de validación
    predicciones_validacion_prob <- predict(modelo, newdata = datos_validacion, type = "raw")[,2]
    
    # Calcular el AUC
    roc_obj <- roc(datos_validacion$ESTADO_ACTUAL, predicciones_validacion_prob, levels = rev(levels(datos_validacion$ESTADO_ACTUAL)))
    auc_actual <- auc(roc_obj)
    
    # Almacenar las predicciones y objetivos
    predicciones_auc[test_indices_inner] <- predicciones_validacion_prob
    targets_auc[test_indices_inner] <- as.numeric(datos_validacion$ESTADO_ACTUAL) - 1
    
    # Calcular la sensibilidad y la precisión
    umbral <- 0.5
    predicciones_finales <- ifelse(predicciones_validacion_prob >= umbral, "SI", "NO")
    TP <- sum(predicciones_finales == "SI" & datos_validacion$ESTADO_ACTUAL == "SI")
    FN <- sum(predicciones_finales == "NO" & datos_validacion$ESTADO_ACTUAL == "SI")
    FP <- sum(predicciones_finales == "SI" & datos_validacion$ESTADO_ACTUAL == "NO")
    TN <- sum(predicciones_finales == "NO" & datos_validacion$ESTADO_ACTUAL == "NO")
    
    recall_actual <- TP / (TP + FN)
    accuracy_actual <- (TP + TN) / (TP + FN + FP + TN)
    
    # Almacenar resultados internos
    resultados_sensibilidad_inner[i_inner] <- recall_actual
    resultados_accuracy_inner[i_inner] <- accuracy_actual
    resultados_auc_inner[i_inner] <- auc_actual
  
  }
  
  # Calcular el AUC final para este pliegue externo
  auc_fold_externo <- mean(resultados_auc_inner)
  accuracy_fold_externo <- mean(resultados_accuracy_inner)
  sensibilidad_fold_externo <- mean(resultados_sensibilidad_inner)
  
  # Almacenar el AUC del fold externo actual
  resultados_auc[i_outer] <- auc_fold_externo
  resultados_accuracy[i_outer] <- accuracy_fold_externo
  resultados_sensibilidad[i_outer] <- sensibilidad_fold_externo
  
  # Predecir en los datos de test para este pliegue externo y almacenar predicciones
  # (Aquí deberías hacer la predicción usando el mejor modelo encontrado en el bucle interno)
}

# Calcular AUC promedio
auc_promedio <- mean(resultados_auc)
accuracy_promedio <- mean(resultados_accuracy)
sensibilidad_promedio <- mean(resultados_sensibilidad)

tabla_metricas <- data.frame(
  Pliegue = 1:k_outer,
  AUC = sapply(resultados_auc, round, digits = 3),
  Accuracy = sapply(resultados_accuracy, round, digits = 3),
  Recall = sapply(resultados_sensibilidad, round, digits = 3)
)

# Agregar fila con promedio de métricas
tabla_metricas_naive15 <- rbind(tabla_metricas, c("Mean", round(auc_promedio, 3), round(accuracy_promedio, 3), round(sensibilidad_promedio, 3)))

# Detener el tiempo de ejecución
end_time <- Sys.time()
execution_time_naive <- end_time - start_time

execution_time_naive15 <- as.numeric(execution_time_naive, units = "secs")
save(tabla_metricas_naive15, file = "tabla_metricas_naive_filter15.RData")
save(execution_time_naive15, file = "execution_time_naive_filter15.RData")

```

```{r}
load("tabla_metricas_naive_filter15.RData")
load("execution_time_naive_filter15.RData")
# Imprimir el tiempo de ejecución
cat("Execution time:", execution_time_naive15,  "seconds \n")



```

::: {.panel-tabset}

# Metrics Table
```{r}

# Imprimir tabla formateada con kableExtra
kable(tabla_metricas_naive15, caption = "Cross Validation Metrics", align = "c") %>%
  kable_styling(full_width = FALSE)
```

# Metrics Plot
```{r}
crear_plot_metricas_per_fold(tabla_metricas_naive15)

```
:::

### 20x10 Cross Validation
```{r,warning=FALSE,message=FALSE, eval=FALSE}
# Iniciar tiempo de ejecución
start_time <- Sys.time()

# Parámetros a cambiar
proportion <- 0.28

# Configurar la validación cruzada 5x2
k_outer <- 20  # Número de pliegues externos
k_inner <- 10  # Número de pliegues internos

# Lista para almacenar resultados
resultados_auc <- numeric(k_outer)
resultados_accuracy <- numeric(k_outer)
resultados_sensibilidad <- numeric(k_outer)
indices <- 1:nrow(data)

# Inicializar matrices para almacenar predicciones y objetivos
predicciones_totales <- matrix(NA, nrow = nrow(data), ncol = k_outer)
targets_totales <- numeric(nrow(data))

# Iterar sobre cada pliegue externo y realizar el proceso de modelado
for (i_outer in 1:k_outer) {
  
  # Obtener los índices para el pliegue externo actual
  test_indices_outer <- indices[folds_outer == i_outer]  # Índices correspondientes al pliegue externo de test
  train_indices_outer <- setdiff(1:nrow(data), test_indices_outer)  # Índices correspondientes al conjunto de entrenamiento
  
  # Dividir datos en entrenamiento y test para el pliegue externo
  datos_entrenamiento_outer <- datafinal[train_indices_outer, ]
  datos_test_outer <- datafinal[test_indices_outer, ]
  
  # Configurar la validación cruzada interna
  folds_inner <- createFolds(datos_entrenamiento_outer$ESTADO_ACTUAL, k = k_inner, list = FALSE)  # Obtener índices de pliegues internos
  
  # Almacenar predicciones y objetivos para el AUC general
  predicciones_auc <- numeric(nrow(datos_entrenamiento_outer))
  targets_auc <- numeric(nrow(datos_entrenamiento_outer))
  
  # Inicializar variables para almacenar resultados internos
  resultados_auc_inner <- numeric(k_inner)
  resultados_accuracy_inner <- numeric(k_inner)
  resultados_sensibilidad_inner <- numeric(k_inner)
  
  # Iterar sobre cada pliegue interno para selección de modelo
  indices_inner <- 1:nrow(datos_entrenamiento_outer)
  for (i_inner in 1:k_inner) {
    # Obtener los índices para el pliegue interno actual
    test_indices_inner <- indices_inner[folds_inner == i_inner]  # Índices correspondientes al pliegue interno de test
    train_indices_inner <- setdiff(1:nrow(datos_entrenamiento_outer), test_indices_inner)  # Índices correspondientes al conjunto de entrenamiento interno
    
    # Dividir datos en entrenamiento y test para el pliegue interno
    datos_entrenamiento_inner <- datos_entrenamiento_outer[train_indices_inner, ]
    datos_validacion <- datos_entrenamiento_outer[test_indices_inner, ]
    
    # Almacenar predicciones y objetivos para el AUC interno
    predicciones_auc <- numeric(nrow(datos_validacion))
    targets_auc <- numeric(nrow(datos_validacion))
    
    # Definir todas las combinaciones de variables (sustituye esto por tus combinaciones reales)

    
    
    # Aplicar oversampling en el conjunto de entrenamiento interno
    datos_entrenamiento_oversampled <- ROSE(ESTADO_ACTUAL ~ ., data = datos_entrenamiento_inner, p = proportion)$data
    
    
    # Entrenar el modelo Naive Bayes
    modelo <- naiveBayes(ESTADO_ACTUAL ~ ., data = datos_entrenamiento_oversampled)
    
    # Realizar predicciones con probabilidades en datos de validación
    predicciones_validacion_prob <- predict(modelo, newdata = datos_validacion, type = "raw")[,2]
    
    # Calcular el AUC
    roc_obj <- roc(datos_validacion$ESTADO_ACTUAL, predicciones_validacion_prob, levels = rev(levels(datos_validacion$ESTADO_ACTUAL)))
    auc_actual <- auc(roc_obj)
    
    # Almacenar las predicciones y objetivos
    predicciones_auc[test_indices_inner] <- predicciones_validacion_prob
    targets_auc[test_indices_inner] <- as.numeric(datos_validacion$ESTADO_ACTUAL) - 1
    
    # Calcular la sensibilidad y la precisión
    umbral <- 0.5
    predicciones_finales <- ifelse(predicciones_validacion_prob >= umbral, "SI", "NO")
    TP <- sum(predicciones_finales == "SI" & datos_validacion$ESTADO_ACTUAL == "SI")
    FN <- sum(predicciones_finales == "NO" & datos_validacion$ESTADO_ACTUAL == "SI")
    FP <- sum(predicciones_finales == "SI" & datos_validacion$ESTADO_ACTUAL == "NO")
    TN <- sum(predicciones_finales == "NO" & datos_validacion$ESTADO_ACTUAL == "NO")
    
    recall_actual <- TP / (TP + FN)
    accuracy_actual <- (TP + TN) / (TP + FN + FP + TN)
    
    # Almacenar resultados internos
    resultados_sensibilidad_inner[i_inner] <- recall_actual
    resultados_accuracy_inner[i_inner] <- accuracy_actual
    resultados_auc_inner[i_inner] <- auc_actual
  
  }
  
  # Calcular el AUC final para este pliegue externo
  auc_fold_externo <- mean(resultados_auc_inner)
  accuracy_fold_externo <- mean(resultados_accuracy_inner)
  sensibilidad_fold_externo <- mean(resultados_sensibilidad_inner)
  
  # Almacenar el AUC del fold externo actual
  resultados_auc[i_outer] <- auc_fold_externo
  resultados_accuracy[i_outer] <- accuracy_fold_externo
  resultados_sensibilidad[i_outer] <- sensibilidad_fold_externo
  
  # Predecir en los datos de test para este pliegue externo y almacenar predicciones
  # (Aquí deberías hacer la predicción usando el mejor modelo encontrado en el bucle interno)
}

# Calcular AUC promedio
auc_promedio <- mean(resultados_auc)
accuracy_promedio <- mean(resultados_accuracy)
sensibilidad_promedio <- mean(resultados_sensibilidad)

tabla_metricas <- data.frame(
  Pliegue = 1:k_outer,
  AUC = sapply(resultados_auc, round, digits = 3),
  Accuracy = sapply(resultados_accuracy, round, digits = 3),
  Recall = sapply(resultados_sensibilidad, round, digits = 3)
)
# Agregar fila con promedio de métricas
tabla_metricas_naive21 <- rbind(tabla_metricas, c("Mean", round(auc_promedio, 3), round(accuracy_promedio, 3), round(sensibilidad_promedio, 3)))

# Detener el tiempo de ejecución
end_time <- Sys.time()
execution_time_naive <- end_time - start_time

execution_time_naive21 <- as.numeric(execution_time_naive, units = "secs")
save(tabla_metricas_naive21, file = "tabla_metricas_naive_filter21.RData")
save(execution_time_naive21, file = "execution_time_naive_filter21.RData")

```

```{r}
load("tabla_metricas_naive_filter21.RData")
load("execution_time_naive_filter21.RData")
# Imprimir el tiempo de ejecución
cat("Execution time:", execution_time_naive21,  "seconds \n")



```

::: {.panel-tabset}

# Metrics Table
```{r}

# Imprimir tabla formateada con kableExtra
kable(tabla_metricas_naive21, caption = "Cross Validation Metrics", align = "c") %>%
  kable_styling(full_width = FALSE)
```

# Metrics Plot
```{r}
crear_plot_metricas_per_fold(tabla_metricas_naive21)

```
:::

:::

The model performance is more stable with a higher number of inner folds. On top of that, if we increase the number of outer folds, the model performance is more consistent. However, the execution time increases significantly with a higher number of folds. 

# Deployment {#deployment}


In this section, we will deploy the model using the Shiny framework. 

Source code of the Shiny app:


```{r,eval=FALSE}
library(shiny)
library(e1071)
library(shinyjs)
#| 
# Load the trained Naive Bayes model
load("modelo_final.RData")
modelo <- modelo_final

# Definir niveles para cada variable
niveles_edad <- c("0-30", "31-40", "41-50", "51-60", "61-70", "+70")
niveles_rest <- c("N", "P")
niveles_grado <- c("1", "2", "3")
niveles_fenotipo <- c("Basal", "Her2", "LumA", "LumB", "Normal")

# Define the UI with a modern theme
ui <- fluidPage(
  includeCSS("bootstrap.css"), # Apply a modern theme
  useShinyjs(), # Include shinyjs for JavaScript manipulation
  tags$head(
    tags$style(
      HTML("
            .prediction-text {
              font-size: 15px;
            }
          ")
    )
  ),
  div(style = "padding: 20px; border-radius: 10px; text-align: center; margin-bottom: 30px;",
      tags$h1("Breast Cancer Prediction App", style = "font-size: 30px;")),
  
  div(class = "container",
      h3("Enter the variables:"),
      fluidRow(
        column(6, selectInput("input_edad", "Age:", choices = niveles_edad, selected = "0-30")),
        column(6, selectInput("input_rest", "REst:", choices = niveles_rest, selected = "N"))
      ),
      fluidRow(
        column(6, selectInput("input_grado", "Grade:", choices = niveles_grado, selected = "1")),
        column(6, selectInput("input_fenotipo", "Phenotype:", choices = niveles_fenotipo, selected = "Basal"))
      ),
      fluidRow(
        column(6, actionButton("submit_button", "Make Prediction", class = "btn-primary")),
        column(6, actionButton("help_button", "Need help with oncological variables?", class = "btn-info"))
      ),
      div(style = "height: 20px;"), # Add space below buttons
      div(style = "border-bottom: 2px solid #ccc; margin-bottom: 20px;"), # Border for separation
      div(id = "resultado_container", style = "display: none;",
          h3("Prediction Result:"),
          div(class = "alert alert-dismissible alert-light prediction-text",
              strong("Prediction: "), 
              textOutput("resultado_prediccion")
          )
      ),
      br(),
      div(id = "help_container", style = "display: none;",
          div(class = "alert alert-dismissible alert-info",
              strong("Help: "), 
              "This alert provides information about the oncological variables used in the prediction:",
              br(),
              p("- Age: The age of the patient."),
              p("- REst: Hormone receptor status (N: Negative, P: Positive)."),
              p("- Grade: Histological grade of the tumor (1, 2, or 3)."),
              p("- Phenotype: Tumor phenotype (Basal, Her2, LumA, LumB, or Normal)."),
              br(),
              "It's important to note that this prediction is based on statistical modeling and should not replace personalized medical advice. Please consult with your healthcare provider for any concerns or questions regarding breast cancer diagnosis or treatment.",
              tags$button(type = "button", class = "btn-close", onclick = "shinyjs.hide('help_container');")
          )
      ),
      br(),
      div(id = "progress_container", style = "display: none;",
          div(class = "progress",
              div(class = "progress-bar", role = "progressbar", 
                  style = "width: 0%;", aria_valuenow = "0", aria_valuemin = "0", aria_valuemax = "100")
          )
      )
  )
)

# Define the server logic
server <- function(input, output, session) {
  # Perform prediction when the button is clicked
  observeEvent(input$submit_button, {
    # Collect user input data
    datos_usuario <- data.frame(
      Edad = factor(input$input_edad, levels = niveles_edad),
      REst = factor(input$input_rest, levels = niveles_rest),
      Grado = factor(input$input_grado, levels = niveles_grado),
      Fenotipo = factor(input$input_fenotipo, levels = niveles_fenotipo)
    )
    
    # Show progress bar
    shinyjs::show("progress_container")
    
    # Update the progress bar
    for (i in seq(0, 100, by = 25)) {
      shinyjs::runjs(paste0("$('.progress-bar').css('width', '", i, "%').attr('aria-valuenow', '", i, "');"))
      Sys.sleep(0.25)  # Shortened to 0.25 seconds to match 1 second total duration
    }
    
    # Perform prediction using the loaded model
    prediccion <- predict(modelo, newdata = datos_usuario, type = "raw")
    probabilidad <- prediccion[2]
    
    # Display the prediction to the user
    output$resultado_prediccion <- renderText({
      paste("The probability that the result is 'YES' is:", round(probabilidad, 2))
    })
    
    # Hide the progress bar after completing the prediction
    shinyjs::hide("progress_container")
    shinyjs::show("resultado_container")
  })
  
  # Display help alert if requested
  observeEvent(input$help_button, {
    shinyjs::show("help_container")
  })
}

# Run the Shiny app
shinyApp(ui, server)
```
To deploy the app we are going to use the rsconnect package. First, we have to set our account information and deploy the app using the following code:


```{r,eval=FALSE}
#subir la app
library(rsconnect)
rsconnect::setAccountInfo(name='nombre_de_usuario', token='TU_TOKEN', secret='TU_SECRET')
rsconnect::deployApp('ruta_de_la_app')
```


<style>
 .iframe-container {
            width: 80%;
            margin: auto;
            border: 1px solid #555;
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 4px 8px rgba(0,0,0,0.2);
            position: relative;
            background-color: #3b3b3b;
        }

        .iframe-tab {
            display: flex;
            justify-content: flex-end;
            align-items: center;
            padding: 8px;
            background-color: #4a4a4a;
            border-bottom: 1px solid #555;
        }

        .iframe-tab-buttons {
            display: flex;
            align-items: center;
        }

        .iframe-tab-buttons button {
            width: 20px;
            height: 20px;
            margin-left: 4px;
            background-color: #666;
            border: none;
            border-radius: 2px;
            display: flex;
            justify-content: center;
            align-items: center;
            cursor: not-allowed; /* Indica que no son funcionales */
            position: relative;
        }

        .iframe-tab-buttons button span {
            display: block;
            background-color: #ccc;
        }

        .iframe-tab-buttons button.minimize span {
            width: 12px;
            height: 2px;
        }

        .iframe-tab-buttons button.maximize span {
            width: 10px;
            height: 10px;
            border: 2px solid #ccc;
            box-sizing: border-box;
        }

        .iframe-tab-buttons button.close span {
            width: 12px;
            height: 12px;
            position: relative;
            display: flex;
            justify-content: center;
            align-items: center;
        }

        .iframe-tab-buttons button.close span:before,
        .iframe-tab-buttons button.close span:after {
            content: '';
            position: absolute;
            width: 6px;
            height: 2px;
            background-color: #ccc;
        }

        .iframe-tab-buttons button.close span:before {
            transform: rotate(45deg);
        }

        .iframe-tab-buttons button.close span:after {
            transform: rotate(-45deg);
        }

        .iframe-content {
            width: 100%;
            height: 500px; /* Ajusta la altura según tus necesidades */
            border: none;
            display: block;
        }
   
</style>

```{=html}
<div class="iframe-container">
        <div class="iframe-tab">
            <div class="iframe-tab-buttons">
                <button class="minimize" title="Minimizar"><span></span></button>
                <button class="maximize" title="Maximizar"><span></span></button>
                <button class="close" title="Cerrar"><span></span></button>
            </div>
        </div>
        <iframe src="https://g9bjvd-alex0silva.shinyapps.io/cancerapp/" class="iframe-content"></iframe>
    </div>

```





# Conclusions {#conclusions}

In this project, we have compared the performance of many machine learning algorithms for predicting methastasis in breast cancer. We have adapted the validation process and the hyperparameters selection to our computational resources. 

<br>

In the variable selection process, we have found that the most performant technique is the wrapper method, but it is computationally expensive. The filter method is less computationally expensive, but it does not have in concern the interaction between variables. Embedded methods are a good compromise between the two previous methods but are not as performant as the wrapper method.

<br>

The final model uses the Naive Bayes algorithm. The model achieved an AUC of 0.75 in the cross-validation process. The model is deployed in a Shiny app that allows users to predict the probability of methastasis in breast cancer based on four variables: age, hormone receptor status, histological grade, and tumor phenotype.

<br>

The next step is to carry out a external validation of the model. This process will allow us to evaluate the model's performance in new data.

# References {#references}

-   Renan Gomes do Nascimento, Kaléu Mormino Otoni. (2020).Histological and molecular classification of breast cancer: what do we know?

-   Predict breast cancer (2024). 
URL: https://breast.predict.cam/tool

-   Gepac. Etapas y grado histológico(2024)
URL:http://ascama.gepac.es/informacion-medica/etapas-y-grado-histologico/

-   Jackson, E. B., Gondara, L., Speers, C., Diocee, R., Nichol, A. M., Lohrisch, C., & Gelmon, K. A. (2023). Does age affect outcome with breast cancer?

```{=html}
<div id="scrollToTopContainer" style="position: fixed; bottom: 20px; right: 20px; display: none;">
  <button onclick="goToTop()" id="btnScrollToTop" type="button" class="btn btn-primary btn-lg">
    ↑
  </button>
</div>
```
importante quitar cellnoseq de los statistic analisis